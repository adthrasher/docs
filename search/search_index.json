{
    "docs": [
        {
            "location": "/", 
            "text": "Getting Started\n\n\nIntroduction\n\n\nWelcome to the St. Jude Cloud documentation! Here, you'll find the \nauthoritative information for accessing data, running analysis tools,\nand exploring results files in St. Jude Cloud. For a brief overview of\neverything St. Jude Cloud provides, we recommend that you visit the video\non \nour home page\n.\n\n\n\n\nNote\n\n\nThroughout the site, you might see some \"Todo\" notices. These are notes\nto ourself about pieces of the documentation that are not complete yet.\nPlease bear with us as we fill in these gaps!\n\n\n\n\nFeatures\n\n\nYou can leverage many different capabilities of St. Jude Cloud in your research, such as:\n\n\n\n\nExplore datasets by diagnosis, publication, or curated dataset.\n\n\nRun your tools on \nour\n data by requesting data in a secure cloud environment (\nrequesting our data\n and \npackaging your tools\n).\n\n\nRun our validated end-to-end workflows on \nyour\n data (\nexample\n). All of the workflows we offer are under \"Tool Guides\" to the left of this text.\n\n\nExplore St. Jude data that we have packaged for the community in our visualizations.\n\n\nLook at \nPecan\n for pediatric cancer data.\n\n\n\n\n\n\n\n\nContact Us\n\n\nAny questions, comments, or concerns can be directed to \nour \"Contact Us\" form\n.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#introduction", 
            "text": "Welcome to the St. Jude Cloud documentation! Here, you'll find the \nauthoritative information for accessing data, running analysis tools,\nand exploring results files in St. Jude Cloud. For a brief overview of\neverything St. Jude Cloud provides, we recommend that you visit the video\non  our home page .   Note  Throughout the site, you might see some \"Todo\" notices. These are notes\nto ourself about pieces of the documentation that are not complete yet.\nPlease bear with us as we fill in these gaps!", 
            "title": "Introduction"
        }, 
        {
            "location": "/#features", 
            "text": "You can leverage many different capabilities of St. Jude Cloud in your research, such as:   Explore datasets by diagnosis, publication, or curated dataset.  Run your tools on  our  data by requesting data in a secure cloud environment ( requesting our data  and  packaging your tools ).  Run our validated end-to-end workflows on  your  data ( example ). All of the workflows we offer are under \"Tool Guides\" to the left of this text.  Explore St. Jude data that we have packaged for the community in our visualizations.  Look at  Pecan  for pediatric cancer data.", 
            "title": "Features"
        }, 
        {
            "location": "/#contact-us", 
            "text": "Any questions, comments, or concerns can be directed to  our \"Contact Us\" form .", 
            "title": "Contact Us"
        }, 
        {
            "location": "/guides/data/data-overview/", 
            "text": "Summary\n\n\n\n\nData in St. Jude Cloud is grouped into different data access units (DAUs) which roughly correspond to projects. \n\n\nUsers are given access to DAUs on a case-by-case basis for a specific amount of time.\n\n\nAccess to data in a given DAU is assessed by the corresponding data access committee (DAC) on a case-by-case basis.\n\n\nThere are a number of terms of use and restrictions outlined in the \ndata access agreement\n. We ask that everyone\n    who will be analyzing the data has read and understands these terms.\n\n\n\n\n\n\nData in St. Jude Cloud is grouped into multiple \ndata access units\n (DAUs),\nwhich are independent projects/data sources with different governance structures. \nEach DAU has a separate \ndata access committee\n (DAC) that evaluates incoming \ndata requests based on a variety of factors. Access is granted at the DAU level\nbased on the decision of each DAC. The first time you request access to files in \na DAU, it is required that you fill out a DAA (which handles contemplated use and\nother legally binding terms). After you've been approved once, you can continue\nchecking out files from that DAU until your access expires (generally after \n1 year) and you need to renew.\n\n\n\n\nExample\n\n\nFor example, if you make a request asking for all of St. Jude's Acute \nLymphoblastic Leukemia sequencing data, you might be asking for data from \nmultiple different projects here at St. Jude. For the sake of the example,\nlet's say the data you want is spread across three different DAUs. Once\nyou place a request, your application will be routed to the corresponding\nthree data access committees for approval. Since each DAC is made up of\ndifferent individuals using different criteria for evaluation, you may or\nmay not be approved for access to all of the files. \n\n\n\n\nEmbargo dates\n\n\nAn embargo date is the time at which access to data is allowed to users\nfor publishing purposes. Typically, samples from the same Data Access\nUnit all have the same embargo date, as they are usually released on St.\nJude Cloud at the same time. Publishing any results derived from this data \nbefore this embargo date is not permitted as outlined in the data access agreement.\n\n\nCurrent Embargo Dates\n\n\n\n\n\n\n\n\nData Access Unit\n\n\nEmbargo Date\n\n\n\n\n\n\n\n\n\n\nPediatric Cancer Genome Project\n\n\nJuly 23, 2018\n\n\n\n\n\n\nSt. Jude LIFE\n\n\nJanuary 15, 2019\n\n\n\n\n\n\nClinical Genomics\n\n\nJanuary 15, 2019\n\n\n\n\n\n\nSickle Cell Genome Project\n\n\nSeptember 1, 2019\n\n\n\n\n\n\n\n\nFrequently asked questions\n\n\nQ. Where can I find the embargo dates?\n\n\nA.\n All of our samples are marked with an embargo date. \nYou can find this by looking at the tags for each file or in the\n\nSAMPLE_INFO.txt\n file that is included with each data request. \nSelect a sample and click info to see more.", 
            "title": "Data Access"
        }, 
        {
            "location": "/guides/data/data-overview/#embargo-dates", 
            "text": "An embargo date is the time at which access to data is allowed to users\nfor publishing purposes. Typically, samples from the same Data Access\nUnit all have the same embargo date, as they are usually released on St.\nJude Cloud at the same time. Publishing any results derived from this data \nbefore this embargo date is not permitted as outlined in the data access agreement.  Current Embargo Dates     Data Access Unit  Embargo Date      Pediatric Cancer Genome Project  July 23, 2018    St. Jude LIFE  January 15, 2019    Clinical Genomics  January 15, 2019    Sickle Cell Genome Project  September 1, 2019", 
            "title": "Embargo dates"
        }, 
        {
            "location": "/guides/data/data-overview/#frequently-asked-questions", 
            "text": "Q. Where can I find the embargo dates?  A.  All of our samples are marked with an embargo date. \nYou can find this by looking at the tags for each file or in the SAMPLE_INFO.txt  file that is included with each data request. \nSelect a sample and click info to see more.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/guides/data/types-of-data/", 
            "text": "Data types\n\n\n\n\nNote\n\n\nThis section of the documentation is currently under construction. If your question is not answered here,\nplease \ncontact us\n!\n\n\n\n\nSt. Jude Cloud hosts both raw genomic data files and processed results files:\n\n\n\n\n\n\n\n\nFile Type\n\n\nShort Description\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nBAM\n\n\nHG38 aligned BAM files produced by \nMicrosoft Genomics Service\n\n\nClick here\n\n\n\n\n\n\ngVCF\n\n\nGenomic VCF\n files produced by \nMicrosoft Genomics Service\n\n\nClick here\n\n\n\n\n\n\nSomatic VCF\n\n\nCurated list of somatic pipelines produced by the St. Jude somatic variant analysis pipeline\n\n\nClick here\n\n\n\n\n\n\n\n\nBAM files\n\n\nIn St. Jude Cloud, we stored aligned sequence reads in the ubiquitous BAM file format. BAM files were produced by the \nMicrosoft Genomics Service\n aligned to HG38 (GRCh38 no alt analysis set). For more information about how Microsoft Genomics produces BAM files or any other questions regarding data generation, please refer to \nthe official Microsoft Genomics whitepaper\n.\n\n\nFor more information on SAM/BAM files, please refer to the \nSAM/BAM specification\n. \n\n\ngVCF files\n\n\nWe provide gVCF files produced by the \nMicrosoft Genomics Service\n. gVCF files are derived from the BAM files produced above as called by \nGATK's haplotype caller\n. Today, we defer to \nthe official specification document\n from the Broad Institute, as well as \nthis discussion\n on the difference between VCF and gVCF files. For more information about how Microsoft Genomics produces gVCF files or any other questions regarding data generation, please refer to \nthe official Microsoft Genomics whitepaper\n.\n\n\nSomatic VCF files\n\n\nSomatic VCF files contain HG38 based SNV/Indel variant calls from the St. Jude somatic variant analysis pipeline as follows. Broadly speaking:\n\n\n\n\nReads were aligned to HG19 using \nbwa backtrack\n (\nbwa aln\n + \nbwa sampe\n) using default parameters.\n\n\nPost processing of aligned reads was performed using \nPicard\n \nCleanSam\n and \nMarkDuplicates\n.\n\n\nVariants were called using the \nBambino\n variant caller (you can download by navigating \nhere\n and searching for \"Bambino package\").\n\n\nVariants were post-processed using a in-house post-processing pipeline that cleans and annotates variants. This pipeline is not currently publicly available.\n\n\nVariants were manually reviewed by analysts and published with \nthe relevant Pediatric Cancer Genome Project (PCGP) paper\n.\n\n\nPost-publication, variants were lifted over to HG38 (the original HG19 coordinates are stored in the \nHG19\n INFO field.).\n\n\n\n\nFor more information on variants for each of the individuals, please refer to the relevant PCGP paper. For more information on the variant calling format (VCF), please see the latest specification for VCF document listed \nhere\n.", 
            "title": "Types of Data"
        }, 
        {
            "location": "/guides/data/types-of-data/#data-types", 
            "text": "Note  This section of the documentation is currently under construction. If your question is not answered here,\nplease  contact us !   St. Jude Cloud hosts both raw genomic data files and processed results files:     File Type  Short Description  Details      BAM  HG38 aligned BAM files produced by  Microsoft Genomics Service  Click here    gVCF  Genomic VCF  files produced by  Microsoft Genomics Service  Click here    Somatic VCF  Curated list of somatic pipelines produced by the St. Jude somatic variant analysis pipeline  Click here", 
            "title": "Data types"
        }, 
        {
            "location": "/guides/data/types-of-data/#bam-files", 
            "text": "In St. Jude Cloud, we stored aligned sequence reads in the ubiquitous BAM file format. BAM files were produced by the  Microsoft Genomics Service  aligned to HG38 (GRCh38 no alt analysis set). For more information about how Microsoft Genomics produces BAM files or any other questions regarding data generation, please refer to  the official Microsoft Genomics whitepaper .  For more information on SAM/BAM files, please refer to the  SAM/BAM specification .", 
            "title": "BAM files"
        }, 
        {
            "location": "/guides/data/types-of-data/#gvcf-files", 
            "text": "We provide gVCF files produced by the  Microsoft Genomics Service . gVCF files are derived from the BAM files produced above as called by  GATK's haplotype caller . Today, we defer to  the official specification document  from the Broad Institute, as well as  this discussion  on the difference between VCF and gVCF files. For more information about how Microsoft Genomics produces gVCF files or any other questions regarding data generation, please refer to  the official Microsoft Genomics whitepaper .", 
            "title": "gVCF files"
        }, 
        {
            "location": "/guides/data/types-of-data/#somatic-vcf-files", 
            "text": "Somatic VCF files contain HG38 based SNV/Indel variant calls from the St. Jude somatic variant analysis pipeline as follows. Broadly speaking:   Reads were aligned to HG19 using  bwa backtrack  ( bwa aln  +  bwa sampe ) using default parameters.  Post processing of aligned reads was performed using  Picard   CleanSam  and  MarkDuplicates .  Variants were called using the  Bambino  variant caller (you can download by navigating  here  and searching for \"Bambino package\").  Variants were post-processed using a in-house post-processing pipeline that cleans and annotates variants. This pipeline is not currently publicly available.  Variants were manually reviewed by analysts and published with  the relevant Pediatric Cancer Genome Project (PCGP) paper .  Post-publication, variants were lifted over to HG38 (the original HG19 coordinates are stored in the  HG19  INFO field.).   For more information on variants for each of the individuals, please refer to the relevant PCGP paper. For more information on the variant calling format (VCF), please see the latest specification for VCF document listed  here .", 
            "title": "Somatic VCF files"
        }, 
        {
            "location": "/guides/data/data-request/", 
            "text": "Introduction\n\n\nCreating a data request is the premier way to access raw St. Jude next \ngeneration sequencing data in the cloud. You can get a \nfree\n copy of \nthe data in a secure cloud environment powered by Microsoft Azure and \nDNAnexus, or you can elect to download the data to your local computing \nenvironment.\n\n\n\n\nNote\n\n\nIf you would like to download the data to local storage, there are\nextra steps you'll need to follow such as getting additional signatures\non your data access agreement. We recommend that you work with the data\nin the cloud if it's feasible; the data provided by St. Jude is free, the compute charges are reasonable, and working in the cloud helps to eliminate the long, error-prone downloading process. Porting your tools to be run in the cloud is easy, as well. We recommend you follow \nthis guide\n to get started.\n\n\n\n\nRequesting Data\n\n\nNavigate to the Data Browser\n\n\nNavigate to the data browser by clicking Access Data and Explore\nData. If you would like to view the requests you have already made, you\ncan do so under Manage Data.\n\n\n\n\nSelect Your Data\n\n\nNext, select the cohort you are interested in requesting. You can\nbrowse our data by disease, publication, dataset, or\nyou can use Pecan, our Pediatric Cancer portal, to find samples\nassociated with specific genes, or mutations. \n\n\n\n\nSelect Your Files\n\n\nOnce you have selected the cohort you would like to study, you can chose what\ntype of files you would like to receive. This step is dynamic, and shows\nyou all of the file types we have available for the dataset you are\ninterested in.\n\n\n\n\nSubmit Your Request\n\n\nIf you are requesting access to a dataset you have not already been approved for, \nyou will see a section called Controlled Data. Under this section, there is a bulleted list\nthat indicates the Data Access Units you need to request access to through our data\naccess agreement. Please use this list to fill in the Datasets section of the Data Request\nForm as shown in the figure below. For more information on filling out this guide,\nsee \nFilling out the DAA\n.\n\n\n\n\nNow, you have successfully submitted your data request. Your request\nwill be sent to the respective data access committees for evaluation.\nYou should expect to hear from us within a week or two on average. \n\n\nData access agreement\n\n\nBroadly speaking, the data access agreement (DAA) is a legal document\nbinding the requestor and the requestor's institution to terms about\nhow our data may be used. We do not negotiate the terms of this document\nunless terms are found to be in conflict with the institution's state law. \nFilling out the Data Access Agreement carefully and completely is crucial \nto having your request filled promptly.\n\n\nFilling out the DAA\n\n\nAll Data Access Agreements require the following 5 items:\n\n\n\n\n\n\nPage 5\n The Data Access Unit(s) you are applying for must be marked.\n\n\n\n\nThese can be found in the section Controlled Data, above the Download Data Request\n  Form button. This can be found on the third step of the data request\n  process under the Controlled Data section. This is a dynamic feature\n  that allows the user to see exactly which Data Access Units (datasets)\n  they are requesting data from.\n\n\n\n\n\n\n\n\n\n\nPage 8\n Signature and information of the Principal Investigator. \n\n\n\n\nThis must be signed by a Principal Investigator or a faculty-level supervisor on the project.\n\n\n\n\n\n\n\n\n\n\nPage 9\n Signature and information of all other applicants. \n\n\n\n\nThis should include any person who will have access to this data. They are legally bound to protecting and handling the data properly.\n\n\n\n\n\n\n\n\n\n\nPage 10\n Signature and information of Institutional or Administrative Authority. \n\n\n\n\nThis individual cannot be the same Principal Investigator that signed above, as this section is to provide a second-party authority of the instituion to ensure that the institution will uphold the terms of this agreement.\n\n\n\n\n\n\n\n\n\n\nPage 12\n Description of contemplated use of St. Jude data. \n\n\n\n\nHere, describe your research question and it's biological significance. The Contemplated Use will be evaluated by the Data Access Committees based on their own set of protocols. \n\n\nPlease contact us if you have any questions regarding the protocols of the approval process. \n\n\n\n\n\n\n\n\nData download permission\n\n\nAdditionally, \nif and only if\n you would like to \ndownload\n the data,\nyou will also need to include the following:\n\n\n\n\n\n\nPage 4\n The applicant's initials in Part 2.\n\n\n\n\n\n\n\n\nPage 11\n Signature and information of the Information Technology Director or Chief Information Security Officer.\n\n\n\n\n\n\n\n\nFrequently asked questions\n\n\nQ. Why do I need to sign the DAA?\n\n\nA.\n Although the DAA serves many purposes, the terms included in the data access\nagreement are ultimately in place to protect our patients. We take\npatient security very seriously, and we require that requestors are\ncommitted to protecting that privacy to the fullest extent.\n\n\nQ. Where can I find the latest version of the DAA?\n\n\nA.\n We keep \nour site\n up to date with the latest version on the Data Access Agreement for you to download, or you can download a copy\n\nhere\n. \n\n\nQ. Where do I submit the DAA??\n\n\nA.\n You can submit your Data Access Agreement in the drag and drop box on the last step of submitting your request.", 
            "title": "Making a Data Request"
        }, 
        {
            "location": "/guides/data/data-request/#introduction", 
            "text": "Creating a data request is the premier way to access raw St. Jude next \ngeneration sequencing data in the cloud. You can get a  free  copy of \nthe data in a secure cloud environment powered by Microsoft Azure and \nDNAnexus, or you can elect to download the data to your local computing \nenvironment.   Note  If you would like to download the data to local storage, there are\nextra steps you'll need to follow such as getting additional signatures\non your data access agreement. We recommend that you work with the data\nin the cloud if it's feasible; the data provided by St. Jude is free, the compute charges are reasonable, and working in the cloud helps to eliminate the long, error-prone downloading process. Porting your tools to be run in the cloud is easy, as well. We recommend you follow  this guide  to get started.", 
            "title": "Introduction"
        }, 
        {
            "location": "/guides/data/data-request/#requesting-data", 
            "text": "", 
            "title": "Requesting Data"
        }, 
        {
            "location": "/guides/data/data-request/#navigate-to-the-data-browser", 
            "text": "Navigate to the data browser by clicking Access Data and Explore\nData. If you would like to view the requests you have already made, you\ncan do so under Manage Data.", 
            "title": "Navigate to the Data Browser"
        }, 
        {
            "location": "/guides/data/data-request/#select-your-data", 
            "text": "Next, select the cohort you are interested in requesting. You can\nbrowse our data by disease, publication, dataset, or\nyou can use Pecan, our Pediatric Cancer portal, to find samples\nassociated with specific genes, or mutations.", 
            "title": "Select Your Data"
        }, 
        {
            "location": "/guides/data/data-request/#select-your-files", 
            "text": "Once you have selected the cohort you would like to study, you can chose what\ntype of files you would like to receive. This step is dynamic, and shows\nyou all of the file types we have available for the dataset you are\ninterested in.", 
            "title": "Select Your Files"
        }, 
        {
            "location": "/guides/data/data-request/#submit-your-request", 
            "text": "If you are requesting access to a dataset you have not already been approved for, \nyou will see a section called Controlled Data. Under this section, there is a bulleted list\nthat indicates the Data Access Units you need to request access to through our data\naccess agreement. Please use this list to fill in the Datasets section of the Data Request\nForm as shown in the figure below. For more information on filling out this guide,\nsee  Filling out the DAA .   Now, you have successfully submitted your data request. Your request\nwill be sent to the respective data access committees for evaluation.\nYou should expect to hear from us within a week or two on average.", 
            "title": "Submit Your Request"
        }, 
        {
            "location": "/guides/data/data-request/#data-access-agreement", 
            "text": "Broadly speaking, the data access agreement (DAA) is a legal document\nbinding the requestor and the requestor's institution to terms about\nhow our data may be used. We do not negotiate the terms of this document\nunless terms are found to be in conflict with the institution's state law. \nFilling out the Data Access Agreement carefully and completely is crucial \nto having your request filled promptly.", 
            "title": "Data access agreement"
        }, 
        {
            "location": "/guides/data/data-request/#filling-out-the-daa", 
            "text": "All Data Access Agreements require the following 5 items:    Page 5  The Data Access Unit(s) you are applying for must be marked.   These can be found in the section Controlled Data, above the Download Data Request\n  Form button. This can be found on the third step of the data request\n  process under the Controlled Data section. This is a dynamic feature\n  that allows the user to see exactly which Data Access Units (datasets)\n  they are requesting data from.      Page 8  Signature and information of the Principal Investigator.    This must be signed by a Principal Investigator or a faculty-level supervisor on the project.      Page 9  Signature and information of all other applicants.    This should include any person who will have access to this data. They are legally bound to protecting and handling the data properly.      Page 10  Signature and information of Institutional or Administrative Authority.    This individual cannot be the same Principal Investigator that signed above, as this section is to provide a second-party authority of the instituion to ensure that the institution will uphold the terms of this agreement.      Page 12  Description of contemplated use of St. Jude data.    Here, describe your research question and it's biological significance. The Contemplated Use will be evaluated by the Data Access Committees based on their own set of protocols.   Please contact us if you have any questions regarding the protocols of the approval process.", 
            "title": "Filling out the DAA"
        }, 
        {
            "location": "/guides/data/data-request/#data-download-permission", 
            "text": "Additionally,  if and only if  you would like to  download  the data,\nyou will also need to include the following:    Page 4  The applicant's initials in Part 2.     Page 11  Signature and information of the Information Technology Director or Chief Information Security Officer.", 
            "title": "Data download permission"
        }, 
        {
            "location": "/guides/data/data-request/#frequently-asked-questions", 
            "text": "Q. Why do I need to sign the DAA?  A.  Although the DAA serves many purposes, the terms included in the data access\nagreement are ultimately in place to protect our patients. We take\npatient security very seriously, and we require that requestors are\ncommitted to protecting that privacy to the fullest extent.  Q. Where can I find the latest version of the DAA?  A.  We keep  our site  up to date with the latest version on the Data Access Agreement for you to download, or you can download a copy here .   Q. Where do I submit the DAA??  A.  You can submit your Data Access Agreement in the drag and drop box on the last step of submitting your request.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/guides/data/data-access-units/", 
            "text": "Data Access Units\n\n\nWhat is a Data Access Unit (DAU)?\n\n\nA Data Access Unit is a grouping of data that typically corresponds to a dataset generated at the same time at the same institution, and can also correspond to a specific study. Each DAU has its own Data Access Committee, which contains the researchers who reside over the data. Each Data Access Committee has its own protocols for approving access to their DAU. If you have questions about committee approval protocols, please Contact Us. Basic clinical data is available for relevant subjects in each DAU. We currently have 3 DAUs. \n\n\nPediatric Cancer Genome Project (PCGP)\n\n\nPCGP is a paired-tumor normal dataset focused on discovering the genetic origins of pediatric cancer.\n\nThe Pediatric Cancer Genome Project is a collaboration between St. Jude Children's Research Hospital and the McDonnell Genome Institute at Washington University School of Medicine that sequenced the genomes of over 600 pediatric cancer patients. \n\n\nSt. Jude Lifetime (SJLIFE)\n\n\nSJLIFE is a germline-only dataset focused on studying the long-term adverse outcomes associated with cancer and cancer-related therapy.\n\nSt. Jude Lifetime (SJLIFE) is a longevity study from St. Jude Children's Research Hospital that aims to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy. This cohort contains unpaired germline samples and does not contain tumor samples. \n\n\nClinical Genomics (Clinical Pilot and G4K)\n\n\nClinical Genomics is a paired tumor-normal dataset focused on identifying variants that influence the development and behavior of childhood tumors.\n\nClinical Genomics is a cohort from St. Jude Children's Research Hospital, comprised of two studies: Clinical Pilot and Genomes4Kids. Clinical Pilot is a smaller, pilot study generated to asses the validity and accuracy of moving forward with the G4K study. These studies aim to identify all inherited and tumor-acquired (somatic) genome sequence and structural variants influencing the development and behavior of childhood tumors. \n\n\nSickle Cell Genome Project (SGP)\n\n\nSGP is a germline-only dataset of Sickle Cell Disease (SCD) patients from birth to young adulthood.\n\nThe Sickle Cell Genome Project (SGP) is a collaboration between St. Jude Children\u2019s Research Hospital and Baylor College of Medicine focused on identifying genetic modifiers that contribute to various health complications in SCD patients. Additional objectives include, but are not limited to, developing accurate methods to characterize germline structural variants in highly homologous globin locus and blood typing.", 
            "title": "Data Access Units"
        }, 
        {
            "location": "/guides/data/data-access-units/#data-access-units", 
            "text": "", 
            "title": "Data Access Units"
        }, 
        {
            "location": "/guides/data/data-access-units/#what-is-a-data-access-unit-dau", 
            "text": "A Data Access Unit is a grouping of data that typically corresponds to a dataset generated at the same time at the same institution, and can also correspond to a specific study. Each DAU has its own Data Access Committee, which contains the researchers who reside over the data. Each Data Access Committee has its own protocols for approving access to their DAU. If you have questions about committee approval protocols, please Contact Us. Basic clinical data is available for relevant subjects in each DAU. We currently have 3 DAUs.", 
            "title": "What is a Data Access Unit (DAU)?"
        }, 
        {
            "location": "/guides/data/data-access-units/#pediatric-cancer-genome-project-pcgp", 
            "text": "PCGP is a paired-tumor normal dataset focused on discovering the genetic origins of pediatric cancer. \nThe Pediatric Cancer Genome Project is a collaboration between St. Jude Children's Research Hospital and the McDonnell Genome Institute at Washington University School of Medicine that sequenced the genomes of over 600 pediatric cancer patients.", 
            "title": "Pediatric Cancer Genome Project (PCGP)"
        }, 
        {
            "location": "/guides/data/data-access-units/#st-jude-lifetime-sjlife", 
            "text": "SJLIFE is a germline-only dataset focused on studying the long-term adverse outcomes associated with cancer and cancer-related therapy. \nSt. Jude Lifetime (SJLIFE) is a longevity study from St. Jude Children's Research Hospital that aims to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy. This cohort contains unpaired germline samples and does not contain tumor samples.", 
            "title": "St. Jude Lifetime (SJLIFE)"
        }, 
        {
            "location": "/guides/data/data-access-units/#clinical-genomics-clinical-pilot-and-g4k", 
            "text": "Clinical Genomics is a paired tumor-normal dataset focused on identifying variants that influence the development and behavior of childhood tumors. \nClinical Genomics is a cohort from St. Jude Children's Research Hospital, comprised of two studies: Clinical Pilot and Genomes4Kids. Clinical Pilot is a smaller, pilot study generated to asses the validity and accuracy of moving forward with the G4K study. These studies aim to identify all inherited and tumor-acquired (somatic) genome sequence and structural variants influencing the development and behavior of childhood tumors.", 
            "title": "Clinical Genomics (Clinical Pilot and G4K)"
        }, 
        {
            "location": "/guides/data/data-access-units/#sickle-cell-genome-project-sgp", 
            "text": "SGP is a germline-only dataset of Sickle Cell Disease (SCD) patients from birth to young adulthood. \nThe Sickle Cell Genome Project (SGP) is a collaboration between St. Jude Children\u2019s Research Hospital and Baylor College of Medicine focused on identifying genetic modifiers that contribute to various health complications in SCD patients. Additional objectives include, but are not limited to, developing accurate methods to characterize germline structural variants in highly homologous globin locus and blood typing.", 
            "title": "Sickle Cell Genome Project (SGP)"
        }, 
        {
            "location": "/guides/data/run-your-tools/", 
            "text": "You can follow \nthis guide\n to request access to\nSt. Jude data in a secure cloud environment. Before you can begin writing your\nown tools to run on our data, you'll need to understand a bit about how\ndata vending in St. Jude Cloud works. Behind the scenes, the \nDNAnexus\n genomic ecosystem is the backbone for the computation\nand storage in St. Jude Cloud. Each data request in St. Jude Cloud corresponds to a project in DNAnexus. We'll explain what this means below, but if you're so inclined, you can read an introduction to their ecosystem \nhere\n. \n\n\nAccessing your data\n\n\nOnce your data access request is approved, the data you requested from St. Jude will automatically be distributed to a DNAnexus project with the same name as your data request. You can go to your \nManage Data\n page to see the requests you have submitted and go directly to your data. \n\n\nUsing Our Data\n\n\nSt. Jude Cloud offers data and tools for you to use. However, many researchers are interested in using out data in combination with their data, so you can upload your own data and tools to your cloud project. To do this, we recommend using the \nData Transfer App\n, or you can use the \ncommand line\n. Anything you upload to St. Jude Cloud will be uploaded to your private, secure project in DNAnexus. \n\n\nFrequently asked questions\n\n\nQ. Can I submit my data to St. Jude Cloud?\n\n\nA.\n At this time, St. Jude Cloud does not accept data submissions from other institutions. You can upload your data to your private, secure project folder, but that is not shared with St. Jude Cloud.", 
            "title": "Working with our Data"
        }, 
        {
            "location": "/guides/data/run-your-tools/#accessing-your-data", 
            "text": "Once your data access request is approved, the data you requested from St. Jude will automatically be distributed to a DNAnexus project with the same name as your data request. You can go to your  Manage Data  page to see the requests you have submitted and go directly to your data.", 
            "title": "Accessing your data"
        }, 
        {
            "location": "/guides/data/run-your-tools/#using-our-data", 
            "text": "St. Jude Cloud offers data and tools for you to use. However, many researchers are interested in using out data in combination with their data, so you can upload your own data and tools to your cloud project. To do this, we recommend using the  Data Transfer App , or you can use the  command line . Anything you upload to St. Jude Cloud will be uploaded to your private, secure project in DNAnexus.", 
            "title": "Using Our Data"
        }, 
        {
            "location": "/guides/data/run-your-tools/#frequently-asked-questions", 
            "text": "Q. Can I submit my data to St. Jude Cloud?  A.  At this time, St. Jude Cloud does not accept data submissions from other institutions. You can upload your data to your private, secure project folder, but that is not shared with St. Jude Cloud.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/guides/data/data-transfer-app/", 
            "text": "Note\n\n\nWe consider the St. Jude Cloud data transfer application to be production ready.\nHowever, we have not yet completed the guide on how to use it. This guide will likely be\ncompleted soon. In the meantime, we hope that you will try to use the app and let us\nknow your feedback at our \ncontact us form\n.\n\n\n\n\nYou can \nclick here\n to download the latest release of the\nSt. Jude Cloud data transfer application. If you are interested in viewing the source code, \nyou can do so \nhere\n. If you\nwould like to file an issue you are experiencing with the application,\nyou can do so \nhere\n.\nFor users looking to upload/download files on the command line, please refer to the \ncommand line interaction guide\n.", 
            "title": "Data Transfer Application"
        }, 
        {
            "location": "/guides/data/command-line/", 
            "text": "Getting started\n\n\nBefore you begin interacting with St. Jude Cloud Platform from the\ncommand line, you'll need to understand some details on the underlying\narchitecture of the platform. The St. Jude Cloud Platform is built on\ntop of a genomics cloud ecosystem provided by \nDNAnexus\n. \nFor a comprehensive overview of how DNAnexus works, see \n\nthis page\n.\n\n\nOverview\n\n\nWorkspaces in DNAnexus are organized by projects, which are essentially\nfolders in the cloud. Each data request and tool in St. Jude Cloud\ncreates its own unique cloud workspace (DNAnexus project). For instance,\na data request creates a DNAnexus project behind the scenes with the\nsame name as the request name you specify when you request data.\n\n\nInstallation\n\n\nOpen-source software provided by DNAnexus called the \ndx-toolkit\n is\nused to interact with the St. Jude Cloud Platform from the command line.\nYou can use this to create these projects, upload and download data, and\nmany other operations. You'll need to install that software on your\ncomputer by following \nthis guide\n.\n\n\n\n\nTip\n\n\nA quickstart to getting up and running with the dx-toolkit:\n\n\n\n\nInstall Python 2.7.13+. Note that using the system-level Python is\n    usually not a good idea (by default, system level Python is\n    typically too old/does not support the latest security protocols\n    required). You can install using \nAnaconda\n (recommended) or using\n    the default \nPython installer\n.\n\n\nRun \npip install dxpy\n.\n\n\nType \ndx --help\n at the command line.\n\n\n\n\n\n\nA quick tour\n\n\nLogging in\n\n\nTo log in using the dx-toolkit, run the following command:\n\n\ndx login --noprojects\n\n# enter username and password when prompted\n\n\n\n\n\n\n\n\nNote\n\n\nIf you are a St. Jude employee, you'll need to follow \nthis\nguide\n to log in instead.\n\n\n\n\nSelecting a project\n\n\nFirst, you'll need to choose which cloud workspace you would like to\naccess. This depends on if you are downloading data from a request or\nworking with input/output files from a tool. You can see the workspaces\navailable to you by running the following command in your terminal:\n\n\ndx \nselect\n\n\n\n\n\n\nThis will present with a prompt similar to the below screenshot. A list\nof your available cloud workspaces will be shown with a number out to\nthe left of each. You should enter the number corresponding to the\nworkspace you are wanting to interact with. In the example below, the\nuser has selected the Rapid RNA-Seq tool.\n\n\n\n\nSome useful commands\n\n\nMoving data back and forth between the cloud and your local computer is\nsimple once you have selected the correct project for your tool.\n\n\nYou will find that many common linux commands with \ndx\n prepended.\n\n\n# list available files for the tool for the main folder\n\ndx ls\n\n\n# list all available files for the tool\n\ndx find .\n\n\n# list all commands\n\ndx --help\n\n\n\n\n\nUploading data\n\n\nYou can use the following process to upload data to be used by St. Jude\nCloud Platform tools:\n\n\n\n\n\n\nFirst, click \"View\" on the tool you'd like to run from \nthis\n    page\n. In this example, we will\n    choose the Rapid RNA-Seq tool.\n\n\n\n\n\n\nIf you have not already, click \"Start\" on the tool you'd like to\n    run. This will create a cloud workspace for you to upload your\n    data to with the same name as the tool.\n\n\n\n\n\n\n\n\nOpen up your terminal application and select the cloud workspace\n    with the same name as the tool you are trying to run.\n\n\n\n\n\n\n\n\nLast, navigate to the local files you'd like to upload to the cloud\n    and use the \ndx upload\n command as specified in\n    [upload-download-data]{role=\"ref\"} to upload your data to St. Jude\n    Cloud.\n\n\n\n\n\n\n\n\nDownloading data\n\n\n\n\nWarning\n\n\nTo download data from a St. Jude Cloud data request, you must have\nindicated that you wished to download the data in your Data Access\nAgreement (DAA) during your submission. Any downloading of St. Jude data\nwithout completing this step is strictly PROHIBITED.\n\n\n\n\nYou can use the following steps to download data from a St. Jude Cloud\ndata request:\n\n\n\n\n\n\nComplete a data request using the St. Jude Cloud Platform. In this\n    example, we've created a request with the name \"Retinoblastoma\n    Data\".\n\n\n\n\n\n\n\n\nOpen up your terminal application and select the cloud workspace\n    relevant to your data request. For instance, in this case we\n    would type \ndx \nselect\n \nRetinoblastoma Data\n.\n\n\n\n\n\n\n\n\nYou can use typical commands like \ndx ls\n,\n    \ndx \npwd\n, and \ndx \ncd\n to navigate around\n    your cloud folder as you would a local folder. Your project may look\n    different based on what data you requested and whether you were\n    previously approved to access the data. Your data should either be\n    in the \nrestricted\n folder (if this is your first time\n    requesting access) or the \nimmediate\n folder (if you\n    were previously granted access permission).\n\n\n\n\n\n\n\n\nIn the root of every data request is a file called\n    \nSAMPLE_INFO.txt\n. This should contain all of the\n    information about the samples you checked out as well as the\n    associated metadata we provide.\n\n\n\n\n\n\nTo download data from the cloud to local storage, use the\n    \ndx download\n command as specified in\n    [upload-download-data]{role=\"ref\"}. For instance, if I wanted to\n    download all of the BAM files to my local computer, I would type\n    \ndx download immediate/bam/*\n.", 
            "title": "Command Line Interaction"
        }, 
        {
            "location": "/guides/data/command-line/#getting-started", 
            "text": "Before you begin interacting with St. Jude Cloud Platform from the\ncommand line, you'll need to understand some details on the underlying\narchitecture of the platform. The St. Jude Cloud Platform is built on\ntop of a genomics cloud ecosystem provided by  DNAnexus . \nFor a comprehensive overview of how DNAnexus works, see  this page .", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/data/command-line/#overview", 
            "text": "Workspaces in DNAnexus are organized by projects, which are essentially\nfolders in the cloud. Each data request and tool in St. Jude Cloud\ncreates its own unique cloud workspace (DNAnexus project). For instance,\na data request creates a DNAnexus project behind the scenes with the\nsame name as the request name you specify when you request data.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/data/command-line/#installation", 
            "text": "Open-source software provided by DNAnexus called the  dx-toolkit  is\nused to interact with the St. Jude Cloud Platform from the command line.\nYou can use this to create these projects, upload and download data, and\nmany other operations. You'll need to install that software on your\ncomputer by following  this guide .   Tip  A quickstart to getting up and running with the dx-toolkit:   Install Python 2.7.13+. Note that using the system-level Python is\n    usually not a good idea (by default, system level Python is\n    typically too old/does not support the latest security protocols\n    required). You can install using  Anaconda  (recommended) or using\n    the default  Python installer .  Run  pip install dxpy .  Type  dx --help  at the command line.", 
            "title": "Installation"
        }, 
        {
            "location": "/guides/data/command-line/#a-quick-tour", 
            "text": "", 
            "title": "A quick tour"
        }, 
        {
            "location": "/guides/data/command-line/#logging-in", 
            "text": "To log in using the dx-toolkit, run the following command:  dx login --noprojects # enter username and password when prompted    Note  If you are a St. Jude employee, you'll need to follow  this\nguide  to log in instead.", 
            "title": "Logging in"
        }, 
        {
            "location": "/guides/data/command-line/#selecting-a-project", 
            "text": "First, you'll need to choose which cloud workspace you would like to\naccess. This depends on if you are downloading data from a request or\nworking with input/output files from a tool. You can see the workspaces\navailable to you by running the following command in your terminal:  dx  select   This will present with a prompt similar to the below screenshot. A list\nof your available cloud workspaces will be shown with a number out to\nthe left of each. You should enter the number corresponding to the\nworkspace you are wanting to interact with. In the example below, the\nuser has selected the Rapid RNA-Seq tool.", 
            "title": "Selecting a project"
        }, 
        {
            "location": "/guides/data/command-line/#some-useful-commands", 
            "text": "Moving data back and forth between the cloud and your local computer is\nsimple once you have selected the correct project for your tool.  You will find that many common linux commands with  dx  prepended.  # list available files for the tool for the main folder \ndx ls # list all available files for the tool \ndx find . # list all commands \ndx --help", 
            "title": "Some useful commands"
        }, 
        {
            "location": "/guides/data/command-line/#uploading-data", 
            "text": "You can use the following process to upload data to be used by St. Jude\nCloud Platform tools:    First, click \"View\" on the tool you'd like to run from  this\n    page . In this example, we will\n    choose the Rapid RNA-Seq tool.    If you have not already, click \"Start\" on the tool you'd like to\n    run. This will create a cloud workspace for you to upload your\n    data to with the same name as the tool.     Open up your terminal application and select the cloud workspace\n    with the same name as the tool you are trying to run.     Last, navigate to the local files you'd like to upload to the cloud\n    and use the  dx upload  command as specified in\n    [upload-download-data]{role=\"ref\"} to upload your data to St. Jude\n    Cloud.", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/data/command-line/#downloading-data", 
            "text": "Warning  To download data from a St. Jude Cloud data request, you must have\nindicated that you wished to download the data in your Data Access\nAgreement (DAA) during your submission. Any downloading of St. Jude data\nwithout completing this step is strictly PROHIBITED.   You can use the following steps to download data from a St. Jude Cloud\ndata request:    Complete a data request using the St. Jude Cloud Platform. In this\n    example, we've created a request with the name \"Retinoblastoma\n    Data\".     Open up your terminal application and select the cloud workspace\n    relevant to your data request. For instance, in this case we\n    would type  dx  select   Retinoblastoma Data .     You can use typical commands like  dx ls ,\n     dx  pwd , and  dx  cd  to navigate around\n    your cloud folder as you would a local folder. Your project may look\n    different based on what data you requested and whether you were\n    previously approved to access the data. Your data should either be\n    in the  restricted  folder (if this is your first time\n    requesting access) or the  immediate  folder (if you\n    were previously granted access permission).     In the root of every data request is a file called\n     SAMPLE_INFO.txt . This should contain all of the\n    information about the samples you checked out as well as the\n    associated metadata we provide.    To download data from the cloud to local storage, use the\n     dx download  command as specified in\n    [upload-download-data]{role=\"ref\"}. For instance, if I wanted to\n    download all of the BAM files to my local computer, I would type\n     dx download immediate/bam/* .", 
            "title": "Downloading data"
        }, 
        {
            "location": "/guides/tools/pecan-pie/", 
            "text": "Pecan PIE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthors\n\n\nMichael Edmonson, Aman Patel\n\n\n\n\n\n\nPublication\n\n\nZhang et al., NEJM 2015\n, supplementary appendix pp. 7-10\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nPecan PIE (the \nPe\ndiatric \nCan\ncer Variant \nP\nathogenicity\n\nI\nnformation \nE\nxchange) is a cloud-based variant classification\nand interpretation service. It annotates and ranks variants by putative\npathogenicity, then displays them in an interactive web interface for\nformal review and classification following ACMG guidelines. The portal\nalso contains a repository of expert-reviewed germline mutations that\nmay predispose individuals to cancer. It is free for non-commercial use.\n\n\nPecan PIE utilizes St. Jude Medal Ceremony, the same pipeline that\npowers our clinical and research genomics projects. Medal Ceremony\nprovides a 3-level ranking of putative pathogenity - Gold, Silver or\nBronze - for mutations within disease-related genes. Medal assignment is\nbased on matches to 22 mutation databases, mutation type, population\nfrequency, tumor suppressor status and predicted functional impact. The\nevidence used for medal assignment is imported into an interactive\nvariant review page where an analyst can enter additional curated\ninformation such as primary diagnosis, presence of subsequent neoplasm,\nfamily history and related literature. Classification tags can be\nassigned to curated data enabling automated calculation of pathogenicity\nrating based on ACMG/AMP 2015 guidelines.\n\n\nSee \nfile_download\n PowerPoint slides\n\npresented at the ASHG 2017 annual meeting (note that some of this\ninformation is out of date, various improvements have been made since\nthen).\n\n\nGo to \nhttps://pecan.stjude.cloud/pie\n to get started!\n\n\nOverview\n\n\n\n\nAn overview of the Pecan PIE workflow:\n\n\n\n\nLog in and upload a VCF of SNVs and indels.\n\n\nThe portal will process your variants, notifying you upon\n    completion. Variants are annotated with VEP+ (VEP with\n    postprocessing for enhanced splice variant calling) then classified\n    with Medal Ceremony.\n\n\nBrowse results, which include a detailed page for each variation.\n    Variants may be formally classified with an interface based on ACMG\n    guidelines.\n\n\n\n\nGetting started\n\n\nStart by logging into the portal with a DNAnexus account, creating an\naccount if you need one. PIE uses DNAnexus as a secure cloud backend.\nLogging in is required for private storage of your data and so that we\ncan send you e-mail notifications when your analysis jobs are complete.\nPIE is free for non-commercial use. St. Jude pays the (small) cloud\ncomputing costs, your DNAnexus account will not be billed.\n\n\nUploading data\n\n\nPecan PIE takes standard VCF files as input, which may be either\nuncompressed or compressed with \nbgzip\n.\n\n\n\n\nClick the \"Securely upload a VCF file\" button.\n\n\nChoose the genome your variants were mapped to, which may be either\n   GRCh37-lite or GRCh38.\n\n\n\n\nAdvanced options\n\n\nThe \"Advanced option\" panel lets you customize the behavior of the\npipeline:\n\n\n\n\nGene list: Pick a gene list from the pulldown. This filters your\n  variants to genes in the specified list. This option is required and turned\n  on automatically if your uploaded file is 2 megabytes or larger. See the\n  \nfrequently asked questions\n for more\n  information. This option reduces the variant processing burden on PIE by\n  removing variants that will not be assigned a medal in any case because\n  they are not on the cancer predisposition gene list. You can review the\n  genes by clicking on the link that will appear just below the pull down\n  titled \"See gene list\".\n\n\nCustom gene list: Choosing \"custom\" as your gene list will open a\n  window that will let you paste in a list of genes. Any invalid genes will\n  be dropped from your list automatically. You can separate your genes by spaces\n  or new lines.\n\n\nMax Population frequency: PIE by default will not call medals for\n  variants present in the ExAC (ex-TCGA) database at an allele\n  frequency greater than 0.001. This option lets you override the\n  filtering threshold to whatever frequency you prefer. To disable\n  filtering altogether, specify a value of 1.\n\n\n\n\nProgress page\n\n\nAfter uploading is complete you will be taken to a status screen showing\nthe progress of your job through the system. Analysis typically takes\n10-15 minutes depending on file size and system availability.\n\n\nIt isn't necessary to keep your browser open on this page until your\nresults are ready: the system will e-mail you with a link to return to\nyour results. Optional browser notifications are also available.\n\n\nAnalysis of Results\n\n\nResults browser\n\n\nWhen your job is complete you will be taken to an overview page where\nyou can browse your results and examine a detailed results page for each\nvariant.\n\n\n\n\nThe variants in the results can be filtered by:\n\n\n\n\n\n\n\n\nFilter\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nClass\n\n\nPredicted effect of variant on protein coding, e.g. missense, nonsense, etc.\n\n\n\n\n\n\nSomatic medal\n\n\nMedal assigned to the variant by the somatic classifier.\n\n\n\n\n\n\nGermline medal\n\n\nMedal assigned to the variant by the germline classifier.\n\n\n\n\n\n\nCommittee Classification\n\n\nIf the variant has been reviewed by the St. Jude germline variant review committee, the result will appear in this column, otherwise it will be blank.\n\n\n\n\n\n\n\n\nThe \"search\" box lets you filter the results by gene and/or amino acid\nchange. The view is dynamically filtered to matching variants as you\ntype.\n\n\nMedal meaning\n\n\nMedals are only assigned for coding and splice-related variants in\ndisease predisposition genes. Germline medals are only assigned for\nnovel variants or those present in the ExAC (ex-TCGA) database with a\nMAF no greater than 0.1% (0.001 expressed fractionally).\n\n\n\n\n\n\nGold medals\n are assigned to truncations in tumor suppressor genes,\nhotspots derived from the COSMIC database, as well as perfect matches to\nvariants in the IARC TP53, PCGP, ASU TERT, ARUP RET, and BIC databases.\n\n\nSilver medals\n are assigned to in-frame indels, truncations in non-tumor\nsuppressor genes, variants predicted deleterious by damage-prediction\nalgorithms, variants receiving a gold medal from the somatic classifier,\nand perfect matches to variants in the following databases: ClinVar\n(predicted pathogenic or likely pathogenic), RB1, LOVD, and UMD.\n\n\nBronze medals\n are assigned to variants predicted tolerated by\ndamage-prediction algorithms. Variants having an imperfect match to a\ndatabase (i.e. different variants at the same genomic position or codon)\ntypically receive a lesser medal.\n\n\n\n\nA summary graphic can be found in slide 4 of the ASHG 2017 presentation (\ndownload here\n). \nFor additional details see \nZhang et al., NEJM\n2015\n\n(supplementary appendix pp. 7-10).\n\n\nVariant page\n\n\nEach variant links to a detailed variant page, which integrates data\nfrom a variety of sources. If either you or the St. Jude germline\nvariant review committee have annotated a variant, that information will\nbe pre-populated.\n\n\nSummary information\n\n\nThe top of the page shows a summary of the variant, including its\ngenomic and HGVS annotations, predicted effect on the protein, and\nsomatic and germline medals. A description of the gene from Entrez\nfollows, and a custom description or selection rationale may also be\nentered.\n\n\nMedal call information\n\n\nClicking on one of the medal icons (gold, silver, bronze, unknown) also\non the top of the page will show a summary of information related to the\nmedal call.\n\n\nProteinPaint\n\n\nAn embedded version of ProteinPaint (\nZhou et al., Nat. Genet.\n2016\n) appears next, showing\nthe variant in the context of a number of pediatric datasets including\nPCGP. A link is provided to the main ProteinPaint application which\nprovides visualizations for additional datasets, including COSMIC and\nClinVar.\n\n\nASHG pathogenicity classification\n\n\nFormal variant pathogenicity classification is supported by an interface\nimplementing ACMG guidelines (\nRichards et al., Genet Med.\n2015\n).\nThe analyst reviews a series of curated category tags, assigning\napplicable tags to the variant and optionally supplying additional\ninformation for each such as PubMed IDs and supporting evidence. The\nsystem will then compute an appropriate pathogenicity score based on the\nuser-flagged categories. Additional free-form custom evidence can also\nbe entered. This structured approach both helps eliminate arbitrary\ndecision-making from the pathogenicity classification process and also\nconstructs a concise summary of the logic and evidence supporting the\nfinal call.\n\n\nClinVar and allele frequency\n\n\nMatches of the variant in ClinVar are also provided, along with\npredicted clinical significance and review status.\n\n\nAllele frequencies for the variant in the PCGP (somatic and\ngermline),NHLBI ESP 6500, and ExAC databases are presented both as\nfractional values and on a log10 plot. Detailed allele population\nbreakdowns are provided for ExAC.\n\n\nDamage prediction algorithms\n\n\nPrecomputed damage-prediction algorithm calls for nonsynonymous coding\nSNVs are presented from the dbNSFP database. Available algorithms are\nPolyPhen2 (HVAR), SIFT, CADD, REVEL, FATHMM, MutationAssessor, and LRT.\nThe calls are presented in a circular diagram with entries color-coded\nbased on the predicted severity of the result.\n\n\nMedal ceremony and linkouts\n\n\nAdditional output from medal ceremony classification can also be\nreviewed. This is only loosely structured, additional fields here may\neventually be integrated into Pecan PIE.\n\n\nLinks are provided to relevant dbSNP entries and other information\nsources.\n\n\nFinal classification\n\n\nThe final 5-tier ACMG classification can be selected after which the\ndecision will be marked as reviewed. A checkbox is also available to\nindicate this variant is a potential candidate for functional review.\n\n\nStandalone usage\n\n\nThis section is intended only for users who want to invoke Pecan PIE's\nunderlying analysis pipelines independently on the\n\nDNAnexus\n platform. If you just want to use\nthe Pecan PIE website you can safely ignore this section of the\ndocumentation. We assume familiarity with the DNAnexus platform. If you aren't\nfamiliar with this, DNAnexus' \nquickstart guide\n is a\ngreat place to start.\n\n\n\n\nWarning\n\n\nThis section of the guide is only relevant to power users!\n\n\n\n\nTwo DNAnexus cloud application pipelines were created during the\ndevelopment of Pecan PIE:\n\n\n\n\n\n\n\n\nName\n\n\nCorresponding DNAnexus App\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVEP+\n\n\napp-stjude_vep_plus\n\n\nA cloud installation of \nVEP\n with improved logic for splice variant calls. Converts an input VCF of variants to annotated, tab-delimited format.\n\n\n\n\n\n\nMedal Ceremony\n\n\napp-stjude_medal_ceremony\n\n\nAdditional annotation and automated variant classification. Requires a special input format which is produced by VEP+.\n\n\n\n\n\n\n\n\nPermissions\n\n\nIn order to run the cloud pipelines independently, your DNAnexus account\nneeds to be granted permissions to access them. After your initial login\nto St. Jude Cloud and/or Pecan PIE, these permissions will be granted automatically. A single\nlogin is required even if you just want to use the standalone pipelines\nrather than the Pecan PIE portal (\ncontact\nus\n if you encounter problems accessing\nthe pipelines).\n\n\nThere are two methods of running pipelines on DNAnexus:\n\n\n\n\nDNAnexus GUI.\n DNAnexus provides a standardized graphical user interface for\n    configurating, launching, and monitoring jobs on the cloud. Our\n    pipelines can be run like any other DNAnexus pipeline.\n\n\nCommand line.\n Jobs may also be invoked via the \ndx\n command line\n    client. Command-line use allows submitting cloud jobs without\n    interacting with a GUI, and so supports scripting and easier\n    integration with local workflows. See \nthis section\n\n    for information on how to get set up with the \ndx-toolkit\n.\n\n\n\n\n\n\nNote\n\n\nThe following examples demonstrate command-line usage.\n\n\n\n\nUploading files\n\n\nAll input files must be uploaded onto the DNAnexus platform. When\nspecifying files for input you can use either the DNAnexus fie IDs (e.g.\n\nfile-FBgvp680gz1bGQ5p8yZKz69g\n), or the filenames if they are unique. For\nan idea of how to upload files to DNAnexus, see \nthis guide\n.\n\n\nStep 1: Running VEP+\n\n\nTo run the VEP+ DNAnexus app, you can use the following \ndx\n\ncommand with your own inputs in place of the example's:\n\n\ndx run app-stjude_vep_plus -iinput_file\n=\nmy_vcf.vcf -igenome_string\n=\nGRCh37-lite -igermline_reviewable_only\n=\ntrue\n\n\n\n\n\n\n\n\nTip\n\n\n\n\ngenome_string\n must be either \nGRCh37-lite\n or \nGRCh38\n. If \nGRCh38\n\nis specified, variants will be lifted over to \nGRCh37-lite\n in output,\ni.e. the output will always be \nGRCh37-lite\n (Medal Ceremony currently only supports \nGRCh37-lite\n).\n\n\nThe input VCF specified by \ninput_file\n may be either\nuncompressed, or compressed with \nbgzip\n \nonly\n (htslib/tabix\npackages).\n\n\nThe \ngermline_reviewable_only\n parameter is optional, but\nstrongly recommended. If specified, only variants in disease-gene\nrelated intervals will be annotated, which is appropriate for\nMedal Ceremony. If this option is not specified all variants will\nbe annotated, which depending on the size of your VCF might take a\nlot longer, and many of the resulting variants won't be usable by\nMedal Ceremony. If you want to do this anyway and have a large\nnumber of variants, consider submitting your job to an instance\nwith more CPU cores (e.g. \nmem1_ssd1_x16\n or \nmem1_ssd1_x32\n) as\nthe code will take advantage of the additional cores. If you are\nusing a custom gene list (below) that takes precedence and this\nparameter is not needed.\n\n\nThe optional parameter \ncustom_genes_file\n specifies a plain\ntext file of HUGO gene symbols to analyze (whitespace separated,\nor one per line). If specified, analysis will be restricted to\nthese genes only.\n\n\nThis pipeline produces two output files, \noutput_file\n contains\nannotations for all variants, while \nmedal_prep_output_file\n is\nthe specially-filtered and formatted file required as input to\nMedal Ceremony below.\n\n\n\n\n\n\nStep 2: Running Medal Ceremony\n\n\nTo run the medal ceremony DNAnexus app, you can use the following \ndx\n\ncommand with your own inputs in place of the example's:\n\n\ndx run app-stjude_medal_ceremony -iinfile\n=\nmedal_prep_output_file\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe optional parameter \ncustom_genes_file\n operates in the same\nway as in the VEP+ pipeline above. For custom gene lists to work\nproperly this parameter must be specified when running both the\nVEP+ and Medal Ceremony pipelines.\n\n\nThe optional parameter \nmax_population_frequency\n may be\nspecified, a fractional value representing the maximum population\nfrequency allowed for a variant in the ExAC (ex-TCGA) database to\nreceive a medal. The default is 0.001, a.k.a. \".1%\".\n\n\n\n\n\n\nFrequently asked questions\n\n\nIf you have any questions not covered here, feel free to reach out on\n\nour contact form\n.\n\n\nQ: Which files are supported?\n\n\nPIE works with variants in VCF format:\n\n\n\n\nUploaded files must be compliant with the \nVCF specification\n.\n\n\nVCF files may be either uncompressed, or compressed with \nbgzip\n \nonly\n. \nbgzip\n is part of the htslib/tabix packages (see below).\n\n\n\n\nImproperly formatted VCF files will not work with PIE. Some common\nproblems include:\n\n\n\n\nMissing header line\n\n\nMissing required columns\n\n\nFiles compressed by any method other than \nbgzip\n (\ngzip\n, \nzip\n, or any other program)\n\n\n\n\nTo verify compatibility of your VCF you can try one of these methods:\n\n\n\n\nCompressing your VCF with\n   \nbgzip\n and indexing it with\n   \ntabix\n, both programs from\n   the \nHTSlib\n package (some systems also\n   use the earlier, pre-HTSlib \"tabix\" package). This process will\n   only succeed for compliant VCF files, and can help diagnose\n   failures.\n\n\nRunning \"vcf-validator\" program from the\n   \nvcftools\n package.\n\n\n\n\nWhile the VCF specification also requires that variants be sorted by\nchromosome name and position, PIE is now often able to automatically\ncorrect sorting issues in uploaded files. PIE requires sorted data in\norder to query data for targeted genes.\n\n\nQ: Are there limits on the size of VCF files?\n\n\nUploaded files must not exceed 4 gigabytes. If an uploaded file is larger\nthan 2 megabytes, the cancer predisposition gene list filter will be\nautomatically enabled unless you are using a custom gene list. This reduces\nthe processing burden on the system by removing variants outside of targeted\ngenes.\n\n\nQ: Is there an example/demo VCF I can try with PIE?\n\n\nA. You can use \nthis VCF\n\nfrom the Genome in a Bottle project. This ~133 megabyte\nbgzip-compressed VCF was used during testing of Pecan PIE and is known\nto work. These variants are mapped to GRCh37.\n\n\nQ. What genome versions are supported?\n\n\nA. Pecan PIE will accept variants mapped to either GRCh37-lite/hg19 or GRCh38.\nGRCh38 variants are automatically lifted over to 37, as the system\nuses 37 internally; the liftover process is able to compensate for\nstrand and reference/variant allele swaps which can occur. A native\nhg38 version is in development, but is not yet available.\n\n\nPecan PIE only works for human data.\n\n\nQ. What genes are on the curated gene list?\n\n\nA. The list consists of disease-related genes, both cancer and\nnon-cancer, see the \nfile_download\n Excel spreadsheet\n\nfor details. Filtering the source variants to a target list of genes\nreduces the processing burden on the system.\n\n\nWhen browsing the results the view may be filtered to disease\nsub-categories of interest.\n\n\nYou can also specify your own custom list of genes to process when\nsubmitting your VCF file (see the advanced options panel).\n\n\nQ. Why is the classification column blank in my results?\n\n\nQ. This column displays the classification assigned by the St. Jude\nGermline Committee reviewers. If a variant was not classified by this\ncommittee before, this field will be blank.\n\n\nPecan PIE provides classifications from the Medal Ceremony pipeline,\nwhich may assign variants gold, silver, or bronze medals. An \"Unknown\"\nmedal may be assigned for non-disease-predisposition genes, variants\npresent in the ExAC (ex-TCGA) database at an allele frequency \n\n0.1%, or variants without functional annotations (which includes most\nsilent variants).\n\n\nQ. What do the medals mean?\n\n\nA. The medal column is a rough indicator of the likelihood of the variant\nbeing clinically significant as predicted by the medal ceremony\nsoftware. Variants with gold medals are most likely to be significant,\nand those with no medal are least likely. More details can be found in\nthe \nAnalysis of Results \nresults\n\nsection.\n\n\nQ. Why are some of my variants missing?\n\n\nA. Currently only coding and splice-related variants in disease-related\ngenes make it to the medaling process. Intergenic, intronic, and UTR\nvariants are excluded, as are those in non-coding transcripts.\n\n\nQ. Why does the ExAC allele frequency shown differ from the ExAC portal?\n\n\nA. The reported ExAC frequency may differ for several reasons:\n\n\n\n\nPIE uses the TCGA-subtracted distribution of ExAC rather than the\n    main distribution.\n\n\nPIE reports the primary allele frequencies in the ExAC database,\n    specifically the AC, AN, and AF fields from the VCF distribution.\n    The \nExAC portal\n appears to use\n    the \"adjusted\" frequencies which may be different.\n\n\n\n\nQ. Is Pecan PIE free?\n\n\nA. Pecan PIE is free for non-commercial use. St. Jude covers the cost of\nrunning the pipeline and hosting. DNANexus accounts are required to\nkeep track of your jobs in the cloud so that you can retrieve and\nmanage from multiple locations. Accounts also make it possible to\nalert you of job completion via email.", 
            "title": "Pecan PIE"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#pecan-pie", 
            "text": "Authors  Michael Edmonson, Aman Patel    Publication  Zhang et al., NEJM 2015 , supplementary appendix pp. 7-10    Technical Support  Contact Us     Pecan PIE (the  Pe diatric  Can cer Variant  P athogenicity I nformation  E xchange) is a cloud-based variant classification\nand interpretation service. It annotates and ranks variants by putative\npathogenicity, then displays them in an interactive web interface for\nformal review and classification following ACMG guidelines. The portal\nalso contains a repository of expert-reviewed germline mutations that\nmay predispose individuals to cancer. It is free for non-commercial use.  Pecan PIE utilizes St. Jude Medal Ceremony, the same pipeline that\npowers our clinical and research genomics projects. Medal Ceremony\nprovides a 3-level ranking of putative pathogenity - Gold, Silver or\nBronze - for mutations within disease-related genes. Medal assignment is\nbased on matches to 22 mutation databases, mutation type, population\nfrequency, tumor suppressor status and predicted functional impact. The\nevidence used for medal assignment is imported into an interactive\nvariant review page where an analyst can enter additional curated\ninformation such as primary diagnosis, presence of subsequent neoplasm,\nfamily history and related literature. Classification tags can be\nassigned to curated data enabling automated calculation of pathogenicity\nrating based on ACMG/AMP 2015 guidelines.  See  file_download  PowerPoint slides \npresented at the ASHG 2017 annual meeting (note that some of this\ninformation is out of date, various improvements have been made since\nthen).  Go to  https://pecan.stjude.cloud/pie  to get started!", 
            "title": "Pecan PIE"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#overview", 
            "text": "An overview of the Pecan PIE workflow:   Log in and upload a VCF of SNVs and indels.  The portal will process your variants, notifying you upon\n    completion. Variants are annotated with VEP+ (VEP with\n    postprocessing for enhanced splice variant calling) then classified\n    with Medal Ceremony.  Browse results, which include a detailed page for each variation.\n    Variants may be formally classified with an interface based on ACMG\n    guidelines.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#getting-started", 
            "text": "Start by logging into the portal with a DNAnexus account, creating an\naccount if you need one. PIE uses DNAnexus as a secure cloud backend.\nLogging in is required for private storage of your data and so that we\ncan send you e-mail notifications when your analysis jobs are complete.\nPIE is free for non-commercial use. St. Jude pays the (small) cloud\ncomputing costs, your DNAnexus account will not be billed.", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#uploading-data", 
            "text": "Pecan PIE takes standard VCF files as input, which may be either\nuncompressed or compressed with  bgzip .   Click the \"Securely upload a VCF file\" button.  Choose the genome your variants were mapped to, which may be either\n   GRCh37-lite or GRCh38.", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#advanced-options", 
            "text": "The \"Advanced option\" panel lets you customize the behavior of the\npipeline:   Gene list: Pick a gene list from the pulldown. This filters your\n  variants to genes in the specified list. This option is required and turned\n  on automatically if your uploaded file is 2 megabytes or larger. See the\n   frequently asked questions  for more\n  information. This option reduces the variant processing burden on PIE by\n  removing variants that will not be assigned a medal in any case because\n  they are not on the cancer predisposition gene list. You can review the\n  genes by clicking on the link that will appear just below the pull down\n  titled \"See gene list\".  Custom gene list: Choosing \"custom\" as your gene list will open a\n  window that will let you paste in a list of genes. Any invalid genes will\n  be dropped from your list automatically. You can separate your genes by spaces\n  or new lines.  Max Population frequency: PIE by default will not call medals for\n  variants present in the ExAC (ex-TCGA) database at an allele\n  frequency greater than 0.001. This option lets you override the\n  filtering threshold to whatever frequency you prefer. To disable\n  filtering altogether, specify a value of 1.", 
            "title": "Advanced options"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#progress-page", 
            "text": "After uploading is complete you will be taken to a status screen showing\nthe progress of your job through the system. Analysis typically takes\n10-15 minutes depending on file size and system availability.  It isn't necessary to keep your browser open on this page until your\nresults are ready: the system will e-mail you with a link to return to\nyour results. Optional browser notifications are also available.", 
            "title": "Progress page"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#analysis-of-results", 
            "text": "", 
            "title": "Analysis of Results"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#results-browser", 
            "text": "When your job is complete you will be taken to an overview page where\nyou can browse your results and examine a detailed results page for each\nvariant.   The variants in the results can be filtered by:     Filter  Meaning      Class  Predicted effect of variant on protein coding, e.g. missense, nonsense, etc.    Somatic medal  Medal assigned to the variant by the somatic classifier.    Germline medal  Medal assigned to the variant by the germline classifier.    Committee Classification  If the variant has been reviewed by the St. Jude germline variant review committee, the result will appear in this column, otherwise it will be blank.     The \"search\" box lets you filter the results by gene and/or amino acid\nchange. The view is dynamically filtered to matching variants as you\ntype.", 
            "title": "Results browser"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#medal-meaning", 
            "text": "Medals are only assigned for coding and splice-related variants in\ndisease predisposition genes. Germline medals are only assigned for\nnovel variants or those present in the ExAC (ex-TCGA) database with a\nMAF no greater than 0.1% (0.001 expressed fractionally).    Gold medals  are assigned to truncations in tumor suppressor genes,\nhotspots derived from the COSMIC database, as well as perfect matches to\nvariants in the IARC TP53, PCGP, ASU TERT, ARUP RET, and BIC databases.  Silver medals  are assigned to in-frame indels, truncations in non-tumor\nsuppressor genes, variants predicted deleterious by damage-prediction\nalgorithms, variants receiving a gold medal from the somatic classifier,\nand perfect matches to variants in the following databases: ClinVar\n(predicted pathogenic or likely pathogenic), RB1, LOVD, and UMD.  Bronze medals  are assigned to variants predicted tolerated by\ndamage-prediction algorithms. Variants having an imperfect match to a\ndatabase (i.e. different variants at the same genomic position or codon)\ntypically receive a lesser medal.   A summary graphic can be found in slide 4 of the ASHG 2017 presentation ( download here ). \nFor additional details see  Zhang et al., NEJM\n2015 \n(supplementary appendix pp. 7-10).", 
            "title": "Medal meaning"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#variant-page", 
            "text": "Each variant links to a detailed variant page, which integrates data\nfrom a variety of sources. If either you or the St. Jude germline\nvariant review committee have annotated a variant, that information will\nbe pre-populated.  Summary information  The top of the page shows a summary of the variant, including its\ngenomic and HGVS annotations, predicted effect on the protein, and\nsomatic and germline medals. A description of the gene from Entrez\nfollows, and a custom description or selection rationale may also be\nentered.  Medal call information  Clicking on one of the medal icons (gold, silver, bronze, unknown) also\non the top of the page will show a summary of information related to the\nmedal call.  ProteinPaint  An embedded version of ProteinPaint ( Zhou et al., Nat. Genet.\n2016 ) appears next, showing\nthe variant in the context of a number of pediatric datasets including\nPCGP. A link is provided to the main ProteinPaint application which\nprovides visualizations for additional datasets, including COSMIC and\nClinVar.  ASHG pathogenicity classification  Formal variant pathogenicity classification is supported by an interface\nimplementing ACMG guidelines ( Richards et al., Genet Med.\n2015 ).\nThe analyst reviews a series of curated category tags, assigning\napplicable tags to the variant and optionally supplying additional\ninformation for each such as PubMed IDs and supporting evidence. The\nsystem will then compute an appropriate pathogenicity score based on the\nuser-flagged categories. Additional free-form custom evidence can also\nbe entered. This structured approach both helps eliminate arbitrary\ndecision-making from the pathogenicity classification process and also\nconstructs a concise summary of the logic and evidence supporting the\nfinal call.  ClinVar and allele frequency  Matches of the variant in ClinVar are also provided, along with\npredicted clinical significance and review status.  Allele frequencies for the variant in the PCGP (somatic and\ngermline),NHLBI ESP 6500, and ExAC databases are presented both as\nfractional values and on a log10 plot. Detailed allele population\nbreakdowns are provided for ExAC.  Damage prediction algorithms  Precomputed damage-prediction algorithm calls for nonsynonymous coding\nSNVs are presented from the dbNSFP database. Available algorithms are\nPolyPhen2 (HVAR), SIFT, CADD, REVEL, FATHMM, MutationAssessor, and LRT.\nThe calls are presented in a circular diagram with entries color-coded\nbased on the predicted severity of the result.  Medal ceremony and linkouts  Additional output from medal ceremony classification can also be\nreviewed. This is only loosely structured, additional fields here may\neventually be integrated into Pecan PIE.  Links are provided to relevant dbSNP entries and other information\nsources.  Final classification  The final 5-tier ACMG classification can be selected after which the\ndecision will be marked as reviewed. A checkbox is also available to\nindicate this variant is a potential candidate for functional review.", 
            "title": "Variant page"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#standalone-usage", 
            "text": "This section is intended only for users who want to invoke Pecan PIE's\nunderlying analysis pipelines independently on the DNAnexus  platform. If you just want to use\nthe Pecan PIE website you can safely ignore this section of the\ndocumentation. We assume familiarity with the DNAnexus platform. If you aren't\nfamiliar with this, DNAnexus'  quickstart guide  is a\ngreat place to start.   Warning  This section of the guide is only relevant to power users!   Two DNAnexus cloud application pipelines were created during the\ndevelopment of Pecan PIE:     Name  Corresponding DNAnexus App  Description      VEP+  app-stjude_vep_plus  A cloud installation of  VEP  with improved logic for splice variant calls. Converts an input VCF of variants to annotated, tab-delimited format.    Medal Ceremony  app-stjude_medal_ceremony  Additional annotation and automated variant classification. Requires a special input format which is produced by VEP+.", 
            "title": "Standalone usage"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#permissions", 
            "text": "In order to run the cloud pipelines independently, your DNAnexus account\nneeds to be granted permissions to access them. After your initial login\nto St. Jude Cloud and/or Pecan PIE, these permissions will be granted automatically. A single\nlogin is required even if you just want to use the standalone pipelines\nrather than the Pecan PIE portal ( contact\nus  if you encounter problems accessing\nthe pipelines).  There are two methods of running pipelines on DNAnexus:   DNAnexus GUI.  DNAnexus provides a standardized graphical user interface for\n    configurating, launching, and monitoring jobs on the cloud. Our\n    pipelines can be run like any other DNAnexus pipeline.  Command line.  Jobs may also be invoked via the  dx  command line\n    client. Command-line use allows submitting cloud jobs without\n    interacting with a GUI, and so supports scripting and easier\n    integration with local workflows. See  this section \n    for information on how to get set up with the  dx-toolkit .    Note  The following examples demonstrate command-line usage.", 
            "title": "Permissions"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#uploading-files", 
            "text": "All input files must be uploaded onto the DNAnexus platform. When\nspecifying files for input you can use either the DNAnexus fie IDs (e.g. file-FBgvp680gz1bGQ5p8yZKz69g ), or the filenames if they are unique. For\nan idea of how to upload files to DNAnexus, see  this guide .", 
            "title": "Uploading files"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#step-1-running-vep", 
            "text": "To run the VEP+ DNAnexus app, you can use the following  dx \ncommand with your own inputs in place of the example's:  dx run app-stjude_vep_plus -iinput_file = my_vcf.vcf -igenome_string = GRCh37-lite -igermline_reviewable_only = true    Tip   genome_string  must be either  GRCh37-lite  or  GRCh38 . If  GRCh38 \nis specified, variants will be lifted over to  GRCh37-lite  in output,\ni.e. the output will always be  GRCh37-lite  (Medal Ceremony currently only supports  GRCh37-lite ).  The input VCF specified by  input_file  may be either\nuncompressed, or compressed with  bgzip   only  (htslib/tabix\npackages).  The  germline_reviewable_only  parameter is optional, but\nstrongly recommended. If specified, only variants in disease-gene\nrelated intervals will be annotated, which is appropriate for\nMedal Ceremony. If this option is not specified all variants will\nbe annotated, which depending on the size of your VCF might take a\nlot longer, and many of the resulting variants won't be usable by\nMedal Ceremony. If you want to do this anyway and have a large\nnumber of variants, consider submitting your job to an instance\nwith more CPU cores (e.g.  mem1_ssd1_x16  or  mem1_ssd1_x32 ) as\nthe code will take advantage of the additional cores. If you are\nusing a custom gene list (below) that takes precedence and this\nparameter is not needed.  The optional parameter  custom_genes_file  specifies a plain\ntext file of HUGO gene symbols to analyze (whitespace separated,\nor one per line). If specified, analysis will be restricted to\nthese genes only.  This pipeline produces two output files,  output_file  contains\nannotations for all variants, while  medal_prep_output_file  is\nthe specially-filtered and formatted file required as input to\nMedal Ceremony below.", 
            "title": "Step 1: Running VEP+"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#step-2-running-medal-ceremony", 
            "text": "To run the medal ceremony DNAnexus app, you can use the following  dx \ncommand with your own inputs in place of the example's:  dx run app-stjude_medal_ceremony -iinfile = medal_prep_output_file   Tip   The optional parameter  custom_genes_file  operates in the same\nway as in the VEP+ pipeline above. For custom gene lists to work\nproperly this parameter must be specified when running both the\nVEP+ and Medal Ceremony pipelines.  The optional parameter  max_population_frequency  may be\nspecified, a fractional value representing the maximum population\nfrequency allowed for a variant in the ExAC (ex-TCGA) database to\nreceive a medal. The default is 0.001, a.k.a. \".1%\".", 
            "title": "Step 2: Running Medal Ceremony"
        }, 
        {
            "location": "/guides/tools/pecan-pie/#frequently-asked-questions", 
            "text": "If you have any questions not covered here, feel free to reach out on our contact form .  Q: Which files are supported?  PIE works with variants in VCF format:   Uploaded files must be compliant with the  VCF specification .  VCF files may be either uncompressed, or compressed with  bgzip   only .  bgzip  is part of the htslib/tabix packages (see below).   Improperly formatted VCF files will not work with PIE. Some common\nproblems include:   Missing header line  Missing required columns  Files compressed by any method other than  bgzip  ( gzip ,  zip , or any other program)   To verify compatibility of your VCF you can try one of these methods:   Compressing your VCF with\n    bgzip  and indexing it with\n    tabix , both programs from\n   the  HTSlib  package (some systems also\n   use the earlier, pre-HTSlib \"tabix\" package). This process will\n   only succeed for compliant VCF files, and can help diagnose\n   failures.  Running \"vcf-validator\" program from the\n    vcftools  package.   While the VCF specification also requires that variants be sorted by\nchromosome name and position, PIE is now often able to automatically\ncorrect sorting issues in uploaded files. PIE requires sorted data in\norder to query data for targeted genes.  Q: Are there limits on the size of VCF files?  Uploaded files must not exceed 4 gigabytes. If an uploaded file is larger\nthan 2 megabytes, the cancer predisposition gene list filter will be\nautomatically enabled unless you are using a custom gene list. This reduces\nthe processing burden on the system by removing variants outside of targeted\ngenes.  Q: Is there an example/demo VCF I can try with PIE?  A. You can use  this VCF \nfrom the Genome in a Bottle project. This ~133 megabyte\nbgzip-compressed VCF was used during testing of Pecan PIE and is known\nto work. These variants are mapped to GRCh37.  Q. What genome versions are supported?  A. Pecan PIE will accept variants mapped to either GRCh37-lite/hg19 or GRCh38.\nGRCh38 variants are automatically lifted over to 37, as the system\nuses 37 internally; the liftover process is able to compensate for\nstrand and reference/variant allele swaps which can occur. A native\nhg38 version is in development, but is not yet available.  Pecan PIE only works for human data.  Q. What genes are on the curated gene list?  A. The list consists of disease-related genes, both cancer and\nnon-cancer, see the  file_download  Excel spreadsheet \nfor details. Filtering the source variants to a target list of genes\nreduces the processing burden on the system.  When browsing the results the view may be filtered to disease\nsub-categories of interest.  You can also specify your own custom list of genes to process when\nsubmitting your VCF file (see the advanced options panel).  Q. Why is the classification column blank in my results?  Q. This column displays the classification assigned by the St. Jude\nGermline Committee reviewers. If a variant was not classified by this\ncommittee before, this field will be blank.  Pecan PIE provides classifications from the Medal Ceremony pipeline,\nwhich may assign variants gold, silver, or bronze medals. An \"Unknown\"\nmedal may be assigned for non-disease-predisposition genes, variants\npresent in the ExAC (ex-TCGA) database at an allele frequency  \n0.1%, or variants without functional annotations (which includes most\nsilent variants).  Q. What do the medals mean?  A. The medal column is a rough indicator of the likelihood of the variant\nbeing clinically significant as predicted by the medal ceremony\nsoftware. Variants with gold medals are most likely to be significant,\nand those with no medal are least likely. More details can be found in\nthe  Analysis of Results  results \nsection.  Q. Why are some of my variants missing?  A. Currently only coding and splice-related variants in disease-related\ngenes make it to the medaling process. Intergenic, intronic, and UTR\nvariants are excluded, as are those in non-coding transcripts.  Q. Why does the ExAC allele frequency shown differ from the ExAC portal?  A. The reported ExAC frequency may differ for several reasons:   PIE uses the TCGA-subtracted distribution of ExAC rather than the\n    main distribution.  PIE reports the primary allele frequencies in the ExAC database,\n    specifically the AC, AN, and AF fields from the VCF distribution.\n    The  ExAC portal  appears to use\n    the \"adjusted\" frequencies which may be different.   Q. Is Pecan PIE free?  A. Pecan PIE is free for non-commercial use. St. Jude covers the cost of\nrunning the pipeline and hosting. DNANexus accounts are required to\nkeep track of your jobs in the cloud so that you can retrieve and\nmanage from multiple locations. Accounts also make it possible to\nalert you of job completion via email.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/guides/tools/neoepitope/", 
            "text": "Authors\n\n\nTi-Cheng Chang\n\n\n\n\n\n\nPublication\n\n\nThe Neoepitope Landscape in Pediatric Cancers. Genome Medicine. 2017. 9.1: 78\n.\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nCancers are caused by somatically acquired alterations including single \nnucleotide variations (SNVs), small insertion/deletions (indels),\ntranslocations, and other types of rearrangements. The genes affected by\nthese mutations may produce altered proteins, some of which may lead to\nthe emergence of tumor-specific immunogenic epitopes. We developed an\nanalytical workflow for identification of putative neoepitopes based on\nsomatic missense mutations and gene fusions using whole genome\nsequencing data. The workflow has been used to characterize neoepitope\nlandscape of 23 subtypes of pediatric cancer in the Pediatric Cancer\nGenome Project\n1\n.\n\n\nOverview\n\n\nInputs\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nFastQ files (\nrequired\n if using FastQ inputs)\n\n\nInput file\n\n\nGzipped FastQ files generated by experiment.\n\n\nSample_R1.fastq.gz and Sample_R2.fastq.gz\n\n\n\n\n\n\nBAM files (\nrequired\n if using BAM inputs)\n\n\nInput file\n\n\nBAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq).\n\n\nSample.bam\n\n\n\n\n\n\nBAM indices (\nrequired\n if using BAM inputs)\n\n\nInput file\n\n\nCorresponding BAM index of the BAM files above.\n\n\nSample.bam.bai\n\n\n\n\n\n\nMutation file (\nrequired\n)\n\n\nInput file\n\n\nFile describing the mutations present in the sample (special format, see below).\n\n\n*.txt (tab-delimited)\n\n\n\n\n\n\nSNV or fusion\n\n\nParameter\n\n\nSpecify the mutation file contains SNV or gene fusion.\n\n\nSNV\n\n\n\n\n\n\nPeptide size\n\n\nParameter\n\n\nSize of the peptide.\n\n\n9\n\n\n\n\n\n\nAffinity threshold\n\n\nParameter\n\n\nAffinity cutoff for epitope prediction report.\n\n\n500\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEpitope affinity prediction (html)\n\n\nEpitope affinity. The peptide with affinity \n cutoff will be highlighted.\n\n\n\n\n\n\nEpitope affinity prediction (xlsx)\n\n\nExcel tables for the infomation of all epitopes\n\n\n\n\n\n\nAffinity (raw output)\n\n\nEpitope affinity\n\n\n\n\n\n\nPeptide sequence (raw output)\n\n\nPeptide sequences in Fasta format\n\n\n\n\n\n\n\n\nHLA Typing Algorithm\n\n\nThe HLA typing algorithm is used to predict the HLA class I alleles.\nUsers can either provide FastQ (paired or single end reads) or a BAM\nfile as input. When using a BAM file as input, the reads surrounding the\nHLA loci and unmapped reads will be extracted. The reads will be fed\ninto Optitype for HLA typing. The default settings for Optitype are\nused. The output of the HLA type can be combined with the our epitope\ndetection algorithm to perform affinity prediction of neoepitopes.\n\n\nIf you use \nFastQ\n files as input:  \n\n\n\n\nThe input FastQs will be aligned against the Optitype HLA reference\n    sequences using razers3 (see \nhttps://github.com/FRED-2/OptiType\n).\n\n\nThe fished FastQs will be used for HLA typing using Opitype.\n\n\n\n\nIf you use \nBAM\n files as input:  \n\n\n\n\nThe reads falling within the HLA loci and their paralogous loci will\n    be extracted.\n\n\nThe reads unmapped to the human genome will be extracted.\n\n\nThe reads from step 1 and 2 will be combined and deduplicated (in\n    FastQ format).\n\n\nThe input FastQs will be aligned against the Optitype HLA reference\n    sequences using razers3 (see \nhttps://github.com/FRED-2/OptiType\n).\n\n\nThe fished FastQs will be used for HLA typing using Opitype.\n\n\n\n\nEpitope Prediction Algorithm\n\n\nThe epitope prediction algorithm first extracts peptides covering an array\nof tiling peptides (size defined by users) overlapping each missense\nmutation or gene fusion. Fusion junctions can be identified using RNA-Seq\nby fusion detection tools (Li et. al, unpublished). NetMHCcons\n3\n is subsequently \nused to predict affinities of the peptide array for each HLA receptor in \neach sample. The neoepitope with affinity lower than the threshold will \nbe highlighted in output file (default 500 nM).\n\n\n\n\nCheck the version of the genomic position of the input SNV/fusion\n    file.\n\n\nLift over the genomic coordinations if the reference genomic\n    position is not HG19. Currently, the internal genome annotation was\n    based on HG19 and the genome coordinates of the mutation files will\n    be adjusted to HG19 for peptide extraction.\n\n\nExtract the peptide flanking the mutations.\n\n\nRun NetMHCcons to obtain the affinity prediction of the peptides.\n\n\nProduce the affinity report of each peptide.\n\n\n\n\nGetting started\n\n\nTo get started, you need to navigate to the \nNeoepitopePred tool page\n. You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.\n\n\n\n\n\n\nNote\n\n\nIf you can't see the \"Start\" button, one of these two scenarios is likely the case:\n\n\n\n\nYou see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.\n\n\nIf you cannot see \nany\n buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.\n\n\n\n\nIf neither of these are the case and you still can't click \"Start\",\n\ncontact us\n.\n\n\n\n\nInput file configuration\n\n\nUsers need to provide a mutation file for SNV or gene fusion. The format\nof the mutation file is shown in the following example. The file can be\nprepared in Excel and saved as a tab-delimited text file to use as\ninput.\n\n\nThe HLA alleles for testing will be derived from the HLA typing module\nusing the workflow. The peptide size and affinity cutoff can be modified\nby users. \n\n\nMutation file format\n\n\n\n\n\n\n\n\nGeneName\n\n\nSample\n\n\nChr\n\n\nPostion_hg19\n\n\nClass\n\n\nAAChange\n\n\nmRNA_acc\n\n\nReferenceAllele\n\n\nMutantAllele\n\n\n\n\n\n\n\n\n\n\nGene1\n\n\nSampleA\n\n\nchr10\n\n\n106150600\n\n\nmissense\n\n\nR663H\n\n\nNM_00101\n\n\nA\n\n\nT\n\n\n\n\n\n\nGene2\n\n\nSampleA\n\n\nchr2\n\n\n32330151\n\n\nmissense\n\n\nN329N\n\n\nNM_00102\n\n\nT\n\n\nG\n\n\n\n\n\n\n\n\n\n\nNotes on preparing the above file\n\n\n\n\nThe chromosome requires a 'chr' prefix.\n\n\nThe position requires a suffix of HG19/HG38 to indicate the human genome assembly version.\n\n\nOnly the missense mutations/gene fusion is supported currently and\n    the other types of mutations will not be processed.\n\n\n\n\n\n\nMutation file example\n\n\n\n\nUploading data\n\n\nNeoepitopePred takes the following files as input:\n\n\n\n\nA pair of Gzipped FastQ files or an HG19/HG38 aligned BAM file. These can be\n  generated from whole genome sequencing, whole exome sequencing, or RNA-Seq. \n\n\nA file describing the mutations in a sample. \n\n\n\n\nYou can upload these files using the \ndata transfer application\n\nor by uploading them through \nthe command line\n.\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.\n\n\n\n\nTip\n\n\nIf you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!\n\n\nFor more information, check out the \ndata transfer application\n guide.\n\n\n\n\nRunning the tool\n\n\n\n\nCaution\n\n\nThis pipeline assumes HG19 coordinates in the mutation file. If the\ncoordinates are based on HG38, the coordinates will lifted over to HG19\nto perform epitope affinity prediction.\n\n\n\n\nOnce you've uploaded data to your cloud workspace, \nclick \"Launch Tool\" on the \ntool's landing page\n. \nA dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with FastQ files or a BAM file.\n\n\n\n\nSelecting parameters\n\n\nThere are a number of other parameters that can be customized. To \nsee the options available, click the gear cog next to the \n\"Neoepitope Prediction\" substep. For a full list of the parameters and their\ndescriptions, see \nthe input section\n (specifically, you are \nlooking at the items in the table labeled \"parameters\").\n\n\n\n\nHooking up inputs\n\n\nNext, you'll need to hook up the FastQ/BAM and mutation files you uploaded in \n\nthe upload data section\n. You can do this by \nclicking on the \nBAM alignment file\n and \nBAM index file\n and \nMutation array\n slots and\nselecting the respective files.\n\n\n\n\nStarting the workflow\n\n\nOnce your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.\n\n\n\n\n\n\nTip\n\n\nIf you cannot click this button, please ensure that all of the inputs are correctly hooked up (see \nhooking up inputs\n).\n\n\nIf you're still have trouble, please \ncontact us\n and include\na screenshot of the workflow screen above.\n\n\n\n\nMonitoring run progress\n\n\nOnce you have started one or more NeoepitopePred runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the \ntool's landing page\n. \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.\n\n\n \n\n\nYou can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.\n\n\n \n\n\n\n\nTip\n\n\nPower users can view the \nDNAnexus Job Monitoring Tutorial\n and the \nDNAnexus Command Line Tutorial for Job Monitoring\n for advanced capabilities for monitoring jobs.\n\n\n\n\nAnalysis of results\n\n\nEach tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.\n\n\nFinding the raw results files\n\n\nNavigate to the \ntool's landing page\n. \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.\n\n\n \n\n\nInterpreting results\n\n\nHLA typing\n\n\nThe output of this app contain the prediction of the HLA class I alleles\nfrom OptiType.\n\n\n\n\nA folder stamped with the time will present in the output folder\n  (optitype), which contains the raw output.\n\n\n\n\n\n\n\n\nThe file contains the predicted HLA alleles of the sample.\n\n\n\n\n\n\nNeoepitope prediction\n\n\n\n\nTodo\n\n\nCleanup the formatting of this section.\n\n\n\n\nThe output contains one summary HTML, one folder with raw output and one\nfolder with outputs in Excel formats:\n\n\n\n\nEpitope_affinity_prediction.html:\n\n\n This file provides a summary of the epitope prediction that can be\n  visualized directly from web browser.\n\n The peptides with affinity lower than user-defined cutoff will be\n  highlighted in green in the webpage.\n\n\n\n\nRaw_output: this folder contains the raw output of the affinity\nprediction.\n\n\n\n\nThere will two major types files present here:\n\n\n\n\naffinity.out: these files are the prediction results from the\n    netMHCcons for each peptide.\n\n\n\n\nThe following columns will be shown in the output:\n\n\n\n\n\n\n\n\nColumn\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGene name\n\n\nthe name of the genes\n\n\n\n\n\n\nSample\n\n\nthe name of the samples\n\n\n\n\n\n\nChromosome (chr)\n\n\nthe chromosome location of the variation\n\n\n\n\n\n\nPosition\n\n\nthe chromosomal position of the variation. Currently, the position will be lifted over to HG19 to ensure correct translation of peptid sequences based on the internal annotation database of the pipeline. Therefore, the position will be labeled as HG19.\n\n\n\n\n\n\nClass\n\n\nclass of the varitaion\n\n\n\n\n\n\nReference allele\n\n\nreference allele at the position\n\n\n\n\n\n\nMutant allele\n\n\nmutated allele at the position\n\n\n\n\n\n\nmRNA_acc\n\n\nNCBI accession number of the mRNA\n\n\n\n\n\n\nAllele\n\n\nHLA allele tested\n\n\n\n\n\n\nPeptide\n\n\nthe neoepitope sequences tested\n\n\n\n\n\n\nGene_variant\n\n\nthe gene and variant residues\n\n\n\n\n\n\n1-log50k\n\n\nPrediction score from netMHCcons\n\n\n\n\n\n\nnM\n\n\nAffinity as IC50 values in nM\n\n\n\n\n\n\n%Rank\n\n\n% Rank of prediction score to a set of 200.000 random natural 9mer peptides\n\n\n\n\n\n\nHLAtype\n\n\nAll of the hla alleles predicted in the specific sample\n\n\n\n\n\n\n\n\nflanking.seq: these files contain the sequences used for the prediction.\n\n\n\n\nXLSX: this folder contains the raw output of the affinity prediction as\ndescribed above in Excel files. The files can be downloaded and opened\nwith Excel for downstream filtering and analyses\n\n\n\n\nFrequently asked questions\n\n\nNone yet! If you have any questions not covered here, feel free to reach\nout on \nour contact form\n.\n\n\n\n\n\n\n\n\n\n\nDowning JR, Wilson RK, Zhang J, et al. The Pediatric Cancer Genome\nProject. Nature genetics. 2012;44(6):619-622.\n\n\n\n\n\n\nSzolek A, Schubert B, Mohr C, Sturm M, Feldhahn M, Kohlbacher O:\nOptiType: precision HLA typing from next-generation sequencing\ndata. Bioinformatics 2014, 30:3310-3316.\n\n\n\n\n\n\nKarosiene E, Lundegaard C, Lund O, Nielsen M: NetMHCcons: a\nconsensus method for the major histocompatibility complex class I\npredictions. Immunogenetics 2012, 64:177-186.", 
            "title": "NeoepitopePred"
        }, 
        {
            "location": "/guides/tools/neoepitope/#overview", 
            "text": "Inputs     Name  Type  Description  Example      FastQ files ( required  if using FastQ inputs)  Input file  Gzipped FastQ files generated by experiment.  Sample_R1.fastq.gz and Sample_R2.fastq.gz    BAM files ( required  if using BAM inputs)  Input file  BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq).  Sample.bam    BAM indices ( required  if using BAM inputs)  Input file  Corresponding BAM index of the BAM files above.  Sample.bam.bai    Mutation file ( required )  Input file  File describing the mutations present in the sample (special format, see below).  *.txt (tab-delimited)    SNV or fusion  Parameter  Specify the mutation file contains SNV or gene fusion.  SNV    Peptide size  Parameter  Size of the peptide.  9    Affinity threshold  Parameter  Affinity cutoff for epitope prediction report.  500     Outputs     Name  Description      Epitope affinity prediction (html)  Epitope affinity. The peptide with affinity   cutoff will be highlighted.    Epitope affinity prediction (xlsx)  Excel tables for the infomation of all epitopes    Affinity (raw output)  Epitope affinity    Peptide sequence (raw output)  Peptide sequences in Fasta format     HLA Typing Algorithm  The HLA typing algorithm is used to predict the HLA class I alleles.\nUsers can either provide FastQ (paired or single end reads) or a BAM\nfile as input. When using a BAM file as input, the reads surrounding the\nHLA loci and unmapped reads will be extracted. The reads will be fed\ninto Optitype for HLA typing. The default settings for Optitype are\nused. The output of the HLA type can be combined with the our epitope\ndetection algorithm to perform affinity prediction of neoepitopes.  If you use  FastQ  files as input:     The input FastQs will be aligned against the Optitype HLA reference\n    sequences using razers3 (see  https://github.com/FRED-2/OptiType ).  The fished FastQs will be used for HLA typing using Opitype.   If you use  BAM  files as input:     The reads falling within the HLA loci and their paralogous loci will\n    be extracted.  The reads unmapped to the human genome will be extracted.  The reads from step 1 and 2 will be combined and deduplicated (in\n    FastQ format).  The input FastQs will be aligned against the Optitype HLA reference\n    sequences using razers3 (see  https://github.com/FRED-2/OptiType ).  The fished FastQs will be used for HLA typing using Opitype.   Epitope Prediction Algorithm  The epitope prediction algorithm first extracts peptides covering an array\nof tiling peptides (size defined by users) overlapping each missense\nmutation or gene fusion. Fusion junctions can be identified using RNA-Seq\nby fusion detection tools (Li et. al, unpublished). NetMHCcons 3  is subsequently \nused to predict affinities of the peptide array for each HLA receptor in \neach sample. The neoepitope with affinity lower than the threshold will \nbe highlighted in output file (default 500 nM).   Check the version of the genomic position of the input SNV/fusion\n    file.  Lift over the genomic coordinations if the reference genomic\n    position is not HG19. Currently, the internal genome annotation was\n    based on HG19 and the genome coordinates of the mutation files will\n    be adjusted to HG19 for peptide extraction.  Extract the peptide flanking the mutations.  Run NetMHCcons to obtain the affinity prediction of the peptides.  Produce the affinity report of each peptide.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/neoepitope/#getting-started", 
            "text": "To get started, you need to navigate to the  NeoepitopePred tool page . You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.    Note  If you can't see the \"Start\" button, one of these two scenarios is likely the case:   You see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.  If you cannot see  any  buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.   If neither of these are the case and you still can't click \"Start\", contact us .", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/neoepitope/#input-file-configuration", 
            "text": "Users need to provide a mutation file for SNV or gene fusion. The format\nof the mutation file is shown in the following example. The file can be\nprepared in Excel and saved as a tab-delimited text file to use as\ninput.  The HLA alleles for testing will be derived from the HLA typing module\nusing the workflow. The peptide size and affinity cutoff can be modified\nby users.   Mutation file format     GeneName  Sample  Chr  Postion_hg19  Class  AAChange  mRNA_acc  ReferenceAllele  MutantAllele      Gene1  SampleA  chr10  106150600  missense  R663H  NM_00101  A  T    Gene2  SampleA  chr2  32330151  missense  N329N  NM_00102  T  G      Notes on preparing the above file   The chromosome requires a 'chr' prefix.  The position requires a suffix of HG19/HG38 to indicate the human genome assembly version.  Only the missense mutations/gene fusion is supported currently and\n    the other types of mutations will not be processed.    Mutation file example", 
            "title": "Input file configuration"
        }, 
        {
            "location": "/guides/tools/neoepitope/#uploading-data", 
            "text": "NeoepitopePred takes the following files as input:   A pair of Gzipped FastQ files or an HG19/HG38 aligned BAM file. These can be\n  generated from whole genome sequencing, whole exome sequencing, or RNA-Seq.   A file describing the mutations in a sample.    You can upload these files using the  data transfer application \nor by uploading them through  the command line .\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.   Tip  If you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!  For more information, check out the  data transfer application  guide.", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/neoepitope/#running-the-tool", 
            "text": "Caution  This pipeline assumes HG19 coordinates in the mutation file. If the\ncoordinates are based on HG38, the coordinates will lifted over to HG19\nto perform epitope affinity prediction.   Once you've uploaded data to your cloud workspace, \nclick \"Launch Tool\" on the  tool's landing page . \nA dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with FastQ files or a BAM file.", 
            "title": "Running the tool"
        }, 
        {
            "location": "/guides/tools/neoepitope/#selecting-parameters", 
            "text": "There are a number of other parameters that can be customized. To \nsee the options available, click the gear cog next to the \n\"Neoepitope Prediction\" substep. For a full list of the parameters and their\ndescriptions, see  the input section  (specifically, you are \nlooking at the items in the table labeled \"parameters\").", 
            "title": "Selecting parameters"
        }, 
        {
            "location": "/guides/tools/neoepitope/#hooking-up-inputs", 
            "text": "Next, you'll need to hook up the FastQ/BAM and mutation files you uploaded in  the upload data section . You can do this by \nclicking on the  BAM alignment file  and  BAM index file  and  Mutation array  slots and\nselecting the respective files.", 
            "title": "Hooking up inputs"
        }, 
        {
            "location": "/guides/tools/neoepitope/#starting-the-workflow", 
            "text": "Once your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.    Tip  If you cannot click this button, please ensure that all of the inputs are correctly hooked up (see  hooking up inputs ).  If you're still have trouble, please  contact us  and include\na screenshot of the workflow screen above.", 
            "title": "Starting the workflow"
        }, 
        {
            "location": "/guides/tools/neoepitope/#monitoring-run-progress", 
            "text": "Once you have started one or more NeoepitopePred runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the  tool's landing page . \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.     You can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.      Tip  Power users can view the  DNAnexus Job Monitoring Tutorial  and the  DNAnexus Command Line Tutorial for Job Monitoring  for advanced capabilities for monitoring jobs.", 
            "title": "Monitoring run progress"
        }, 
        {
            "location": "/guides/tools/neoepitope/#analysis-of-results", 
            "text": "Each tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.", 
            "title": "Analysis of results"
        }, 
        {
            "location": "/guides/tools/neoepitope/#finding-the-raw-results-files", 
            "text": "Navigate to the  tool's landing page . \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.", 
            "title": "Finding the raw results files"
        }, 
        {
            "location": "/guides/tools/neoepitope/#interpreting-results", 
            "text": "HLA typing  The output of this app contain the prediction of the HLA class I alleles\nfrom OptiType.   A folder stamped with the time will present in the output folder\n  (optitype), which contains the raw output.     The file contains the predicted HLA alleles of the sample.    Neoepitope prediction   Todo  Cleanup the formatting of this section.   The output contains one summary HTML, one folder with raw output and one\nfolder with outputs in Excel formats:   Epitope_affinity_prediction.html:   This file provides a summary of the epitope prediction that can be\n  visualized directly from web browser.  The peptides with affinity lower than user-defined cutoff will be\n  highlighted in green in the webpage.   Raw_output: this folder contains the raw output of the affinity\nprediction.   There will two major types files present here:   affinity.out: these files are the prediction results from the\n    netMHCcons for each peptide.   The following columns will be shown in the output:     Column  Description      Gene name  the name of the genes    Sample  the name of the samples    Chromosome (chr)  the chromosome location of the variation    Position  the chromosomal position of the variation. Currently, the position will be lifted over to HG19 to ensure correct translation of peptid sequences based on the internal annotation database of the pipeline. Therefore, the position will be labeled as HG19.    Class  class of the varitaion    Reference allele  reference allele at the position    Mutant allele  mutated allele at the position    mRNA_acc  NCBI accession number of the mRNA    Allele  HLA allele tested    Peptide  the neoepitope sequences tested    Gene_variant  the gene and variant residues    1-log50k  Prediction score from netMHCcons    nM  Affinity as IC50 values in nM    %Rank  % Rank of prediction score to a set of 200.000 random natural 9mer peptides    HLAtype  All of the hla alleles predicted in the specific sample     flanking.seq: these files contain the sequences used for the prediction.   XLSX: this folder contains the raw output of the affinity prediction as\ndescribed above in Excel files. The files can be downloaded and opened\nwith Excel for downstream filtering and analyses", 
            "title": "Interpreting results"
        }, 
        {
            "location": "/guides/tools/neoepitope/#frequently-asked-questions", 
            "text": "None yet! If you have any questions not covered here, feel free to reach\nout on  our contact form .      Downing JR, Wilson RK, Zhang J, et al. The Pediatric Cancer Genome\nProject. Nature genetics. 2012;44(6):619-622.    Szolek A, Schubert B, Mohr C, Sturm M, Feldhahn M, Kohlbacher O:\nOptiType: precision HLA typing from next-generation sequencing\ndata. Bioinformatics 2014, 30:3310-3316.    Karosiene E, Lundegaard C, Lund O, Nielsen M: NetMHCcons: a\nconsensus method for the major histocompatibility complex class I\npredictions. Immunogenetics 2012, 64:177-186.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/guides/tools/chipseq/", 
            "text": "Authors\n\n\nXing Tang, Yong Cheng\n\n\n\n\n\n\nPublication\n\n\nN/A (not published)\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nThe ChIP-Seq Peak Calling workflow follows ENCODE best practices to call \nbroad or narrow peaks on Illumina-generated ChIP-Seq data. \nHere, a Gzipped FastQ file from an Immunoprecipitation (IP) experiment \nis considered the \"case sample file\" and a Gzipped FastQ file from a control \nexperiment is considered the \"control sample file\". The pipeline can run on\nmatched case/control samples (recommended for better results) or just a \ncase sample.\n\n\nOverview\n\n\nInputs\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nFastQ files (\nrequired\n if using FastQ inputs)\n\n\nInput file\n\n\nGzipped FastQ files generated by experiment.\n\n\nSample_R1.fastq.gz and Sample_R2.fastq.gz\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\n\n\nFormat\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBED file\n\n\n.bed\n\n\nPeak calls\n\n\n\n\n\n\nBinary file\n\n\n.bb\n\n\nBinary format for BED file\n\n\n\n\n\n\nBigWig file\n\n\n.bw\n\n\nShows read coverage\n\n\n\n\n\n\nMetrics file\n\n\n.txt\n\n\nShows mapping and duplication rate\n\n\n\n\n\n\nCross correlation plot\n\n\n.pdf\n\n\nQuality plot showing if the forward and reverse reads tend to be centered around binding sites.\n\n\n\n\n\n\n\n\nProcess\n\n\n\n\nThe reads of the FastQ file(s) are aligned to the specified reference genome. \n\n\nThe aligned reads are then post-processed based on best-practice QC techniques\n(removing multiple mapped reads, removing duplicated reads, etc). \n\n\nPeaks are called by SICER (broad peak analysis) or MACS2 (narrow peak\nanalysis). \n\n\nQualified peaks will be output as BED (.bed) and big BED (.bb)\nfiles. \n\n\nThe coverage information will be output as a bigWig (.bw)\nfile. \n\n\nA cross correlation plot and general metrics file are generated to help check\nthe quality of experiment.\n\n\n\n\n\n\n\n\nGetting started\n\n\nTo get started, you need to navigate to the \nChIP-Seq tool page\n. You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.\n\n\n\n\n\n\nNote\n\n\nIf you can't see the \"Start\" button, one of these two scenarios is likely the case:\n\n\n\n\nYou see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.\n\n\nIf you cannot see \nany\n buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.\n\n\n\n\nIf neither of these are the case and you still can't click \"Start\",\n\ncontact us\n.\n\n\n\n\nUploading data\n\n\nThe ChIP-Seq Peak Caller takes Gzipped FastQ files generated from an\nIP experiment as input. You can upload your input FastQ files by\nusing the \ndata transfer application\n\nor by uploading them through \nthe command line\n.\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.\n\n\n\n\nTip\n\n\nIf you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!\n\n\nFor more information, check out the \ndata transfer application\n guide.\n\n\n\n\nRunning the tool\n\n\nOnce you've uploaded data to your cloud workspace, \nclick \"Launch Tool\" on the \ntool's landing page\n. \nA dropdown will present the different presets for running the ChIP-Seq workflow.\nYou'll need to decide \n(1)\n whether you'd like to run broad/narrow peak\ncalling and \n(2)\n whether you have a case sample and a control sample (preferred)\nor just a case sample. This will determine which preset you should\nclick in this dropdown. There are various other parameters that you can \nset, but they are covered in further sections of this guide.\n\n\n\n\nBroad vs. narrow peak calling\n\n\nChoosing between broad and narrow peak calling depends on the experiment\ndesign. The following are good rules of thumb for choosing between the\ntwo configurations. If you are not sure which configuration to use,\nplease consult with an expert at your institution or \ncontact us\n.\n\n\nNarrow Peak Calling\n\n\nIf your target protein is a transcription factor, you should probably\nchoose narrow peak calling. You can also try the narrow peak calling\nworkflows for the following histone marks:\n\n\n\n\nH3K4me3\n\n\nH3K4me2\n\n\nH3K9-14ac\n\n\nH3K27ac\n\n\nH2A.Z\n\n\n\n\nBroad Peak Calling\n\n\nYou should try the broad peak calling workflows for the following\nhistone marks:\n\n\n\n\nH3K36me3\n\n\nH3K79me2\n\n\nH3K27me3\n\n\nH3K9me3\n\n\nH3K9me1\n\n\n\n\nSpecial Cases\n\n\nIn some scenarios, H3K4me1, H3K9me2 and H3K9me3 might behave between\nnarrow and broad shape, you might need to look into each peak region and\nconsult experts.\n\n\n\n\nWarning\n\n\nIf your fragment size is less than 50 base pairs, please refer to the\n\nfrequently asked questions\n.\n\n\n\n\nSelecting parameters\n\n\nThere are a number of other parameters that can be customized. To \nsee the options available, click the gear cog next to the \n\"Parameter Wrapper\" substep.\n\n\n\n\nThe following are the parameters that can be set, a short\ndescription of each parameter, and an example value. If you\nhave questions, please \ncontact us\n.\n\n\n\n\n\n\n\n\nParameter Name\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nOutput prefix (\nrequired\n)\n\n\nA name used a prefix for all outputs in the run\n\n\nSAMPLE1\n\n\n\n\n\n\nReference genome (\nrequired\n)\n\n\nSupported reference genome from one of hg19, GRCh38, mm9, mm10, dm3\n\n\nGRCh38\n\n\n\n\n\n\nOutput bigWig\n\n\nWhether or not to include a bigwig file in the output\n\n\nTrue\n\n\n\n\n\n\nRemove blacklist peaks\n\n\nWhether or not to remove known problem areas\n\n\nTrue\n\n\n\n\n\n\nFragment length\n\n\nHardcoded fragment length of your reads. 'NA' for auto-detect.\n\n\nNA\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\nPlease be aware of the following stumbling points when setting parameters:\n\n\n\n\nDo not use spaces anywhere in your input file names, your output\n  prefix, or any of the other parameters. This is generally bad\n  practice and doesn't play well with the pipeline (consider using\n  \"_\" instead).\n\n\nDo not change the output directory when you run the pipeline. At\n  the top of parameter input page, there is a text box that allows\n  you to change the output folder. \nPlease ignore that setting\n. You\n  only need to specify an output prefix as described above. All of\n  the results will be put under \n/Results/[OUTPUT_PREFIX]\n.\n\n\n\n\n\n\nHooking up inputs\n\n\nNext, you'll need to hook up the FastQ files you uploaded in \n\nthe upload data section\n. You can do this by \nclicking on the \nChIP Reads\n and \nControl Reads\n slots and\nselecting the respective files. If you are not doing a case/control\nrun, you only need to hook up the case sample.\n\n\n\n\nStarting the workflow\n\n\nOnce your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.\n\n\n\n\n\n\nTip\n\n\nIf you cannot click this button, please ensure that:\n\n\n\n\nall of the inputs are correctly hooked up (see \nhooking up inputs\n), and \n\n\nall of the required parameters are set (see \nsetting parameters\n).\n\n\n\n\nIf you're still have trouble, please \ncontact us\n and include\na screenshot of the workflow screen above.\n\n\n\n\nMonitoring run progress\n\n\nOnce you have started one or more ChIP-Seq runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the \ntool's landing page\n. \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.\n\n\n \n\n\nYou can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand the job logs can be accessed by clicking around the sub-items.\n\n\n \n\n\n\n\nTip\n\n\nPower users can view the \nDNAnexus Job Monitoring Tutorial\n and the \nDNAnexus Command Line Tutorial for Job Monitoring\n for advanced capabilities for monitoring jobs.\n\n\n\n\nInteractive visualizations\n\n\nToday, the ChIP-Seq pipeline does not produce an interactive visualization. We are\nworking on adding this! In the meantime, you can view the cross-correlation plot(s)\nas outlined in the sections below.\n\n\nFinding the raw results files\n\n\nNavigate to the \ntool's landing page\n. \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view of your cloud workspace. This is similar to the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files. To access ChIP-Seq results, you should click on the \n\nResults\n folder, then select the output folder name you gave in the \nselecting parameters\n part of the guide.\n\n\n\n\nInterpreting results\n\n\nFor the ChIP-Seq pipeline, every pipeline run outputs a \nREADME.doc\n file\nwhich contains the latest information on which results are included.\nYou can refer to that file for the most up to date information on raw outputs.\n\n\n\n\nFrequently asked questions\n\n\nIf you have any questions not covered here, feel free to \n\ncontact us\n.\n\n\nQ: Should I choose narrow peak calling pipeline or broad peak calling pipeline?\n\n\nA. We built two workflows: one for narrow peak calling and another broad\npeak calling. If your target protein is a transcription factor, please\nuse narrow peak calling workflow. For histone marks H3K4me3, H3K4me2,\nH3K9-14ac, H3K27ac and H2A.Z, you could try narrow peak calling\nworkflow. For histone marks H3K36me3, H3K79me2, H3K27me3, H3K9me3 and\nH3K9me1, you could try broad peak calling workflow. In some scenario,\nH3K4me1, H3K9me2 and H3K9me3 might behave between narrow and broad\nshape, you might need to look into each peak region and consult\nexperts.\n\n\nQ. What to do if your fragment size is less than 50 base pairs?\n\n\nA. We estimate fragment size from the data based on the cross correlation\nplot. Usually the fragment size is above 50bp. If the estimated\nfragment size lower than 50bp, the workflow will stop at the peak\ncalling stage (MACS2/SICER) after BWA mapping finishes. You can rerun\nthe analysis with a specified fragment length.", 
            "title": "ChIP-Seq Peak Calling"
        }, 
        {
            "location": "/guides/tools/chipseq/#overview", 
            "text": "Inputs     Name  Type  Description  Example      FastQ files ( required  if using FastQ inputs)  Input file  Gzipped FastQ files generated by experiment.  Sample_R1.fastq.gz and Sample_R2.fastq.gz     Outputs     Name  Format  Description      BED file  .bed  Peak calls    Binary file  .bb  Binary format for BED file    BigWig file  .bw  Shows read coverage    Metrics file  .txt  Shows mapping and duplication rate    Cross correlation plot  .pdf  Quality plot showing if the forward and reverse reads tend to be centered around binding sites.     Process   The reads of the FastQ file(s) are aligned to the specified reference genome.   The aligned reads are then post-processed based on best-practice QC techniques\n(removing multiple mapped reads, removing duplicated reads, etc).   Peaks are called by SICER (broad peak analysis) or MACS2 (narrow peak\nanalysis).   Qualified peaks will be output as BED (.bed) and big BED (.bb)\nfiles.   The coverage information will be output as a bigWig (.bw)\nfile.   A cross correlation plot and general metrics file are generated to help check\nthe quality of experiment.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/chipseq/#getting-started", 
            "text": "To get started, you need to navigate to the  ChIP-Seq tool page . You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.    Note  If you can't see the \"Start\" button, one of these two scenarios is likely the case:   You see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.  If you cannot see  any  buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.   If neither of these are the case and you still can't click \"Start\", contact us .", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/chipseq/#uploading-data", 
            "text": "The ChIP-Seq Peak Caller takes Gzipped FastQ files generated from an\nIP experiment as input. You can upload your input FastQ files by\nusing the  data transfer application \nor by uploading them through  the command line .\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.   Tip  If you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!  For more information, check out the  data transfer application  guide.", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/chipseq/#running-the-tool", 
            "text": "Once you've uploaded data to your cloud workspace, \nclick \"Launch Tool\" on the  tool's landing page . \nA dropdown will present the different presets for running the ChIP-Seq workflow.\nYou'll need to decide  (1)  whether you'd like to run broad/narrow peak\ncalling and  (2)  whether you have a case sample and a control sample (preferred)\nor just a case sample. This will determine which preset you should\nclick in this dropdown. There are various other parameters that you can \nset, but they are covered in further sections of this guide.", 
            "title": "Running the tool"
        }, 
        {
            "location": "/guides/tools/chipseq/#broad-vs-narrow-peak-calling", 
            "text": "Choosing between broad and narrow peak calling depends on the experiment\ndesign. The following are good rules of thumb for choosing between the\ntwo configurations. If you are not sure which configuration to use,\nplease consult with an expert at your institution or  contact us .  Narrow Peak Calling  If your target protein is a transcription factor, you should probably\nchoose narrow peak calling. You can also try the narrow peak calling\nworkflows for the following histone marks:   H3K4me3  H3K4me2  H3K9-14ac  H3K27ac  H2A.Z   Broad Peak Calling  You should try the broad peak calling workflows for the following\nhistone marks:   H3K36me3  H3K79me2  H3K27me3  H3K9me3  H3K9me1   Special Cases  In some scenarios, H3K4me1, H3K9me2 and H3K9me3 might behave between\nnarrow and broad shape, you might need to look into each peak region and\nconsult experts.   Warning  If your fragment size is less than 50 base pairs, please refer to the frequently asked questions .", 
            "title": "Broad vs. narrow peak calling"
        }, 
        {
            "location": "/guides/tools/chipseq/#selecting-parameters", 
            "text": "There are a number of other parameters that can be customized. To \nsee the options available, click the gear cog next to the \n\"Parameter Wrapper\" substep.   The following are the parameters that can be set, a short\ndescription of each parameter, and an example value. If you\nhave questions, please  contact us .     Parameter Name  Description  Example      Output prefix ( required )  A name used a prefix for all outputs in the run  SAMPLE1    Reference genome ( required )  Supported reference genome from one of hg19, GRCh38, mm9, mm10, dm3  GRCh38    Output bigWig  Whether or not to include a bigwig file in the output  True    Remove blacklist peaks  Whether or not to remove known problem areas  True    Fragment length  Hardcoded fragment length of your reads. 'NA' for auto-detect.  NA      Caution  Please be aware of the following stumbling points when setting parameters:   Do not use spaces anywhere in your input file names, your output\n  prefix, or any of the other parameters. This is generally bad\n  practice and doesn't play well with the pipeline (consider using\n  \"_\" instead).  Do not change the output directory when you run the pipeline. At\n  the top of parameter input page, there is a text box that allows\n  you to change the output folder.  Please ignore that setting . You\n  only need to specify an output prefix as described above. All of\n  the results will be put under  /Results/[OUTPUT_PREFIX] .", 
            "title": "Selecting parameters"
        }, 
        {
            "location": "/guides/tools/chipseq/#hooking-up-inputs", 
            "text": "Next, you'll need to hook up the FastQ files you uploaded in  the upload data section . You can do this by \nclicking on the  ChIP Reads  and  Control Reads  slots and\nselecting the respective files. If you are not doing a case/control\nrun, you only need to hook up the case sample.", 
            "title": "Hooking up inputs"
        }, 
        {
            "location": "/guides/tools/chipseq/#starting-the-workflow", 
            "text": "Once your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.    Tip  If you cannot click this button, please ensure that:   all of the inputs are correctly hooked up (see  hooking up inputs ), and   all of the required parameters are set (see  setting parameters ).   If you're still have trouble, please  contact us  and include\na screenshot of the workflow screen above.", 
            "title": "Starting the workflow"
        }, 
        {
            "location": "/guides/tools/chipseq/#monitoring-run-progress", 
            "text": "Once you have started one or more ChIP-Seq runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the  tool's landing page . \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.     You can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand the job logs can be accessed by clicking around the sub-items.      Tip  Power users can view the  DNAnexus Job Monitoring Tutorial  and the  DNAnexus Command Line Tutorial for Job Monitoring  for advanced capabilities for monitoring jobs.", 
            "title": "Monitoring run progress"
        }, 
        {
            "location": "/guides/tools/chipseq/#interactive-visualizations", 
            "text": "Today, the ChIP-Seq pipeline does not produce an interactive visualization. We are\nworking on adding this! In the meantime, you can view the cross-correlation plot(s)\nas outlined in the sections below.", 
            "title": "Interactive visualizations"
        }, 
        {
            "location": "/guides/tools/chipseq/#finding-the-raw-results-files", 
            "text": "Navigate to the  tool's landing page . \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view of your cloud workspace. This is similar to the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files. To access ChIP-Seq results, you should click on the  Results  folder, then select the output folder name you gave in the  selecting parameters  part of the guide.", 
            "title": "Finding the raw results files"
        }, 
        {
            "location": "/guides/tools/chipseq/#interpreting-results", 
            "text": "For the ChIP-Seq pipeline, every pipeline run outputs a  README.doc  file\nwhich contains the latest information on which results are included.\nYou can refer to that file for the most up to date information on raw outputs.", 
            "title": "Interpreting results"
        }, 
        {
            "location": "/guides/tools/chipseq/#frequently-asked-questions", 
            "text": "If you have any questions not covered here, feel free to  contact us .  Q: Should I choose narrow peak calling pipeline or broad peak calling pipeline?  A. We built two workflows: one for narrow peak calling and another broad\npeak calling. If your target protein is a transcription factor, please\nuse narrow peak calling workflow. For histone marks H3K4me3, H3K4me2,\nH3K9-14ac, H3K27ac and H2A.Z, you could try narrow peak calling\nworkflow. For histone marks H3K36me3, H3K79me2, H3K27me3, H3K9me3 and\nH3K9me1, you could try broad peak calling workflow. In some scenario,\nH3K4me1, H3K9me2 and H3K9me3 might behave between narrow and broad\nshape, you might need to look into each peak region and consult\nexperts.  Q. What to do if your fragment size is less than 50 base pairs?  A. We estimate fragment size from the data based on the cross correlation\nplot. Usually the fragment size is above 50bp. If the estimated\nfragment size lower than 50bp, the workflow will stop at the peak\ncalling stage (MACS2/SICER) after BWA mapping finishes. You can rerun\nthe analysis with a specified fragment length.", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/", 
            "text": "Authors\n\n\nScott Newman, Clay McLeod, Yongjin Li\n\n\n\n\n\n\nPublication\n\n\nN/A (not published)\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nFusion genes are important for cancer diagnosis, subtype definition and\ntargeted therapy. RNASeq is useful for detecting fusion transcripts.\nComputational methods face challenges to identify fusion transcripts\narising from internal tandem duplication (ITD), multiple genes, low\nexpression or non-templated insertions. Here we present an end-to-end\nclinically validated pipeline \"Rapid RNA-Seq\" that detects gene fusions\nand ITDs from human RNA-Seq.\n\n\nOverview\n\n\nInputs\n\n\nThe input can be either of the two entries below based on whether you want to start\nwith FastQ files or a BAM file.\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nPaired FastQ files\n\n\nGzipped FastQ files generated by human RNA-Seq\n\n\nSample_R1.fastq.gz and Sample_R2.fastq.gz\n\n\n\n\n\n\nBAM file\n\n\nAligned reads file from human RNA-Seq\n\n\nSample.bam\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\nIf you provide a BAM file to the pipeline, it \nmust\n be aligned to GRCh37-lite.\nRunning a BAM aligned to any other reference genome is not supported. Maybe more\nimportantly, we do not check the genome build of the BAM, so errors in computation\nor the results can occur. If your BAM is \nnot\n aligned to this genome build, we \nrecommend converting the BAM back to FastQ files using \n\nPicard's SamToFastq\n\nfunctionality and using the FastQ version of the pipeline.\n\n\n\n\nOutputs\n\n\nThe Rapid RNA-Seq pipeline produces the following outputs:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPredicted gene fusions (.txt)\n\n\nFile containing putative gene fusions.\n\n\n\n\n\n\nCoverage file (.bw)\n\n\nbigWig file containing coverage information.\n\n\n\n\n\n\nSplice junction read counts (.txt)\n\n\nRead counts for the splice junction detected.\n\n\n\n\n\n\nInteractive fusion visualization\n\n\nFusion visualization produced by ProteinPaint.\n\n\n\n\n\n\nInteractive coverage visualization\n\n\nCoverage visualization produced by ProteinPaint.\n\n\n\n\n\n\n\n\nProcess\n\n\n\n\nThe raw sequence data is aligned to GRCh37-lite using standard STAR\n   mapping.\n\n\nA coverage bigWig (.bw) file is produced to allow the user to assess\n   sample quality across the genome.\n\n\nTwo gene fusion detection algorithms are run in parallel.\n\n\nThe \nFuzzion\n (Rice et al. unpublished data) fusion detection\n  algorithm is run to provide high sensitivity for recurrent gene\n  fusions.\n\n\nThe \nRNAPEG\n (Edmonson et al. unpublished data) splice\n  junction read counting algorithm is run to quantify read counts\n  for splice junctions. These splice junction read counts are then\n  used by \nCicero\n (Li et al. unpublished data) to detect\n  putative gene fusions.\n\n\n\n\n\n\nCustom visualizations for putative gene fusions and genome coverage\n   are produced by ProteinPaint.\n\n\n\n\nMapping\n\n\nWe use the \nSTAR aligner\n \nto rapidly map reads to the GRCh37 human reference genome. This step generally \ntakes around one hour to complete assuming approximately 55-75 million paired reads\nare supplied.\n\n\nCoverage\n\n\nInternally developed scripts calculate the coverage of mapped reads\ngenome wide. The resulting bigWig file can be viewed in ProteinPaint or\nused for quality control.\n\n\nSplice junction read quantification\n\n\nWe use our RNAPEG software to quantify reads spanning known and novel\nsplice junctions. RNAPEG also corrects improper mappings at splice\njunction boundaries for more accurate definition of novel splice\njunctions. The resulting junctions file can be viewed along with the\ncoverage bigWig file to gain insights into gene expression and splicing\npatterns\n\n\nGenome-wide fusion prediction\n\n\nWe developed an assembly-based algorithm CICERO (Clipped-reads Extended\nfor RNA Optimization) that is able to extend the read-length spanning\nfusion junctions for detecting complex fusions. CICERO finds clipped\nreads and junction spanning reads, assembles them into a contig and maps\nthe contig back to the reference genome. Mapped contigs are then\nannotated and filtered. Those with potential genic effects including\ngene fusion, ITD, readthrough or circular RNA are reported in the\n\nfinal_fusions.txt\n file. An interactive version of this file with\npredictions sorted by quality can be inspected with the ProteinPaint\ninteractive fusion viewer.\n\n\nAn abstract describing CICERO was presented at ASHG, 2014:\n\nhttp://www.ashg.org/2014meeting/abstracts/fulltext/f140120024.htm\n\n\nLow stringency fusion gene \"Hotpot\" search\n\n\nWe have observed that certain fusions such as KIAA1549-BRAF in low-grade\nglioma have apparently limited read support in the bam file \u2014 either due\nto low expression or low tumor purity. In these cases, we use a\nsecondary tool, FUZZION, that performs fuzzy matching for known fusion\ngene junctions for every read in the bam file (both mapped and\nunmapped). FUZZION can recover even a single low quality read\npotentially supporting a known fusion gene junction. The FUZZION output\nis a simple text file with read IDs and sequences supporting a\nparticular gene fusion. The fusion point is indicated with square\nbrackets \n[]\n.\n\n\nGetting started\n\n\n\n\nCaution\n\n\nThis pipeline assumes GRCh37-lite coordinates. If your BAM is \n\nnot\n aligned to this genome build, we recommend converting the BAM \nback to FastQ files using \nPicard's SamToFastq\n\nfunctionality.\n\n\n\n\nTo get started, you need to navigate to the \nRapid RNA-Seq tool page\n. You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.\n\n\n\n\n\n\nNote\n\n\nIf you can't see the \"Start\" button, one of these two scenarios is likely the case:\n\n\n\n\nYou see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.\n\n\nIf you cannot see \nany\n buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.\n\n\n\n\nIf neither of these are the case and you still can't click \"Start\",\n\ncontact us\n.\n\n\n\n\nUploading data\n\n\nThe Rapid RNA-Seq pipeline takes either a paired set of Gzipped FastQ files or \na GRCh37-lite aligned BAM from human RNA-Seq. You can upload your input file(s)\nusing the \ndata transfer application\n\nor by uploading them through \nthe command line\n.\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.\n\n\n\n\nTip\n\n\nIf you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!\n\n\nFor more information, check out the \ndata transfer application\n guide.\n\n\n\n\nRunning the tool\n\n\nOnce you've uploaded data to your cloud workspace, click \"Launch Tool\" on the \ntool's landing page\n. A dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with FastQ files or a BAM file.\n\n\n\n\nHooking up inputs\n\n\nNext, you'll need to hook up the FastQ files you uploaded in \n\nthe upload data section\n. In this example,\nwe are using the FastQ version of the pipeline, so you can \nhook up the inputs by clicking on the \nFastq/R1\n and \nFastq/R2\n\nslots and selecting the respective files. If you are using \nthe BAM-based workflow, the process is similar.\n\n\n\n\nStarting the workflow\n\n\nOnce your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.\n\n\n\n\n\n\nTip\n\n\nIf you cannot click this button, please ensure that all of the inputs are correctly hooked up (see \nhooking up inputs\n).\n\n\nIf you're still have trouble, please \ncontact us\n and include\na screenshot of the workflow screen above.\n\n\n\n\nMonitoring run progress\n\n\nOnce you have started one or more Rapid RNA-Seq runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the \ntool's landing page\n. \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.\n\n\n \n\n\nYou can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.\n\n\n \n\n\n\n\nTip\n\n\nPower users can view the \nDNAnexus Job Monitoring Tutorial\n and the \nDNAnexus Command Line Tutorial for Job Monitoring\n for advanced capabilities for monitoring jobs.\n\n\n\n\nAnalysis of results\n\n\nEach tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.\n\n\nFinding the raw results files\n\n\nNavigate to the \ntool's landing page\n. \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.\n\n\n \n\n\nInterpreting results\n\n\nThe complete output file specification is listed in the \noverview section\n\nof this guide. Here, we will discuss each of the different output files in more detail.\n\n\n\n\nPredicted gene fusions\n: The putative gene fusions will be\n  contained in the file \n[SAMPLE].final_fusions.txt\n. This file is a\n  tab-delimited file containing many fields for each of the predicted\n  SV. The most important columns are the following.\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsample\n\n\nSample name\n\n\n\n\n\n\ngene*\n\n\nGene name\n\n\n\n\n\n\nchr*\n\n\nChromosome name\n\n\n\n\n\n\npos*\n\n\nGenomic Location\n\n\n\n\n\n\nort*\n\n\nStrand\n\n\n\n\n\n\nreads*\n\n\nSupporting reads\n\n\n\n\n\n\nmedal\n\n\nEstimated pathogenicity assessment using St. Jude Medal Ceremony\n\n\n\n\n\n\n\n\n\n\nCoverage file\n: A standard\n    \nbigWig\n file\n    used to describe genomic read coverage.\n\n\nSplice junction read counts\n: A custom file format describing the\n    junction read counts. The following fields are included in the\n    tab-delimited output file.\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njunction\n\n\nSplice junction in the TCGA format \"chrX:start:+,chrX:end,+\". \"start\" and \"end\" are the 1-based position of the last mapped nucleotide before the skip and the first mapped nucleotide after the skip (i.e. the last base of the previous exon and the first base of the next exon). Note that in \n.bed\n output these coordinates will be different, see the .bed output section below. The \"+\" is currently hardcoded, though this may change in the future.\n\n\n\n\n\n\ncount\n\n\nRaw count of reads supporting the junction. During correction counts for ambiguous junctions can be combined, though obviously these additional reads will not be visible in the raw BAM file.\n\n\n\n\n\n\ntype\n\n\nEither \"known\" (matching a reference junction) or \"novel\" (not observed in the reference junction collection).\n\n\n\n\n\n\ngenes\n\n\nGene symbols from the junction calling process. These still need work in the raw junction calling process, it's recommended to use the \"annotated\" output files instead which assign matching HUGO gene symbols based on the UCSC refGene table.\n\n\n\n\n\n\ntranscripts\n\n\nList of known transcript IDs matching the junction.\n\n\n\n\n\n\nqc_flanking\n\n\nCount of supporting reads passing flanking sequence checks (junctions observed adjacent to read ends require 18+ nt of flanking sequence, otherwise 10+ nt).\n\n\n\n\n\n\nqc_plus\n\n\nCount of supporting reads aligned to the + strand.\n\n\n\n\n\n\nqc_minus\n\n\nCount of supporting reads aligned to the - strand.\n\n\n\n\n\n\nqc_perfect_reads\n\n\nCount of supporting reads with perfect alignments (no reference mismatches of quality 15+, indels, or soft clips).\n\n\n\n\n\n\nqc_clean_reads\n\n\nCount of supporting reads whose alignments are not perfect but which have a ratio of \n= 5% of reference mismatches of quality 15+, indels, or soft clips relative to the count of aligned bases on both the left and right flanking sequence. Note: qc_clean_reads does NOT include qc_perfect_reads: to get a count of \"perfect plus pretty good\" reads the two values must be added together.\n\n\n\n\n\n\n\n\nKnown issues\n\n\n\n\nAdapter contamination\n\n\nThis pipeline does not, at present, remove adapter sequences. If the\nsequencing library is contaminated with adapters, CICERO runtimes can\nincrease exponentially. We recommend running FastQ files through a QC\npipeline such as FastQC and trimming adapters with tools such as\nTrimmomatic if adapters are found.\n\n\n\n\n\n\nHigh coverage regions\n\n\nCertain cell types show very high transcription of certain loci, for\nexample, the immunoglobulin heavy chain locus in plasma cells. The\npresence of very highly covered regions (typically 100,000-1,000,000+ X)\nhas an adverse effect on CICERO runtimes. Presently, we have no good\nsolution to this problem as strategies such as down-sampling may reduce\nsensitivity over important regions of the genome.\n\n\n\n\n\n\nInteractive Visualizations Exon vs Intron Nomenclature\n\n\nWhen a codon is split over a fusion gene junction, the annotation\nsoftware marks the event as intronic when really, the event should be\nexonic. We are working to fix this bug. In the mean time, if a fusion is\npredicted to be in frame but the interactive plot shows \"intronic\", we\nsuggest the user blat the contig shown just below to clarify if the true\njunction is either in the intron or exon.\n\n\n\n\nFrequently asked questions\n\n\nNone yet! If you have any questions not covered here, feel free to reach\nout on \nour contact form\n.", 
            "title": "Rapid RNA-Seq Fusion Detection"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#overview", 
            "text": "Inputs  The input can be either of the two entries below based on whether you want to start\nwith FastQ files or a BAM file.     Name  Description  Example      Paired FastQ files  Gzipped FastQ files generated by human RNA-Seq  Sample_R1.fastq.gz and Sample_R2.fastq.gz    BAM file  Aligned reads file from human RNA-Seq  Sample.bam      Caution  If you provide a BAM file to the pipeline, it  must  be aligned to GRCh37-lite.\nRunning a BAM aligned to any other reference genome is not supported. Maybe more\nimportantly, we do not check the genome build of the BAM, so errors in computation\nor the results can occur. If your BAM is  not  aligned to this genome build, we \nrecommend converting the BAM back to FastQ files using  Picard's SamToFastq \nfunctionality and using the FastQ version of the pipeline.   Outputs  The Rapid RNA-Seq pipeline produces the following outputs:     Name  Description      Predicted gene fusions (.txt)  File containing putative gene fusions.    Coverage file (.bw)  bigWig file containing coverage information.    Splice junction read counts (.txt)  Read counts for the splice junction detected.    Interactive fusion visualization  Fusion visualization produced by ProteinPaint.    Interactive coverage visualization  Coverage visualization produced by ProteinPaint.     Process   The raw sequence data is aligned to GRCh37-lite using standard STAR\n   mapping.  A coverage bigWig (.bw) file is produced to allow the user to assess\n   sample quality across the genome.  Two gene fusion detection algorithms are run in parallel.  The  Fuzzion  (Rice et al. unpublished data) fusion detection\n  algorithm is run to provide high sensitivity for recurrent gene\n  fusions.  The  RNAPEG  (Edmonson et al. unpublished data) splice\n  junction read counting algorithm is run to quantify read counts\n  for splice junctions. These splice junction read counts are then\n  used by  Cicero  (Li et al. unpublished data) to detect\n  putative gene fusions.    Custom visualizations for putative gene fusions and genome coverage\n   are produced by ProteinPaint.   Mapping  We use the  STAR aligner  \nto rapidly map reads to the GRCh37 human reference genome. This step generally \ntakes around one hour to complete assuming approximately 55-75 million paired reads\nare supplied.  Coverage  Internally developed scripts calculate the coverage of mapped reads\ngenome wide. The resulting bigWig file can be viewed in ProteinPaint or\nused for quality control.  Splice junction read quantification  We use our RNAPEG software to quantify reads spanning known and novel\nsplice junctions. RNAPEG also corrects improper mappings at splice\njunction boundaries for more accurate definition of novel splice\njunctions. The resulting junctions file can be viewed along with the\ncoverage bigWig file to gain insights into gene expression and splicing\npatterns  Genome-wide fusion prediction  We developed an assembly-based algorithm CICERO (Clipped-reads Extended\nfor RNA Optimization) that is able to extend the read-length spanning\nfusion junctions for detecting complex fusions. CICERO finds clipped\nreads and junction spanning reads, assembles them into a contig and maps\nthe contig back to the reference genome. Mapped contigs are then\nannotated and filtered. Those with potential genic effects including\ngene fusion, ITD, readthrough or circular RNA are reported in the final_fusions.txt  file. An interactive version of this file with\npredictions sorted by quality can be inspected with the ProteinPaint\ninteractive fusion viewer.  An abstract describing CICERO was presented at ASHG, 2014: http://www.ashg.org/2014meeting/abstracts/fulltext/f140120024.htm  Low stringency fusion gene \"Hotpot\" search  We have observed that certain fusions such as KIAA1549-BRAF in low-grade\nglioma have apparently limited read support in the bam file \u2014 either due\nto low expression or low tumor purity. In these cases, we use a\nsecondary tool, FUZZION, that performs fuzzy matching for known fusion\ngene junctions for every read in the bam file (both mapped and\nunmapped). FUZZION can recover even a single low quality read\npotentially supporting a known fusion gene junction. The FUZZION output\nis a simple text file with read IDs and sequences supporting a\nparticular gene fusion. The fusion point is indicated with square\nbrackets  [] .", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#getting-started", 
            "text": "Caution  This pipeline assumes GRCh37-lite coordinates. If your BAM is  not  aligned to this genome build, we recommend converting the BAM \nback to FastQ files using  Picard's SamToFastq \nfunctionality.   To get started, you need to navigate to the  Rapid RNA-Seq tool page . You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.    Note  If you can't see the \"Start\" button, one of these two scenarios is likely the case:   You see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.  If you cannot see  any  buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.   If neither of these are the case and you still can't click \"Start\", contact us .", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#uploading-data", 
            "text": "The Rapid RNA-Seq pipeline takes either a paired set of Gzipped FastQ files or \na GRCh37-lite aligned BAM from human RNA-Seq. You can upload your input file(s)\nusing the  data transfer application \nor by uploading them through  the command line .\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.   Tip  If you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!  For more information, check out the  data transfer application  guide.", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#running-the-tool", 
            "text": "Once you've uploaded data to your cloud workspace, click \"Launch Tool\" on the  tool's landing page . A dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with FastQ files or a BAM file.", 
            "title": "Running the tool"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#hooking-up-inputs", 
            "text": "Next, you'll need to hook up the FastQ files you uploaded in  the upload data section . In this example,\nwe are using the FastQ version of the pipeline, so you can \nhook up the inputs by clicking on the  Fastq/R1  and  Fastq/R2 \nslots and selecting the respective files. If you are using \nthe BAM-based workflow, the process is similar.", 
            "title": "Hooking up inputs"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#starting-the-workflow", 
            "text": "Once your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.    Tip  If you cannot click this button, please ensure that all of the inputs are correctly hooked up (see  hooking up inputs ).  If you're still have trouble, please  contact us  and include\na screenshot of the workflow screen above.", 
            "title": "Starting the workflow"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#monitoring-run-progress", 
            "text": "Once you have started one or more Rapid RNA-Seq runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the  tool's landing page . \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.     You can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.      Tip  Power users can view the  DNAnexus Job Monitoring Tutorial  and the  DNAnexus Command Line Tutorial for Job Monitoring  for advanced capabilities for monitoring jobs.", 
            "title": "Monitoring run progress"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#analysis-of-results", 
            "text": "Each tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.", 
            "title": "Analysis of results"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#finding-the-raw-results-files", 
            "text": "Navigate to the  tool's landing page . \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.", 
            "title": "Finding the raw results files"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#interpreting-results", 
            "text": "The complete output file specification is listed in the  overview section \nof this guide. Here, we will discuss each of the different output files in more detail.   Predicted gene fusions : The putative gene fusions will be\n  contained in the file  [SAMPLE].final_fusions.txt . This file is a\n  tab-delimited file containing many fields for each of the predicted\n  SV. The most important columns are the following.      Field Name  Description      sample  Sample name    gene*  Gene name    chr*  Chromosome name    pos*  Genomic Location    ort*  Strand    reads*  Supporting reads    medal  Estimated pathogenicity assessment using St. Jude Medal Ceremony      Coverage file : A standard\n     bigWig  file\n    used to describe genomic read coverage.  Splice junction read counts : A custom file format describing the\n    junction read counts. The following fields are included in the\n    tab-delimited output file.      Field Name  Description      junction  Splice junction in the TCGA format \"chrX:start:+,chrX:end,+\". \"start\" and \"end\" are the 1-based position of the last mapped nucleotide before the skip and the first mapped nucleotide after the skip (i.e. the last base of the previous exon and the first base of the next exon). Note that in  .bed  output these coordinates will be different, see the .bed output section below. The \"+\" is currently hardcoded, though this may change in the future.    count  Raw count of reads supporting the junction. During correction counts for ambiguous junctions can be combined, though obviously these additional reads will not be visible in the raw BAM file.    type  Either \"known\" (matching a reference junction) or \"novel\" (not observed in the reference junction collection).    genes  Gene symbols from the junction calling process. These still need work in the raw junction calling process, it's recommended to use the \"annotated\" output files instead which assign matching HUGO gene symbols based on the UCSC refGene table.    transcripts  List of known transcript IDs matching the junction.    qc_flanking  Count of supporting reads passing flanking sequence checks (junctions observed adjacent to read ends require 18+ nt of flanking sequence, otherwise 10+ nt).    qc_plus  Count of supporting reads aligned to the + strand.    qc_minus  Count of supporting reads aligned to the - strand.    qc_perfect_reads  Count of supporting reads with perfect alignments (no reference mismatches of quality 15+, indels, or soft clips).    qc_clean_reads  Count of supporting reads whose alignments are not perfect but which have a ratio of  = 5% of reference mismatches of quality 15+, indels, or soft clips relative to the count of aligned bases on both the left and right flanking sequence. Note: qc_clean_reads does NOT include qc_perfect_reads: to get a count of \"perfect plus pretty good\" reads the two values must be added together.", 
            "title": "Interpreting results"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#known-issues", 
            "text": "Adapter contamination  This pipeline does not, at present, remove adapter sequences. If the\nsequencing library is contaminated with adapters, CICERO runtimes can\nincrease exponentially. We recommend running FastQ files through a QC\npipeline such as FastQC and trimming adapters with tools such as\nTrimmomatic if adapters are found.    High coverage regions  Certain cell types show very high transcription of certain loci, for\nexample, the immunoglobulin heavy chain locus in plasma cells. The\npresence of very highly covered regions (typically 100,000-1,000,000+ X)\nhas an adverse effect on CICERO runtimes. Presently, we have no good\nsolution to this problem as strategies such as down-sampling may reduce\nsensitivity over important regions of the genome.    Interactive Visualizations Exon vs Intron Nomenclature  When a codon is split over a fusion gene junction, the annotation\nsoftware marks the event as intronic when really, the event should be\nexonic. We are working to fix this bug. In the mean time, if a fusion is\npredicted to be in frame but the interactive plot shows \"intronic\", we\nsuggest the user blat the contig shown just below to clarify if the true\njunction is either in the intron or exon.", 
            "title": "Known issues"
        }, 
        {
            "location": "/guides/tools/rapid-rnaseq/#frequently-asked-questions", 
            "text": "None yet! If you have any questions not covered here, feel free to reach\nout on  our contact form .", 
            "title": "Frequently asked questions"
        }, 
        {
            "location": "/guides/tools/warden/", 
            "text": "Authors\n\n\nLance Palmer\n\n\n\n\n\n\nPublication\n\n\nN/A (not published)\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nThe WARDEN (\nW\norkflow for the \nA\nnalysis of \nR\nNA-Seq \nD\nifferential \nE\nxpressio\nN\n)\nsoftware uses RNA-Seq sequence files to perform alignment, coverage\nanalysis, gene counts and differential expression analysis.\n\n\nOverview\n\n\nInputs\n\n\nThe WARDEN workflow requires 2 types of input files and along with two\nrequired parameters to be set. All other parameters are preset with sane defaults.\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nFastQ files (\nrequired\n)\n\n\nInput file(s)\n\n\nGzipped FastQ files generated by experiment\n\n\nSample1.fastq.gz, Sample2.fastq.gz\n\n\n\n\n\n\nSample sheet (\nrequired\n)\n\n\nInput file\n\n\nSample sheet generated and uploaded by the user\n\n\n*.txt\n\n\n\n\n\n\n\n\n\n\nTodo\n\n\nDelete these if they are in the parameters below.\n\n\n\n\n| Reference genome | Parameter | Supported reference genome (HG19, HG38, mm9, mm10, dm3, dm6) | HG38 |\n| Sequencing method | Parameter | Sequencing method of experiment (forward, reverse, unstranded) | forward |\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nFastQC Report\n\n\nQuality control analysis by FastQC.\n\n\n\n\n\n\nAligned BAM\n\n\nAligned BAM files from STAR mapping.\n\n\n\n\n\n\nSplice junctions\n\n\nSplice junction information from STAR mapping.\n\n\n\n\n\n\nCoverage files\n\n\nbigWig (\n.bw\n) and BED (\n.bed\n) files detailing coverage.\n\n\n\n\n\n\nGene counts\n\n\nGene counts generated by HT-Seq count.\n\n\n\n\n\n\nVOOM/LIMMA results\n\n\nPairwise comparisons of expression data. Requires at least 3 samples vs 3 samples.\n\n\n\n\n\n\nSimple DE analysis\n\n\nNo statistical analysis, requires only a 1 samples vs 1 sample comparison.\n\n\n\n\n\n\nMA/Volcano plots\n\n\nBoth of the above produce tabular outputs, MA plots and volcano plots.\n\n\n\n\n\n\n\n\nProcess\n\n\n\n\nFastQ files generated by RNA-Seq are mapped to a reference genome using the STAR.\n\n\nHT-Seq count is used to assign mapped reads to genes. \n\n\nDifferential expression analysis is performed using VOOM normalization of counts and\nLIMMA analysis. \n\n\nCoverage plots of mapped reads are generated as interactive visualizations.\n\n\n\n\nGetting started\n\n\nTo get started, you need to navigate to the \nWARDEN tool page\n. You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.\n\n\n\n\n\n\nNote\n\n\nIf you can't see the \"Start\" button, one of these two scenarios is likely the case:\n\n\n\n\nYou see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.\n\n\nIf you cannot see \nany\n buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.\n\n\n\n\nIf neither of these are the case and you still can't click \"Start\",\n\ncontact us\n.\n\n\n\n\nUploading data\n\n\nThe WARDEN Differential Expression analysis pipeline takes Gzipped FastQ\nfiles generated by an RNA-Seq experiment as input. You can upload your input \nfile(s) using the \ndata transfer application\n\nor by uploading them through \nthe command line\n.\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.\n\n\n\n\nTip\n\n\nIf you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!\n\n\nFor more information, check out the \ndata transfer application\n guide.\n\n\n\n\nSample sheet\n\n\nOnce your data is uploaded, you'll need to create a sample sheet which\ndescribes the relationship between case and control samples,\nphenotype/condition information, and the comparisons you would like to\nperform. The sample sheet is a tab-delimited text document that can be\ncreated in Microsoft Excel (recommended) or a text editor.\n\n\n\n\nNote\n\n\nYou will need to upload your sample sheet in a similar manner as your\nFastQ files, so you can follow the \nsame uploading instructions\n \nto achieve this.\n\n\n\n\nPrepare using Microsoft Excel\n\n\n\n\nTip\n\n\nDownload the \nfile_download\n sample excel spreadsheet\n as a starting\npoint!\n\n\n\n\nThe final product for the excel spreadsheet will look like the\nscreenshot below. If you create the sample sheet from scratch, please\nensure the the columns are \nexactly\n in this order.\n\n\n\n\n\n\n\n\nSample rows\n\n\nEach row in the spreadsheet (except for the last row, which we will talk \nabout in the next section) corresponds to a sample with one or more FastQ files. You should fill in these rows based on your data and the guidelines below:\n\n\n\n\nGuidelines\n\n\n\n\nThe sample name should be unique and should only contain letters,\nnumbers and underscores.\n\n\nThe condition/phenotype column associates similar samples together.\nThe values should contain only letters, numbers and underscores.\n\n\nReadFile1 should contain forward reads (e.g. \n*.R1.fastq.gz\n or \n*_1.fastq.gz\n).\n\n\nReadFile2 will contain reads in reverse orientation to ReadFile2\n(e.g. \n*.R2.fastq.gz\n or \n*_2.fastq.gz\n).\n\n\nFor single end reads a single dash ('-') should be entered in the ReadFile2 column.\n\n\n\n\n\n\nComparison row\n\n\nThe last line in the sample sheet is called the \"comparison row\". This\nline specifies the comparisons to be done between conditions/phenotypes.\nAll pairwise combinations of the values in the \"Phenotype\" column can be \nanalyzed. To specify the comparisons, on a separate line, include \n#comparisons=\n followed be a comma delimited list of two conditions separated by a dash. \n\n\n\n\nExample\n\n\nThe following lines are all valid examples.\n\n\n\n\n#comparisons=KO-WT\n\n\n#comparisons=Condition1-Control,Condition2-Control\n\n\n#comparisons=Phenotype2-Phenotype1,Phenotype3-Phenotype2,Phenotype3-Phenotype1\n\n\n\n\n\n\n\n\nNote\n\n\nIf a comparison has at least 3 samples for each condition/phenotype,\nVOOM/LIMMA will be run. A simple differential comparison will be run on\nall samples.\n\n\n\n\nFinalizing the sample sheet\n\n\nTo finalize the sample sheet, save the Microsoft Excel file with\nwhatever name you like. Save the file as an Excel Workbook with the\n.xlsx extension.\n\n\nPrepare using a text editor\n\n\n\n\nTip\n\n\nDownload the \nfile_download\n sample text file\n as a starting\npoint!\n\n\n\n\nCreating a sample sheet with a text editor is an option for advanced\nusers. The process of creating a sample sheet with a text editor is the same as\ncreating one with Microsoft Excel, with the small difference that you\nmust manually create your columns using the tab character. Save the file\nwith a .txt extension.\n\n\nRunning the tool\n\n\n\n\nNote\n\n\nThe WARDEN tool operation is slightly different than the other pipelines\nbecause it accepts a variable number of samples. \nFirst\n, you will run\na \"bootstrapping\" step that creates a custom executable for your\nanalysis. \nSecond\n, you will need to manually execute the generated\nworkflow from the first step. This allows us to take advantage of many\nnice features, like check-pointing and cost reduction. Don't worry, \nwe'll show you how to do this step by step below.\n\n\n\n\nOnce you've uploaded data to your cloud workspace, click \"Launch Tool\" on the \ntool's landing page\n. You will be redirected to the virtual\ncloud workspace with the workflow screen opened for you.\n\n\n\n\nHooking up inputs\n\n\nNext, you'll need to hook up the FastQ files and sample sheet\n you uploaded in \nthe upload data section\n. \nClick the \nFASTQ_FILES\n input field and select \nall\n FastQ files.\nNext, click the \nsampleList\n input field and select the corresponding\nsamplesheet.\n\n\n\n\nSelecting parameters\n\n\nWe now need to configure the parameters for the pipeline, such as reference\ngenome and sequencing method. You can access all of the available parameters \nby clicking on the \nWARDEN WORKFLOW GENERATOR\n substep.\n\n\n\n\nParameter setup steps\n\n\n\n\nIn the \nOutput Folder\n field, select a folder to output to. You can\nstructure your experiments however you like (e.g. \n/My_Outputs\n)\n\n\nIn the \nanalysisName\n field, enter a prefix for all of the output files. This\ncan be any value you want to use to remember this run. \nBe sure to use underscores\ninstead of spaces here!\n\n\nSelect the \nsequenceStandedness\n from the drop down menu. \nThis information can be determined from the sequencing or source \nof the data. If you don't know what to put here, select \"no\".\n\n\nSelect the \nGenome\n pulldown menu. Choose the appropriate box.\n\n\nThe LIMMA parameters can be left alone for most analyses. If you are\nan advanced LIMMA user, you can change the various settings exposed\nbelow the required parameters.\n\n\nWhen all parameters have been set, press the save button.\n\n\n\n\n\n\n\n\nStarting the workflow\n\n\nOnce your input files are hooked up and your parameters are set, \nyou should be able to start the workflow by clicking the \"Run as Analysis...\" \nbutton in the top right hand corner of the workflow dialog.\n\n\n\n\nTip\n\n\nIf you cannot click this button, please ensure that all of the inputs are correctly hooked up (see \nhooking up inputs\n).\n\n\nIf you're still have trouble, please \ncontact us\n and include\na screenshot of the workflow screen above.\n\n\n\n\nThe tool will begin running and will automatically take you to the Monitor page, where you should see that your workflow is \"In Progress\".\n\n\n\n\nWhen the custom workflow has finished generating, the word 'Done' will\nappear in green in the status column. This indicates that the\nbootstrapping step has completed successfully. \n\n\n\n\nCustom Workflow Process\n\n\n\n\nWait for the workflow generator to finish.\n\n\n\n\nClick on the WARDEN name in the name column.\n\n\n\n\n\n\nYou will now be on a page specific to the running of the workflow.\n    On the left side, you will see the inputs you selected for the\n    workflow generator. On the right side are the output files\n    (including the generated workflow). Select the generated workflow as\n    shown in the picture below.\n\n\n\n\n\n\n\n\nYou will now be within the output folder you specified earlier.\n    Select the file that begins with 'WARDEN WORKFLOW:'\n\n\n\n\n\n\n\n\nA workflow generated for your data will be presented to you. Select\n    'Run as analysis' in the upper right.\n\n\n\n\n\n\n\n\nThe workflow will initiate, and you will be brought to the 'Monitor'\n    page. (Note to get back to this page, you can select 'Monitor' on\n    one of the menu bars near the top ) Expand the the workflow progress\n    be selecting the '+' sign next to 'In Progress'\n\n\n\n\n\n\n\n\nAs parts of the pipeline are run, you will see different tasks in\n    different colors. Green means done, blue is running, orange is\n    waiting, and red means error.\n\n\n\n\n\n\n\n\nWhen done the status will be shown as 'Done'. Select the Workflow\n    name under Status.\n\n\n\n\n\n\n\n\nYou will be brought to a page that show more information about the\n    workflow analysis. Click on the output folder to go to the output.\n\n\n\n\n\n\n\n\nThe output folders will now be shown.\n\n\n\n\n\n\n\n\nFor a description of the output, please refer to \nNavigating Results\n.\n\n\nMonitoring run progress\n\n\nOnce you have started one or more WARDEN runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the \ntool's landing page\n. \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kick off\ngets one row in the Monitor section.\n\n\nYou can click the \"+\" on any of the runs to check \nthe status of individual steps of the pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.\n\n\n\n\nTip\n\n\nPower users can view the \nDNAnexus Job Monitoring Tutorial\n and the \nDNAnexus Command Line Tutorial for Job Monitoring\n for advanced capabilities for monitoring jobs.\n\n\n\n\nAnalysis of results\n\n\nEach tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.\n\n\nFinding the raw results files\n\n\nNavigate to the \ntool's landing page\n. \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.\n\n\n\n\nNavigating Results\n\n\n\n\nNote\n\n\nNavigating to the raw results of your runs is the same for all\npipelines. This guide will feature the \nrapid-rnaseq\n pipeline, but you can follow along for\nany tool.\n\n\n\n\nRaw result files\n\n\nNavigate to your tool's description page (for instance, Rapid RNA-Seq's\ndescription page is\n\nhere\n). You should\nsee a screen similar to the one in the screenshot below. In the left\nhand pane, select \"View Results Files\".\n\n\n\n\nYou should now be in the tool's workspace with access to files that you\nuploaded and results files that are generated. How/where the result\nfiles are generated are specific to each pipeline. Please refer to your\nindividual pipeline's documentation on where the output files are kept.\n\n\n\n\nCustom visualization results\n\n\nNavigate to your tool's description page (for instance, Rapid RNA-Seq's\ndescription page is\n\nhere\n). You should\nsee a screen similar to the one in the screenshot below. In the left\nhand pane, select \"Visualize Results\".\n\n\n\n\nYou should now be in the tool's workspace with access to files that you\nuploaded and results files that are generated. How/where the result\nfiles are generated are specific to each pipeline. Please refer to your\nindividual pipeline's documentation on where the output files are kept.\n\n\n\n\nInterpreting results\n\n\nThe complete output file specification is listed in the \noverview section\n\nof this guide. Here, we will discuss each of the different output files in more detail.\n\n\n\n\nPredicted gene fusions\n: The putative gene fusions will be\n  contained in the file \n[SAMPLE].final_fusions.txt\n. This file is a\n  tab-delimited file containing many fields for each of the predicted\n  SV. The most important columns are the following.\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsample\n\n\nSample name\n\n\n\n\n\n\ngene*\n\n\nGene name\n\n\n\n\n\n\nchr*\n\n\nChromosome name\n\n\n\n\n\n\npos*\n\n\nGenomic Location\n\n\n\n\n\n\nort*\n\n\nStrand\n\n\n\n\n\n\nreads*\n\n\nSupporting reads\n\n\n\n\n\n\nmedal\n\n\nEstimated pathogenicity assessment using St. Jude Medal Ceremony\n\n\n\n\n\n\n\n\n\n\nCoverage file\n: A standard\n    \nbigWig\n file\n    used to describe genomic read coverage.\n\n\nSplice junction read counts\n: A custom file format describing the\n    junction read counts. The following fields are included in the\n    tab-delimited output file.\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njunction\n\n\nSplice junction in the TCGA format \"chrX:start:+,chrX:end,+\". \"start\" and \"end\" are the 1-based position of the last mapped nucleotide before the skip and the first mapped nucleotide after the skip (i.e. the last base of the previous exon and the first base of the next exon). Note that in \n.bed\n output these coordinates will be different, see the .bed output section below. The \"+\" is currently hardcoded, though this may change in the future.\n\n\n\n\n\n\ncount\n\n\nRaw count of reads supporting the junction. During correction counts for ambiguous junctions can be combined, though obviously these additional reads will not be visible in the raw BAM file.\n\n\n\n\n\n\ntype\n\n\nEither \"known\" (matching a reference junction) or \"novel\" (not observed in the reference junction collection).\n\n\n\n\n\n\ngenes\n\n\nGene symbols from the junction calling process. These still need work in the raw junction calling process, it's recommended to use the \"annotated\" output files instead which assign matching HUGO gene symbols based on the UCSC refGene table.\n\n\n\n\n\n\ntranscripts\n\n\nList of known transcript IDs matching the junction.\n\n\n\n\n\n\nqc_flanking\n\n\nCount of supporting reads passing flanking sequence checks (junctions observed adjacent to read ends require 18+ nt of flanking sequence, otherwise 10+ nt).\n\n\n\n\n\n\nqc_plus\n\n\nCount of supporting reads aligned to the + strand.\n\n\n\n\n\n\nqc_minus\n\n\nCount of supporting reads aligned to the - strand.\n\n\n\n\n\n\nqc_perfect_reads\n\n\nCount of supporting reads with perfect alignments (no reference mismatches of quality 15+, indels, or soft clips).\n\n\n\n\n\n\nqc_clean_reads\n\n\nCount of supporting reads whose alignments are not perfect but which have a ratio of \n= 5% of reference mismatches of quality 15+, indels, or soft clips relative to the count of aligned bases on both the left and right flanking sequence. Note: qc_clean_reads does NOT include qc_perfect_reads: to get a count of \"perfect plus pretty good\" reads the two values must be added together.\n\n\n\n\n\n\n\n\nPrimary Results\n\n\nAlignment statistics\n\n\nSeveral files should be examined initially to determine the quality of\nthe results. \nalignmentStatistics.txt\n shows alignment statistics for\nall samples. This file is a plain text tab-delimited file that can be\nopened in Excel or a text editor such as Notepad++. This file contains\ninformation on the total reads per sample, the percantage of duplicate\nreads and the percentage of mapped reads. An example of this file is\nbelow. (Within the DNAnexus output directory structure, these files will\nbe in the COMBINED_FLAGSTAT directory.)\n\n\n\n\n\n\n\n\nMultidimensional scaling (MDS) Plot\n\n\nThe second set of files to look at are the Multidimensional scaling\n(MDS) plots (\nhttps://en.wikipedia.org/wiki/Multidimensional_scaling\n)\nusing the plotMDS function within LIMMA. Similar to PCA, these graphs\nwill show how similar samples are to each other. There are different\nsets of MDS plots. For comparisons where there are 3 or more samples per\ncondition, an MDS plot using Voom (Limma) normalized values are\ngenerated. An example can be seen below. These files will be labeled\n\nmdsPlot.png\n. For all comparisons, regardless of sample size, and MDS\nplot will also be generated with Counts per million (CPM) normalized\ngene counts. These files will be labeled \nmdsPlot.normCPM.png\n.\n(Within the DNAnexus output directory structure, these files will be in\nthe LIMMA directory.)\n\n\n\n\nMDS plot from just CPM normalized data.\n\n\n\n\nProteinPaint Visualizations\n\n\nSeveral files on DNAnexus allow the data to be viewed in the Protein\nPaint viewer. (Note: We plan to have links downloaded in the future to\nallow the viewing of these files off of DNAnexus.)\n\n\nLIMMA differential expression viewer\n\n\nWithin LIMMA/VIEWERS direcory (note if no comparisons meet the 3 sample\ncondition, the LIMMA folder will not exist), there will be a viewer file\nfor each valid comparison ( *\nresults.\n.txt.viewer**). Simply select\nthe file and press 'Launch viewer' in the lower right. A viewer will pop\nup showing both the MA Plot and Volcano plot. By moving the mouse over a\ncircle, the circle will hilight and the corresponding gene on the other\ngraph will also hilight. Additional information about the gene and its\nexpression values will also be shown. One can also type in multiple gene\nsymbols in the provided text box. By pressing 'Show gene labels' all\nthese genes will show up on the plots.\n\n\n\n\nSimple differential expression viewer\n\n\nThere will also be a viewer for the simple differential expresssion\nanalysis in SIMPLE_DIFEX/VIEWERS. The P-value for the results have all\nbeen set to 1, so the volcano plot will not be relevant.\n\n\nbigWig viewer\n\n\nIn the BIGWIG_VIEWER directory there will be a bigwigViewer file.\nSelect this file and then 'Launch viewer'. A graph of coverage for the\ngenome should be visible.\n\n\nSecondary Results\n\n\n\n\nInteractive MA/Volcano Plots\n\n\nIn addition to viewing the MA and volcano plots through the visualization tool\n\n\n\n\nDifferential expression results\n \n\n\nOther useful differential expression results will be downloaded by the desktop app. This included tabular output from the differential expression analysis. For each comparison with three or more samples per condition, \nresults.*.txt\n will be produced.\n\n\n\n\nGSEA.input.txt\n and \nGSEA.tStat.txt\n \n\n\nInput files that can be used for GSEA analysis. The tStat file is preferred for a more accurate analysis, but will not give a heatmap diagram.\n- (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.)\n\n\n\n\n\n\nFor plain text results from the simple differential expresison analysis, the files will be named \nsimpleDE.*.txt\n. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)\n\n\n\n\n\n\nPrelabelled MA and volcano plots are provided for the analysis. These files are labeled \nmaPlot.*.png\n and \nvolcanoPlot.*.png\n where \n*\n is the comparison (e.g. ko_vs_wt)\n\n\n\n\n\n\nThe MA plot shows the average expression of the gene on the X-axis, and Log2 fold change between condition/phenotype is on the Y-axis (if the name is for example maPlot.condition2-condition1.png then the fold change would represent condition1 minus condition2). Each gene is represented by a circle. The top 20 genes (by p-value) are identified on the plot. The genes are color coded by the chosen multiple testing correction method (False Discovery Rate (FDR) by default. An exmaple MA plot can be seen below. (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.)\n\n\n\n\n\n\n\n\nThe volcano plot shows the Log2Fold change between the conditions on the\nX-axis, and the -Log10 of the multiple testing corrected P-value on the\nY-axis.\n\n\n\n\nAn MA plot is generated for all comparisons regardless of number of\nsamples. This is the \nsimpleDEPlot.*.png\n no statistics are shown\nand genes are not labeled. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)\n\n\n\n\nDifferential analysis input\n\n\nInputs and commands are provided for rerunning differential expression\nanalysis on ones own computer. The R commands used for the analysis are\nfound in \nvoomLimma.R\n. An experienced R user can rerun the analysis\nwith any desired changes. This analysis requires the input\n\ncountFile.txt\n which contains counts per genes, the\n\nRparameters.txt\n file containing input parameters, and a processed\nsample list file \nsampleList.txt\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe LIMMA directory.)\n\n\nThe input for the simple differential analysis expression will be\n\nRparameters_simple.txt\n, \nsimpleDE.R\n, \ncountFile.txt\n and\n\nsampleList.txt\n. \ncountFile.txt\n and \nsampleList.txt\n are the\nsame files used by the LIMMA analysis.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe SIMPLE_DIFEX directory.)\n\n\n\n\nCoverage results\n\n\nbigWig files will be generated for use in genome browsers (such as IGV\n\nhttp://software.broadinstitute.org/software/igv/\n). For each smaple,\nmultiple bigWig files will be found. For all types of sequencing\nstrandedness, there will be bigWig files labelled,\n\n*.sortedCoverageFile.bed.bw\n where '\n' is the sample name. For\nstranded data there will also be*.sortedPosCoverageFile.bed.bw\n* and\n\n*.sortedNegCoverageFile.bed.bw\n which contains coverage information\nfor the positive and negative strand of the genome.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe BIGWIG directory.)\n\n\n\n\nQuality Control Results (FastQC)\n\n\nWithin the FastQC directory, foreach sample and read direction there\nwill be an html file and a zip file (\n*.FastQc.html\n\n\n*.FastQc.zip\n where '*' is the base FastQ name), containing results\nfrom FastQTC. For the average user the html file is sufficient. This\nfile can give some basic statistics on the quality of the data.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe FastQC directory.)\n\n\n\n\nBAM alignment files\n\n\nThere are two BAM files generated per sample that contain mapping\ninformation for all reads. The first is labeled\n\n*.Aligned.sortedByCoord.dup.bam\n where '\n' is the sample name. The\nBAM file is sorted by coordinates and has duplicates marked. The second\nfile is*.Aligned.toTranscriptome.out.bam\n* and contains reads mapped\nto transcripts.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe ALIGN directory.)\n\n\n\n\nChimeric reads and junction files\n\n\nAdditional files created by STAR are provided. More information on these\nfiles can be found at\n\nhttp://labshare.cshl.edu/shares/gingeraslab/www-data/dobin/STAR/STAR.posix/doc/STARmanual.pdf\n.\n\n*.SJ.out.tab\n contain splice junction information. Fusion detectino\nfiles are labelled \n*.Chimeric.out.bam\n and\n\n*.Chimeric.out.junction\n.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe ALIGN directory.)\n\n\n\n\nFPKM and count files (per sample)\n\n\nPer sample files containing FPKM and raw count values for each gene can\nbe found in \n*.fpkm.txt\n and \n*.htseq_counts.txt\n where '*' is\nthe sample name.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe \nNOTE FIND OUT WHICH DIRECTORY\n directory.)\n\n\n\n\nMethods Files\n\n\nA more human readable explanation is found in \nmethods.docx\n. Detailed\ndocumentation can be found in \nmethods.txt\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe METHODS directory.)\n\n\nAuxilary Files\n\n\nThis section describes the files that exist within the DNAnexus output\nfolder. Most of these files will not be of interest to the average user.\nHowever, interactive viewers are describe in \nLIMMA differential\nexpression viewer\n and \nSimple\ndifferential expression viewer\n.\n\n\nThe output will be divided into multiple folders. The results being the\nmost useful will be the differential expression analysis results in the\nLIMMA and SIMPLE_DIFEX folders. Bigwig files for viewing read coverage\nwill be in the BIGWIG folder. Other folder contain different types of\ndata and are explained in further detail below.\n\n\n\n\n\n\n\n\nThe following description of files is sorted by their output directory.\n\n\n\n\nALIGN\n\n\nThis directory contains the BAM files described in \nBAM alignment\nfiles\n and the chimeric and junction files are\ndescribed in \nChimeric reads and junction\nfiles\n. In adition there are 2 log\nfiles. \n*Log.final.out\n has relevant statistics for the alignemnt.\nThe \n*.Log.out\n file just contains a log of the analysis run,\nincluding input parameters. Per sample FLAGSTAT results are found in\n\n*.flagStatOut.txt\n. These flagstat files are combined into the file\n\nalignmentStatistics.txt\n described in \nInitial analysis of\nresults\n. Finally the ALIGN directory has\nmultiple \n.starAlign.methods.txt files. These files can be ignored as\nthey are summarized in the finalmethods.docx\n* and \nmethods.txt\n\nfiles described in \nMethods Files\n.\n\n\n\n\nBIGWIG\n\n\nAll of the files here are described in section \nCoverage\nresults\n. The \nbgToBw.methods.txt\n files can be\nignored as they are summarized in the files described in \nMethods\nFiles\n.\n\n\n\n\nBIGWIG_VIEWER\n\n\nSee \nbigWig viewer\n\n\n\n\nCOMBINED_FLAGSTAT\n\n\n\n\nTodo\n\n\nDescribe combined flagstat.\n\n\n\n\n\n\nCOMBINED_HTSEQ\n\n\nUsed for input in differential expression analysis. The\ncombineCountFile.txt is the same as countFile.txt described in\n\nDifferential analysis input\n\n\n\n\nCOVERAGE\n\n\nBED graph files used to generate bigWig files are here.\n\n\n\n\nFastQC\n\n\nSee \nQuality Control Results (FastQC)\n\n\n\n\nHTSEQ\n\n\nPer-sample HTSEQ-count results (\n*.htseq_counts.txt\n) and FPKM\nresults (\n*.fpkm.txt\n). Temporary methods files are found as\n*.htseq-count.methods.txt\n\n\n\n\nLIMMA\n\n\nmdsPlot.png\n, \nmaPlot.\n.png,volcanoPlot.\n.png\n are described in\n\nInitial analysis of results\n\n\nresults.\n.txt,GSEA.input.\n.txt\n and *\nGSEA.tStat.\n.txt** are\ndescribe in \nDifferential expression\nresults\n\n\nvoomLimma.R\n, \ncountFile.txt\n, \nRparameters.txt\n, and\n\nsampleList.txt\n are described in \nDifferential analysis\ninput\n\n\nSee \nLIMMA differential expression\nviewer\n for a description of the\nVIEWERS directory.\n\n\nOther files in the LIMMA directory include contrastFiles.txt\ncontrastsFile.txt, and limmaSampleList.txt which are used internally.\nlimmaMethods.txt is an intermediate file describing methods. Out.tar.gz\nis used for testing purposes. The sessionInfo.txt file describe the R\nsession working parameters and modules loaded. meanVariance.png is a\nplot for assessing quality of count data\n(\nhttps://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-2-r29\n)\n\n\n\n\nMETHODS\n\n\nThe files here are described in \nMethods Files\n.\n\n\n\n\nSAMPLELIST\n\n\nThese files are used internally by the pipeline.\n\n\n\n\nSIMPLE_DIFEX\n\n\nmdsPlot.normCPM.png\n and *\nsimpleDEPlot.\n.png** are described in\n\nInitial analysis of results\n\n\n*\nsimpleDE.\n.txt** are describe in \nDifferential expression\nresults\n\n\nsimpleDE.R\n, \ncountFile.txt\n, \nRparameters_simple.txt\n, and\n\nsampleList.txt\n are described in \nDifferential analysis\ninput\n\n\nSee \nSimple differential expression\nviewer\n for a description of the\nVIEWERS directory.\n\n\nOther files in the SIMPLE_DIFEX directory include contrastFiles.txt\ncontrastsFile.txt, and limmaSampleList.txt which are used internally.\nsimpleDifEx.methods.txt is an intermediate file describing methods.\nOut.tar.gz is used for testing purposes. The sessionInfo.txt file\ndescribe the R session working parameters and modules loaded.\n\n\nKnown Issues\n\n\n\n\nAdapter contamination\n\n\nThis pipeline does not, at present, remove adapter sequences. If the\nsequencing library is contaminated with adapters, CICERO runtimes can\nincrease exponentially. We recommend running FastQ files through a QC\npipeline such as FastQC and trimming adapters with tools such as\nTrimmomatic if adapters are found.\n\n\n\n\n\n\nHigh coverage regions\n\n\nCertain cell types show very high transcription of certain loci, for\nexample, the immunoglobulin heavy chain locus in plasma cells. The\npresence of very highly covered regions (typically 100,000-1,000,000+ X)\nhas an adverse effect on CICERO runtimes. Presently, we have no good\nsolution to this problem as strategies such as down-sampling may reduce\nsensitivity over important regions of the genome.\n\n\n\n\n\n\nInteractive Visualizations Exon vs Intron Nomenclature\n\n\nWhen a codon is split over a fusion gene junction, the annotation\nsoftware marks the event as intronic when really, the event should be\nexonic. We are working to fix this bug. In the mean time, if a fusion is\npredicted to be in frame but the interactive plot shows \"intronic\", we\nsuggest the user blat the contig shown just below to clarify if the true\njunction is either in the intron or exon.\n\n\n\n\nFrequently Asked Questions\n\n\nNone yet! If you have any questions not covered here, feel free to reach\nout on \nour contact\nform\n.", 
            "title": "WARDEN Differential Expression Analysis"
        }, 
        {
            "location": "/guides/tools/warden/#overview", 
            "text": "Inputs  The WARDEN workflow requires 2 types of input files and along with two\nrequired parameters to be set. All other parameters are preset with sane defaults.     Name  Type  Description  Example      FastQ files ( required )  Input file(s)  Gzipped FastQ files generated by experiment  Sample1.fastq.gz, Sample2.fastq.gz    Sample sheet ( required )  Input file  Sample sheet generated and uploaded by the user  *.txt      Todo  Delete these if they are in the parameters below.   | Reference genome | Parameter | Supported reference genome (HG19, HG38, mm9, mm10, dm3, dm6) | HG38 |\n| Sequencing method | Parameter | Sequencing method of experiment (forward, reverse, unstranded) | forward |  Outputs     Name  Description      FastQC Report  Quality control analysis by FastQC.    Aligned BAM  Aligned BAM files from STAR mapping.    Splice junctions  Splice junction information from STAR mapping.    Coverage files  bigWig ( .bw ) and BED ( .bed ) files detailing coverage.    Gene counts  Gene counts generated by HT-Seq count.    VOOM/LIMMA results  Pairwise comparisons of expression data. Requires at least 3 samples vs 3 samples.    Simple DE analysis  No statistical analysis, requires only a 1 samples vs 1 sample comparison.    MA/Volcano plots  Both of the above produce tabular outputs, MA plots and volcano plots.     Process   FastQ files generated by RNA-Seq are mapped to a reference genome using the STAR.  HT-Seq count is used to assign mapped reads to genes.   Differential expression analysis is performed using VOOM normalization of counts and\nLIMMA analysis.   Coverage plots of mapped reads are generated as interactive visualizations.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/warden/#getting-started", 
            "text": "To get started, you need to navigate to the  WARDEN tool page . You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.    Note  If you can't see the \"Start\" button, one of these two scenarios is likely the case:   You see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.  If you cannot see  any  buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.   If neither of these are the case and you still can't click \"Start\", contact us .", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/warden/#uploading-data", 
            "text": "The WARDEN Differential Expression analysis pipeline takes Gzipped FastQ\nfiles generated by an RNA-Seq experiment as input. You can upload your input \nfile(s) using the  data transfer application \nor by uploading them through  the command line .\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.   Tip  If you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!  For more information, check out the  data transfer application  guide.", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/warden/#sample-sheet", 
            "text": "Once your data is uploaded, you'll need to create a sample sheet which\ndescribes the relationship between case and control samples,\nphenotype/condition information, and the comparisons you would like to\nperform. The sample sheet is a tab-delimited text document that can be\ncreated in Microsoft Excel (recommended) or a text editor.   Note  You will need to upload your sample sheet in a similar manner as your\nFastQ files, so you can follow the  same uploading instructions  \nto achieve this.", 
            "title": "Sample sheet"
        }, 
        {
            "location": "/guides/tools/warden/#prepare-using-microsoft-excel", 
            "text": "Tip  Download the  file_download  sample excel spreadsheet  as a starting\npoint!   The final product for the excel spreadsheet will look like the\nscreenshot below. If you create the sample sheet from scratch, please\nensure the the columns are  exactly  in this order.     Sample rows  Each row in the spreadsheet (except for the last row, which we will talk \nabout in the next section) corresponds to a sample with one or more FastQ files. You should fill in these rows based on your data and the guidelines below:   Guidelines   The sample name should be unique and should only contain letters,\nnumbers and underscores.  The condition/phenotype column associates similar samples together.\nThe values should contain only letters, numbers and underscores.  ReadFile1 should contain forward reads (e.g.  *.R1.fastq.gz  or  *_1.fastq.gz ).  ReadFile2 will contain reads in reverse orientation to ReadFile2\n(e.g.  *.R2.fastq.gz  or  *_2.fastq.gz ).  For single end reads a single dash ('-') should be entered in the ReadFile2 column.    Comparison row  The last line in the sample sheet is called the \"comparison row\". This\nline specifies the comparisons to be done between conditions/phenotypes.\nAll pairwise combinations of the values in the \"Phenotype\" column can be \nanalyzed. To specify the comparisons, on a separate line, include  #comparisons=  followed be a comma delimited list of two conditions separated by a dash.    Example  The following lines are all valid examples.   #comparisons=KO-WT  #comparisons=Condition1-Control,Condition2-Control  #comparisons=Phenotype2-Phenotype1,Phenotype3-Phenotype2,Phenotype3-Phenotype1     Note  If a comparison has at least 3 samples for each condition/phenotype,\nVOOM/LIMMA will be run. A simple differential comparison will be run on\nall samples.   Finalizing the sample sheet  To finalize the sample sheet, save the Microsoft Excel file with\nwhatever name you like. Save the file as an Excel Workbook with the\n.xlsx extension.", 
            "title": "Prepare using Microsoft Excel"
        }, 
        {
            "location": "/guides/tools/warden/#prepare-using-a-text-editor", 
            "text": "Tip  Download the  file_download  sample text file  as a starting\npoint!   Creating a sample sheet with a text editor is an option for advanced\nusers. The process of creating a sample sheet with a text editor is the same as\ncreating one with Microsoft Excel, with the small difference that you\nmust manually create your columns using the tab character. Save the file\nwith a .txt extension.", 
            "title": "Prepare using a text editor"
        }, 
        {
            "location": "/guides/tools/warden/#running-the-tool", 
            "text": "Note  The WARDEN tool operation is slightly different than the other pipelines\nbecause it accepts a variable number of samples.  First , you will run\na \"bootstrapping\" step that creates a custom executable for your\nanalysis.  Second , you will need to manually execute the generated\nworkflow from the first step. This allows us to take advantage of many\nnice features, like check-pointing and cost reduction. Don't worry, \nwe'll show you how to do this step by step below.   Once you've uploaded data to your cloud workspace, click \"Launch Tool\" on the  tool's landing page . You will be redirected to the virtual\ncloud workspace with the workflow screen opened for you.", 
            "title": "Running the tool"
        }, 
        {
            "location": "/guides/tools/warden/#hooking-up-inputs", 
            "text": "Next, you'll need to hook up the FastQ files and sample sheet\n you uploaded in  the upload data section . \nClick the  FASTQ_FILES  input field and select  all  FastQ files.\nNext, click the  sampleList  input field and select the corresponding\nsamplesheet.", 
            "title": "Hooking up inputs"
        }, 
        {
            "location": "/guides/tools/warden/#selecting-parameters", 
            "text": "We now need to configure the parameters for the pipeline, such as reference\ngenome and sequencing method. You can access all of the available parameters \nby clicking on the  WARDEN WORKFLOW GENERATOR  substep.   Parameter setup steps   In the  Output Folder  field, select a folder to output to. You can\nstructure your experiments however you like (e.g.  /My_Outputs )  In the  analysisName  field, enter a prefix for all of the output files. This\ncan be any value you want to use to remember this run.  Be sure to use underscores\ninstead of spaces here!  Select the  sequenceStandedness  from the drop down menu. \nThis information can be determined from the sequencing or source \nof the data. If you don't know what to put here, select \"no\".  Select the  Genome  pulldown menu. Choose the appropriate box.  The LIMMA parameters can be left alone for most analyses. If you are\nan advanced LIMMA user, you can change the various settings exposed\nbelow the required parameters.  When all parameters have been set, press the save button.", 
            "title": "Selecting parameters"
        }, 
        {
            "location": "/guides/tools/warden/#starting-the-workflow", 
            "text": "Once your input files are hooked up and your parameters are set, \nyou should be able to start the workflow by clicking the \"Run as Analysis...\" \nbutton in the top right hand corner of the workflow dialog.   Tip  If you cannot click this button, please ensure that all of the inputs are correctly hooked up (see  hooking up inputs ).  If you're still have trouble, please  contact us  and include\na screenshot of the workflow screen above.   The tool will begin running and will automatically take you to the Monitor page, where you should see that your workflow is \"In Progress\".   When the custom workflow has finished generating, the word 'Done' will\nappear in green in the status column. This indicates that the\nbootstrapping step has completed successfully.", 
            "title": "Starting the workflow"
        }, 
        {
            "location": "/guides/tools/warden/#custom-workflow-process", 
            "text": "Wait for the workflow generator to finish.   Click on the WARDEN name in the name column.    You will now be on a page specific to the running of the workflow.\n    On the left side, you will see the inputs you selected for the\n    workflow generator. On the right side are the output files\n    (including the generated workflow). Select the generated workflow as\n    shown in the picture below.     You will now be within the output folder you specified earlier.\n    Select the file that begins with 'WARDEN WORKFLOW:'     A workflow generated for your data will be presented to you. Select\n    'Run as analysis' in the upper right.     The workflow will initiate, and you will be brought to the 'Monitor'\n    page. (Note to get back to this page, you can select 'Monitor' on\n    one of the menu bars near the top ) Expand the the workflow progress\n    be selecting the '+' sign next to 'In Progress'     As parts of the pipeline are run, you will see different tasks in\n    different colors. Green means done, blue is running, orange is\n    waiting, and red means error.     When done the status will be shown as 'Done'. Select the Workflow\n    name under Status.     You will be brought to a page that show more information about the\n    workflow analysis. Click on the output folder to go to the output.     The output folders will now be shown.     For a description of the output, please refer to  Navigating Results .", 
            "title": "Custom Workflow Process"
        }, 
        {
            "location": "/guides/tools/warden/#monitoring-run-progress", 
            "text": "Once you have started one or more WARDEN runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the  tool's landing page . \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kick off\ngets one row in the Monitor section.  You can click the \"+\" on any of the runs to check \nthe status of individual steps of the pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.   Tip  Power users can view the  DNAnexus Job Monitoring Tutorial  and the  DNAnexus Command Line Tutorial for Job Monitoring  for advanced capabilities for monitoring jobs.", 
            "title": "Monitoring run progress"
        }, 
        {
            "location": "/guides/tools/warden/#analysis-of-results", 
            "text": "Each tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.", 
            "title": "Analysis of results"
        }, 
        {
            "location": "/guides/tools/warden/#finding-the-raw-results-files", 
            "text": "Navigate to the  tool's landing page . \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.", 
            "title": "Finding the raw results files"
        }, 
        {
            "location": "/guides/tools/warden/#navigating-results", 
            "text": "Note  Navigating to the raw results of your runs is the same for all\npipelines. This guide will feature the  rapid-rnaseq  pipeline, but you can follow along for\nany tool.", 
            "title": "Navigating Results"
        }, 
        {
            "location": "/guides/tools/warden/#raw-result-files", 
            "text": "Navigate to your tool's description page (for instance, Rapid RNA-Seq's\ndescription page is here ). You should\nsee a screen similar to the one in the screenshot below. In the left\nhand pane, select \"View Results Files\".   You should now be in the tool's workspace with access to files that you\nuploaded and results files that are generated. How/where the result\nfiles are generated are specific to each pipeline. Please refer to your\nindividual pipeline's documentation on where the output files are kept.", 
            "title": "Raw result files"
        }, 
        {
            "location": "/guides/tools/warden/#custom-visualization-results", 
            "text": "Navigate to your tool's description page (for instance, Rapid RNA-Seq's\ndescription page is here ). You should\nsee a screen similar to the one in the screenshot below. In the left\nhand pane, select \"Visualize Results\".   You should now be in the tool's workspace with access to files that you\nuploaded and results files that are generated. How/where the result\nfiles are generated are specific to each pipeline. Please refer to your\nindividual pipeline's documentation on where the output files are kept.", 
            "title": "Custom visualization results"
        }, 
        {
            "location": "/guides/tools/warden/#interpreting-results", 
            "text": "The complete output file specification is listed in the  overview section \nof this guide. Here, we will discuss each of the different output files in more detail.   Predicted gene fusions : The putative gene fusions will be\n  contained in the file  [SAMPLE].final_fusions.txt . This file is a\n  tab-delimited file containing many fields for each of the predicted\n  SV. The most important columns are the following.      Field Name  Description      sample  Sample name    gene*  Gene name    chr*  Chromosome name    pos*  Genomic Location    ort*  Strand    reads*  Supporting reads    medal  Estimated pathogenicity assessment using St. Jude Medal Ceremony      Coverage file : A standard\n     bigWig  file\n    used to describe genomic read coverage.  Splice junction read counts : A custom file format describing the\n    junction read counts. The following fields are included in the\n    tab-delimited output file.      Field Name  Description      junction  Splice junction in the TCGA format \"chrX:start:+,chrX:end,+\". \"start\" and \"end\" are the 1-based position of the last mapped nucleotide before the skip and the first mapped nucleotide after the skip (i.e. the last base of the previous exon and the first base of the next exon). Note that in  .bed  output these coordinates will be different, see the .bed output section below. The \"+\" is currently hardcoded, though this may change in the future.    count  Raw count of reads supporting the junction. During correction counts for ambiguous junctions can be combined, though obviously these additional reads will not be visible in the raw BAM file.    type  Either \"known\" (matching a reference junction) or \"novel\" (not observed in the reference junction collection).    genes  Gene symbols from the junction calling process. These still need work in the raw junction calling process, it's recommended to use the \"annotated\" output files instead which assign matching HUGO gene symbols based on the UCSC refGene table.    transcripts  List of known transcript IDs matching the junction.    qc_flanking  Count of supporting reads passing flanking sequence checks (junctions observed adjacent to read ends require 18+ nt of flanking sequence, otherwise 10+ nt).    qc_plus  Count of supporting reads aligned to the + strand.    qc_minus  Count of supporting reads aligned to the - strand.    qc_perfect_reads  Count of supporting reads with perfect alignments (no reference mismatches of quality 15+, indels, or soft clips).    qc_clean_reads  Count of supporting reads whose alignments are not perfect but which have a ratio of  = 5% of reference mismatches of quality 15+, indels, or soft clips relative to the count of aligned bases on both the left and right flanking sequence. Note: qc_clean_reads does NOT include qc_perfect_reads: to get a count of \"perfect plus pretty good\" reads the two values must be added together.", 
            "title": "Interpreting results"
        }, 
        {
            "location": "/guides/tools/warden/#primary-results", 
            "text": "", 
            "title": "Primary Results"
        }, 
        {
            "location": "/guides/tools/warden/#alignment-statistics", 
            "text": "Several files should be examined initially to determine the quality of\nthe results.  alignmentStatistics.txt  shows alignment statistics for\nall samples. This file is a plain text tab-delimited file that can be\nopened in Excel or a text editor such as Notepad++. This file contains\ninformation on the total reads per sample, the percantage of duplicate\nreads and the percentage of mapped reads. An example of this file is\nbelow. (Within the DNAnexus output directory structure, these files will\nbe in the COMBINED_FLAGSTAT directory.)", 
            "title": "Alignment statistics"
        }, 
        {
            "location": "/guides/tools/warden/#multidimensional-scaling-mds-plot", 
            "text": "The second set of files to look at are the Multidimensional scaling\n(MDS) plots ( https://en.wikipedia.org/wiki/Multidimensional_scaling )\nusing the plotMDS function within LIMMA. Similar to PCA, these graphs\nwill show how similar samples are to each other. There are different\nsets of MDS plots. For comparisons where there are 3 or more samples per\ncondition, an MDS plot using Voom (Limma) normalized values are\ngenerated. An example can be seen below. These files will be labeled mdsPlot.png . For all comparisons, regardless of sample size, and MDS\nplot will also be generated with Counts per million (CPM) normalized\ngene counts. These files will be labeled  mdsPlot.normCPM.png .\n(Within the DNAnexus output directory structure, these files will be in\nthe LIMMA directory.)   MDS plot from just CPM normalized data.", 
            "title": "Multidimensional scaling (MDS) Plot"
        }, 
        {
            "location": "/guides/tools/warden/#proteinpaint-visualizations", 
            "text": "Several files on DNAnexus allow the data to be viewed in the Protein\nPaint viewer. (Note: We plan to have links downloaded in the future to\nallow the viewing of these files off of DNAnexus.)  LIMMA differential expression viewer  Within LIMMA/VIEWERS direcory (note if no comparisons meet the 3 sample\ncondition, the LIMMA folder will not exist), there will be a viewer file\nfor each valid comparison ( * results. .txt.viewer**). Simply select\nthe file and press 'Launch viewer' in the lower right. A viewer will pop\nup showing both the MA Plot and Volcano plot. By moving the mouse over a\ncircle, the circle will hilight and the corresponding gene on the other\ngraph will also hilight. Additional information about the gene and its\nexpression values will also be shown. One can also type in multiple gene\nsymbols in the provided text box. By pressing 'Show gene labels' all\nthese genes will show up on the plots.   Simple differential expression viewer  There will also be a viewer for the simple differential expresssion\nanalysis in SIMPLE_DIFEX/VIEWERS. The P-value for the results have all\nbeen set to 1, so the volcano plot will not be relevant.  bigWig viewer  In the BIGWIG_VIEWER directory there will be a bigwigViewer file.\nSelect this file and then 'Launch viewer'. A graph of coverage for the\ngenome should be visible.", 
            "title": "ProteinPaint Visualizations"
        }, 
        {
            "location": "/guides/tools/warden/#secondary-results", 
            "text": "Interactive MA/Volcano Plots  In addition to viewing the MA and volcano plots through the visualization tool   Differential expression results    Other useful differential expression results will be downloaded by the desktop app. This included tabular output from the differential expression analysis. For each comparison with three or more samples per condition,  results.*.txt  will be produced.   GSEA.input.txt  and  GSEA.tStat.txt    Input files that can be used for GSEA analysis. The tStat file is preferred for a more accurate analysis, but will not give a heatmap diagram.\n- (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.)    For plain text results from the simple differential expresison analysis, the files will be named  simpleDE.*.txt . (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)    Prelabelled MA and volcano plots are provided for the analysis. These files are labeled  maPlot.*.png  and  volcanoPlot.*.png  where  *  is the comparison (e.g. ko_vs_wt)    The MA plot shows the average expression of the gene on the X-axis, and Log2 fold change between condition/phenotype is on the Y-axis (if the name is for example maPlot.condition2-condition1.png then the fold change would represent condition1 minus condition2). Each gene is represented by a circle. The top 20 genes (by p-value) are identified on the plot. The genes are color coded by the chosen multiple testing correction method (False Discovery Rate (FDR) by default. An exmaple MA plot can be seen below. (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.)     The volcano plot shows the Log2Fold change between the conditions on the\nX-axis, and the -Log10 of the multiple testing corrected P-value on the\nY-axis.   An MA plot is generated for all comparisons regardless of number of\nsamples. This is the  simpleDEPlot.*.png  no statistics are shown\nand genes are not labeled. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)   Differential analysis input  Inputs and commands are provided for rerunning differential expression\nanalysis on ones own computer. The R commands used for the analysis are\nfound in  voomLimma.R . An experienced R user can rerun the analysis\nwith any desired changes. This analysis requires the input countFile.txt  which contains counts per genes, the Rparameters.txt  file containing input parameters, and a processed\nsample list file  sampleList.txt  (Within the DNAnexus output directory structure, these files will be in\nthe LIMMA directory.)  The input for the simple differential analysis expression will be Rparameters_simple.txt ,  simpleDE.R ,  countFile.txt  and sampleList.txt .  countFile.txt  and  sampleList.txt  are the\nsame files used by the LIMMA analysis.  (Within the DNAnexus output directory structure, these files will be in\nthe SIMPLE_DIFEX directory.)   Coverage results  bigWig files will be generated for use in genome browsers (such as IGV http://software.broadinstitute.org/software/igv/ ). For each smaple,\nmultiple bigWig files will be found. For all types of sequencing\nstrandedness, there will be bigWig files labelled, *.sortedCoverageFile.bed.bw  where ' ' is the sample name. For\nstranded data there will also be*.sortedPosCoverageFile.bed.bw * and *.sortedNegCoverageFile.bed.bw  which contains coverage information\nfor the positive and negative strand of the genome.  (Within the DNAnexus output directory structure, these files will be in\nthe BIGWIG directory.)   Quality Control Results (FastQC)  Within the FastQC directory, foreach sample and read direction there\nwill be an html file and a zip file ( *.FastQc.html  *.FastQc.zip  where '*' is the base FastQ name), containing results\nfrom FastQTC. For the average user the html file is sufficient. This\nfile can give some basic statistics on the quality of the data.  (Within the DNAnexus output directory structure, these files will be in\nthe FastQC directory.)   BAM alignment files  There are two BAM files generated per sample that contain mapping\ninformation for all reads. The first is labeled *.Aligned.sortedByCoord.dup.bam  where ' ' is the sample name. The\nBAM file is sorted by coordinates and has duplicates marked. The second\nfile is*.Aligned.toTranscriptome.out.bam * and contains reads mapped\nto transcripts.  (Within the DNAnexus output directory structure, these files will be in\nthe ALIGN directory.)   Chimeric reads and junction files  Additional files created by STAR are provided. More information on these\nfiles can be found at http://labshare.cshl.edu/shares/gingeraslab/www-data/dobin/STAR/STAR.posix/doc/STARmanual.pdf . *.SJ.out.tab  contain splice junction information. Fusion detectino\nfiles are labelled  *.Chimeric.out.bam  and *.Chimeric.out.junction .  (Within the DNAnexus output directory structure, these files will be in\nthe ALIGN directory.)   FPKM and count files (per sample)  Per sample files containing FPKM and raw count values for each gene can\nbe found in  *.fpkm.txt  and  *.htseq_counts.txt  where '*' is\nthe sample name.  (Within the DNAnexus output directory structure, these files will be in\nthe  NOTE FIND OUT WHICH DIRECTORY  directory.)", 
            "title": "Secondary Results"
        }, 
        {
            "location": "/guides/tools/warden/#methods-files", 
            "text": "A more human readable explanation is found in  methods.docx . Detailed\ndocumentation can be found in  methods.txt  (Within the DNAnexus output directory structure, these files will be in\nthe METHODS directory.)", 
            "title": "Methods Files"
        }, 
        {
            "location": "/guides/tools/warden/#auxilary-files", 
            "text": "This section describes the files that exist within the DNAnexus output\nfolder. Most of these files will not be of interest to the average user.\nHowever, interactive viewers are describe in  LIMMA differential\nexpression viewer  and  Simple\ndifferential expression viewer .  The output will be divided into multiple folders. The results being the\nmost useful will be the differential expression analysis results in the\nLIMMA and SIMPLE_DIFEX folders. Bigwig files for viewing read coverage\nwill be in the BIGWIG folder. Other folder contain different types of\ndata and are explained in further detail below.     The following description of files is sorted by their output directory.   ALIGN  This directory contains the BAM files described in  BAM alignment\nfiles  and the chimeric and junction files are\ndescribed in  Chimeric reads and junction\nfiles . In adition there are 2 log\nfiles.  *Log.final.out  has relevant statistics for the alignemnt.\nThe  *.Log.out  file just contains a log of the analysis run,\nincluding input parameters. Per sample FLAGSTAT results are found in *.flagStatOut.txt . These flagstat files are combined into the file alignmentStatistics.txt  described in  Initial analysis of\nresults . Finally the ALIGN directory has\nmultiple  .starAlign.methods.txt files. These files can be ignored as\nthey are summarized in the finalmethods.docx * and  methods.txt \nfiles described in  Methods Files .   BIGWIG  All of the files here are described in section  Coverage\nresults . The  bgToBw.methods.txt  files can be\nignored as they are summarized in the files described in  Methods\nFiles .   BIGWIG_VIEWER  See  bigWig viewer   COMBINED_FLAGSTAT   Todo  Describe combined flagstat.    COMBINED_HTSEQ  Used for input in differential expression analysis. The\ncombineCountFile.txt is the same as countFile.txt described in Differential analysis input   COVERAGE  BED graph files used to generate bigWig files are here.   FastQC  See  Quality Control Results (FastQC)   HTSEQ  Per-sample HTSEQ-count results ( *.htseq_counts.txt ) and FPKM\nresults ( *.fpkm.txt ). Temporary methods files are found as\n*.htseq-count.methods.txt   LIMMA  mdsPlot.png ,  maPlot. .png,volcanoPlot. .png  are described in Initial analysis of results  results. .txt,GSEA.input. .txt  and * GSEA.tStat. .txt** are\ndescribe in  Differential expression\nresults  voomLimma.R ,  countFile.txt ,  Rparameters.txt , and sampleList.txt  are described in  Differential analysis\ninput  See  LIMMA differential expression\nviewer  for a description of the\nVIEWERS directory.  Other files in the LIMMA directory include contrastFiles.txt\ncontrastsFile.txt, and limmaSampleList.txt which are used internally.\nlimmaMethods.txt is an intermediate file describing methods. Out.tar.gz\nis used for testing purposes. The sessionInfo.txt file describe the R\nsession working parameters and modules loaded. meanVariance.png is a\nplot for assessing quality of count data\n( https://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-2-r29 )   METHODS  The files here are described in  Methods Files .   SAMPLELIST  These files are used internally by the pipeline.   SIMPLE_DIFEX  mdsPlot.normCPM.png  and * simpleDEPlot. .png** are described in Initial analysis of results  * simpleDE. .txt** are describe in  Differential expression\nresults  simpleDE.R ,  countFile.txt ,  Rparameters_simple.txt , and sampleList.txt  are described in  Differential analysis\ninput  See  Simple differential expression\nviewer  for a description of the\nVIEWERS directory.  Other files in the SIMPLE_DIFEX directory include contrastFiles.txt\ncontrastsFile.txt, and limmaSampleList.txt which are used internally.\nsimpleDifEx.methods.txt is an intermediate file describing methods.\nOut.tar.gz is used for testing purposes. The sessionInfo.txt file\ndescribe the R session working parameters and modules loaded.", 
            "title": "Auxilary Files"
        }, 
        {
            "location": "/guides/tools/warden/#known-issues", 
            "text": "Adapter contamination  This pipeline does not, at present, remove adapter sequences. If the\nsequencing library is contaminated with adapters, CICERO runtimes can\nincrease exponentially. We recommend running FastQ files through a QC\npipeline such as FastQC and trimming adapters with tools such as\nTrimmomatic if adapters are found.    High coverage regions  Certain cell types show very high transcription of certain loci, for\nexample, the immunoglobulin heavy chain locus in plasma cells. The\npresence of very highly covered regions (typically 100,000-1,000,000+ X)\nhas an adverse effect on CICERO runtimes. Presently, we have no good\nsolution to this problem as strategies such as down-sampling may reduce\nsensitivity over important regions of the genome.    Interactive Visualizations Exon vs Intron Nomenclature  When a codon is split over a fusion gene junction, the annotation\nsoftware marks the event as intronic when really, the event should be\nexonic. We are working to fix this bug. In the mean time, if a fusion is\npredicted to be in frame but the interactive plot shows \"intronic\", we\nsuggest the user blat the contig shown just below to clarify if the true\njunction is either in the intron or exon.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/guides/tools/warden/#frequently-asked-questions", 
            "text": "None yet! If you have any questions not covered here, feel free to reach\nout on  our contact\nform .", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/guides/tools/cis-x/", 
            "text": "Warning\n\n\ncis-X is an upcoming St. Jude Cloud tool and is not yet publicly available.\nSee \ncis-X on St. Jude Research\n for more information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthors\n\n\nYu Liu, Chunliang Li, Shuhong Shen\n\n\n\n\n\n\nPublication\n\n\nN/A (not published)\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nActivating regular variants usually cause the cis-activation of target genes.\nTo find cis-activated genes, allelic specific/imbalance expressions (ASE) and\noutlier high expression (OHE) signals are used. Variants in the same\ntopologically associated domains with the candidates can then be searched,\nincluding structural variants (SV), copy number aberrations (CNA), and single\nnucleotide variations (SNV) and insertion/deletions (indel).\n\n\nA transcription factor binding analysis is also done, using motifs from\n\nHOCOMOCO\n v10 models.\n\n\ncis-X currently only works with hg19 (GRCh37).\n\n\nOverview\n\n\nInputs\n\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nSample ID\n\n\nString\n\n\nThe ID of the input sample\n\n\nSJALL018373_D1\n\n\n\n\n\n\nDisease subtype\n\n\nString\n\n\nThe disease name under analysis. Must be either TALL or AML.\n\n\nTALL\n\n\n\n\n\n\nSingle nucleotide variants\n\n\nFile\n\n\nTab-delimited file containing raw sequence variants\n\n\n*.txt\n\n\n\n\n\n\nCNV/LOH regions\n\n\nFile\n\n\nTab-delimited file containing any aneuploidy region existing in the tumor genome under analysis\n\n\n*.txt\n\n\n\n\n\n\nRNA-seq BAM\n\n\nFile\n\n\nBAM file aligned to hg19 (GRCh37)\n\n\n*.bam\n\n\n\n\n\n\nRNA-seq BAM index\n\n\nFile\n\n\nBAM index for the given BAM\n\n\n*.bam.bai\n\n\n\n\n\n\nGene expression table\n\n\nFile\n\n\nTab-delimited file containing gene level expressions for the tumor under analysis in FPKM\n\n\n*.txt\n\n\n\n\n\n\nSomatic SNV/indels\n\n\nFile\n\n\nTab-delimited file containing somatic SNV/indels in the tumor genome\n\n\n*.txt\n\n\n\n\n\n\nSomatic SVs\n\n\nFile\n\n\nTab-delimited file containing somatic acquired structural variants in the tumor genome\n\n\n*.txt\n\n\n\n\n\n\nSomatic CNVs\n\n\nFile\n\n\nTab-delimited file containing copy number aberrations in the tumor genome\n\n\n*.txt\n\n\n\n\n\n\nCNV/LOH action\n\n\nString\n\n\nThe behavior when handling markers in CNV/LOH regions. Can be either \nkeep\n or \ndrop\n.\n\n\ndrop\n\n\n\n\n\n\nMinimum coverage for WGS\n\n\nInteger\n\n\nThe minimum coverage in WGS to be included in the analysis\n\n\n10\n\n\n\n\n\n\nMinimum coverage for RNA-seq\n\n\nInteger\n\n\nThe minimum coverage in RNA-seq to be included in the analysis\n\n\n5\n\n\n\n\n\n\nCandidate FPKM threshold\n\n\nFloat\n\n\nThe FPKM threshold for the nomination of a cis-activated candidate\n\n\n0.1\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncis-activated candidates\n\n\ncis-activated candidates in the tumor genome under analysis\n\n\n\n\n\n\nSV candidates\n\n\nStructural variant (SV) candidates predicted as the causal for the cis-activated genes in the regulatory territory\n\n\n\n\n\n\nCNA candidates\n\n\nCopy number aberrations (CNA) predicted as the causal for the cis-activated genes in the regulatory territory\n\n\n\n\n\n\nSNV/indel candidates\n\n\nSNV/indel candidates predicted as functional and predicted transcription factors\n\n\n\n\n\n\nOHE results\n\n\nRaw outlier high expression (OHE) results\n\n\n\n\n\n\nGene level ASE results\n\n\nRaw gene level allelic specific expression (ASE) results\n\n\n\n\n\n\nSingle marker ASE results\n\n\nRaw single marker allelic specific expression (ASE) results\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\nTodo\n\n\n\n\nInput file configuration\n\n\ncis-X requires six tab-delimited input files to be prepared in advance.\n\n\n\n\nNote\n\n\nEven though CNV/LOH regions, somatic SNV/indels, somatic SVs, and\nsomatic CNVs can be \"empty\", using such inputs will produce results with a\nmuch higher false positive rate.\n\n\n\n\nSingle nucleotide variants\n\n\n\nA list of single nucleotide markers is a tab-delimited file with the\nfollowing columns:\n\n\n\n\nChr\n: chromosome name for the marker\n\n\nPos\n: genomic start location for the marker\n\n\nChr_Allele\n: reference allele\n\n\nAlternative_Allele\n: alternative allele\n\n\nreference_tumor_count\n: reference allele count in the tumor genome\n\n\nalternative_tumor_count\n: alterative allele count in the tumor genome\n\n\nreference_normal_count\n: reference allele count in the matched normal genome\n\n\nalternative_normal_count\n: alternative count in the matched normal genome\n\n\n\n\nThis file can be generated with Bambino.\n\n\nExample\n\n\n\n\n\n\n\n\n\nChr\n\n\nPos\n\n\nChr_Allele\n\n\nAlternative_Allele\n\n\nreference_tumor_count\n\n\nalternative_tumor_count\n\n\nreference_normal_count\n\n\nalternative_normal_count\n\n\n\n\n\n\n\n\n\n\nchr11\n\n\n61396\n\n\nTT\n\n\n\n\n0\n\n\n3\n\n\n0\n\n\n10\n\n\n\n\n\n\nchr11\n\n\n72981\n\n\n\n\nT\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n\n\n\n\n\n\nCNV/LOH regions\n\n\n\nThe CNV/LOH regions are all the genomic regions carrying copy number\nvariations (CNV) or loss of heterozygosity (LOH), which will be filtered out\nduring analysis.\n\n\nThis is a tab-delimited file in the bed format. It must have at least the\nfollowing three columns:\n\n\n\n\nchrom\n: chromosome name\n\n\nloc.start\n: genomic start location\n\n\nloc.end\n: genomic end location\n\n\n\n\nIf no CNV/LOH are in the genome under analysis, a file with no rows (but\nincluding headers) can be provided.\n\n\nThis file can be generated with CONSERTING.\n\n\nExample\n\n\n\n\n\n\n\n\n\nchrom\n\n\nloc.start\n\n\nloc.end\n\n\nSample\n\n\nseg.mean\n\n\nLogRatio\n\n\nsource\n\n\n\n\n\n\n\n\n\n\nchr9\n\n\n10712\n\n\n37855747\n\n\nSJALL018373_D1\n\n\n0.471181417\n\n\n\n\nLOH\n\n\n\n\n\n\nchr9\n\n\n20276901\n\n\n20703900\n\n\nSJALL018373_D1\n\n\n-0.978\n\n\n-5.696\n\n\nCNV\n\n\n\n\n\n\n\n\nGene expression table\n\n\n\nThe gene expression table is a tab-delimited file containing gene level\nexpressions for the tumor under analysis. The expressions are in FPKM\n(fragments per kilobase of transcript per million mapped reads).\n\n\n\n\nGeneID\n: gene \nEnsembl\n ID\n\n\nGeneName\n: gene symbol\n\n\nType\n: \ntranscript type\n\n\nStatus\n: transcript status (must be \nKNOWN\n, \nNOVEL\n, or \nPUTATIVE\n)\n\n\nChr\n: chromosome name\n\n\nStart\n genomic start location\n\n\nEnd\n: genomic end location\n\n\n[SampleID...]: FPKM for the given sample\n\n\n\n\nThis file can be generated with the output of HTseq-count preprocessed\nthrough \nmergeData_geneName.pl\n (available with the distribution of cis-X).\nThe data must be able to match values in the given gene specific reference\nexpression matrices generated from a larger cohort.\n\n\nExample\n\n\n\n\n\n\n\n\n\nGeneID\n\n\nGeneName\n\n\nType\n\n\nStatus\n\n\nChr\n\n\nStart\n\n\nEnd\n\n\nSJALL018373_D1\n\n\n\n\n\n\n\n\n\n\nENSG00000261122.2\n\n\n5S_rRNA\n\n\nlincRNA\n\n\nNOVEL\n\n\nchr16\n\n\n34977639\n\n\n34990886\n\n\n0.0000\n\n\n\n\n\n\nENSG00000249352.3\n\n\n7SK\n\n\nlincRNA\n\n\nNOVEL\n\n\nchr5\n\n\n68266266\n\n\n68325992\n\n\n4.5937\n\n\n\n\n\n\n\n\nSomatic SNV/indels\n\n\n\nThis is a tab-delimited file containing somatic sequence mutations present in\nthe genome under analysis. It includes both single nucleotide variants (SNV)\nand small insertion/deletions (indel). The file must have the following\ncolumns:\n\n\n\n\nchr\n: chromosome name\n\n\npos\n: genomic start location\n\n\nref\n: reference nucleotide\n\n\nmutant\n: mutant nucleotide\n\n\ntype\n: mutation type (must be either \nsnv\n or \nindel\n)\n\n\n\n\nNote that the coordinate used for an indel is after the inserted sequence.\n\n\nIf no SNV/indels are in the sample under analysis, a file with no rows\n(but including headers) can be provided.\n\n\nThis file can can be created with Bambino and then preprocessed using the\nsteps taken in \"\nThe genetic basis of early T-cell precursor acute lymphoblastic leukaemia\n\".\n\n\nExample\n\n\n\n\n\n\n\n\n\nchr\n\n\npos\n\n\nref\n\n\nmut\n\n\ntype\n\n\n\n\n\n\n\n\n\n\nchr1\n\n\n24782720\n\n\nG\n\n\nA\n\n\nsnv\n\n\n\n\n\n\nchr11\n\n\n82896176\n\n\nT\n\n\nC\n\n\nsnv\n\n\n\n\n\n\n\n\nSomatic SVs\n\n\n\nThis is a tab-delimited file containing somatic-acquired structural variants\n(SV) in the cancer genome. The file must have the following columns:\n\n\n\n\nchrA\n: chromosome name of the left breakpoint\n\n\nposA\n: genomic location of the left breakpoint\n\n\nortA\n: strand orientation of the left breakpoint\n\n\nchrB\n: chromosome name of the right breakpoint\n\n\nposB\n: genomic location of the right breakpoint\n\n\nortB\n: strand orientation of the right breakpoint\n\n\n\n\nStrand orientations are denoted with a \n+\n for a sense or coding strand and\n\n-\n for a antisense or non-coding strand.\n\n\nIf no somatic SVs are in the sample under analysis, a file with no rows (but\nincluding headers) can be provided.\n\n\nThis file can be generated by CREST.\n\n\nExample\n\n\n\n\n\n\n\n\n\nchrA\n\n\nposA\n\n\nortA\n\n\nchrB\n\n\nposB\n\n\nortB\n\n\ntype\n\n\n\n\n\n\n\n\n\n\nchr11\n\n\n33913169\n\n\n+\n\n\nchr7\n\n\n142494049\n\n\n-\n\n\nCTX\n\n\n\n\n\n\nchr11\n\n\n64219334\n\n\n+\n\n\nchr2\n\n\n205042527\n\n\n-\n\n\nCTX\n\n\n\n\n\n\n\n\nSomatic CNVs\n\n\n\nThis is a tab-delimited file containing the genomic\nregions with somatic-acquired copy number aberrations (CNA) in the cancer\ngenome.\n\n\n\n\nchr\n: chromosome name\n\n\nstart\n: genomic start location\n\n\nend\n: genomic end location\n\n\nlogR\n: log2 ratio\n\n\n\n\nIf no somatic CNVs are in the sample under analysis, a file with no rows\n(but including headers) can be provided.\n\n\nThis file can be generating by CONSERTING.\n\n\nExample\n\n\n\n\n\n\n\n\n\nchr\n\n\nstart\n\n\nend\n\n\nlogR\n\n\n\n\n\n\n\n\n\n\nchr9\n\n\n20276901\n\n\n20703900\n\n\n-5.696\n\n\n\n\n\n\n\n\nUploading data\n\n\ncis-X requires a total of eight files to be uploaded, as described in \"\nInput\nfile configuration\n\". These files can be uploaded via the \ndata transfer\napplication\n or \ncommand line\n.\n\n\nRunning the tool\n\n\n\n\nTodo\n\n\n\n\nMonitoring run progress\n\n\n\n\nTodo\n\n\n\n\nAnalysis of results\n\n\nUpon a successful run of cis-X, seven tab-delimited files are saved to the\nresults directory. These raw results can be processed through external tools\nfor further analysis.\n\n\nInterpreting results\n\n\ncis-activated candidates\n\n\n\nThe main result file contains the cis-activated candidates in the tumor genome\nunder analysis.\n\n\n\n\ngene\n: gene accession number (\nRefSeq\n ID)\n\n\ngsym\n: gene symbol\n\n\nchrom\n: chromosome name\n\n\nstrand\n: strand orientation\n\n\nstart\n: genomic start location\n\n\nend\n: genomic end location\n\n\ncdsStartStat\n: coding sequence (CDS) start status\n\n\ncdsEndStat\n: coding sequence (CDS) end status\n\n\nmarkers\n: number of heterozygous markers in this gene\n\n\nase_markers\n: number of heterozygous markers showing allelic specific expressions (ASE)\n\n\naverage_ai_all\n: average B-allele frequency (BAF) difference between RNA and DNA for all heterozygous markers\n\n\naverage_ai_ase\n: average BAF difference between RNA and DNA for ASE markers\n\n\npval_all_markers\n: p-value for each marker in the ASE test\n\n\npval_ase_markers\n: p-value for ASE markers in the ASE test\n\n\nai_all_markers\n: BAF difference between RNA and DNA for all heterozygrous markers\n\n\nai_ase_markers\n: BAF difference between RNA and DNA for ASE markers\n\n\ncomb.pval\n: combined p-value for the ASE test\n\n\nmean.delta\n: average BAF difference between RNA and DNA for all markers\n\n\nrawp\n: raw p-value for the ASE test\n\n\nBonferroni\n: adjusted p-value for the ASE test (single-step Bonferroni)\n\n\nABH\n: adjusted p-value for the ASE test (Benjamini-Hochberg)\n\n\nFPKM\n: FPKM value\n\n\nloo.source\n: which reference expression matrix was used in the outlier high expression (OHE) test\n\n\nloo.cohort.size\n: number of cases in the reference expression matrix for this gene\n\n\nloo.pval\n: p-value of the OHE test\n\n\nloo.rank\n: rank of the case under analysis among the reference cases\n\n\nimprinting.status\n: imprinting status of the gene\n\n\ncandidate.group\n: status of the gene, combining both ASE and outlier tests\n\n\n\n\nStrand orientations are denoted with a \n+\n for a sense or coding strand and\n\n-\n for a antisense or non-coding strand.\n\n\nCoding sequence status is typically one of \"none\" (not specified), \"unk\"\n(unknown), \"incmpl\" (incomplete), or \"cmpl\" (complete).\n\n\nExample\n\n\n\n\n\n\n\n\n\ngene\n\n\ngsym\n\n\nchrom\n\n\nstrand\n\n\nstart\n\n\nend\n\n\ncdsStartStat\n\n\ncdsEndStat\n\n\nmarkers\n\n\nase_markers\n\n\naverage_ai_all\n\n\naverage_ai_ase\n\n\npval_all_markers\n\n\npval_ase_markers\n\n\nai_all_markers\n\n\nai_ase_markers\n\n\ncomb.pval\n\n\nmean.delta\n\n\nrawp\n\n\nBonferroni\n\n\nABH\n\n\nFPKM\n\n\nloo.source\n\n\nloo.cohort.size\n\n\nloo.pval\n\n\nloo.rank\n\n\nimprinting.status\n\n\ncandidate.group\n\n\n\n\n\n\n\n\n\n\nNM_145804\n\n\nABTB2\n\n\nchr11\n\n\n-\n\n\n34172533\n\n\n34379555\n\n\ncmpl\n\n\ncmpl\n\n\n5\n\n\n5\n\n\n0.5\n\n\n0.500\n\n\n0.001953125,0.001953125,0.001953125,6.10351562500001e-05,0.000244140625\n\n\n0.001953125,0.001953125,0.001953125,6.10351562500001e-05,0.000244140625\n\n\n0.5,0.5,0.5,0.5,0.5\n\n\n0.5,0.5,0.5,0.5,0.5\n\n\n0.000644290972057077\n\n\n0.5\n\n\n0.000644290972057077\n\n\n0.632049443587993\n\n\n0.0110866672927557\n\n\n7.6776\n\n\nbi_cohort\n\n\n40\n\n\n0.0367241086505276\n\n\n1\n\n\n\n\nase_outlier\n\n\n\n\n\n\nNM_003189\n\n\nTAL1\n\n\nchr1\n\n\n-\n\n\n47681961\n\n\n47698007\n\n\ncmpl\n\n\ncmpl\n\n\n2\n\n\n2\n\n\n0.482\n\n\n0.482\n\n\n6.66361745922277e-28,3.30872245021211e-24\n\n\n6.66361745922277e-28,3.30872245021211e-24\n\n\n0.464912280701754,0.5\n\n\n0.464912280701754,0.5\n\n\n4.69553625126628e-26\n\n\n0.482456140350877\n\n\n4.69553625126628e-26\n\n\n4.60632106249222e-23\n\n\n6.11761294450693e-24\n\n\n8.8168\n\n\nwhite_list\n\n\n167\n\n\n0.0139385771987089\n\n\n1\n\n\n\n\nase_outlier\n\n\n\n\n\n\n\n\nSV candidates\n\n\n\nStructural variant (SV) candidates include candidates predicted as the causal\nfor the cis-activated genes in the regulatory territory.\n\n\n\n\nleft.candidate.inTAD\n: cis-activated candidate near the left breakpoint\n\n\nright.candidate.inTAD\n: cis-activated candidate near the right breakpoint\n\n\nchrA\n: chromosome name of the left breakpoint\n\n\nposA\n: genomic location of the left breakpoint\n\n\nortA\n: strand orientation of the left breakpoint\n\n\nchrB\n: chromosome name of the right breakpoint\n\n\nposB\n: genomic location of the right breakpoint\n\n\nortB\n: strand orientation of the right breakpoint\n\n\ntype\n: type of translocation\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nleft.candidate.inTAD\n\n\nright.candidate.inTAD\n\n\nchrA\n\n\nposA\n\n\nortA\n\n\nchrB\n\n\nposB\n\n\nortB\n\n\ntype\n\n\n\n\n\n\n\n\n\n\nLMO2\n\n\n\n\nchr11\n\n\n33913169\n\n\n+\n\n\nchr7\n\n\n142494049\n\n\n-\n\n\nCTX\n\n\n\n\n\n\n\n\nCNA candidates\n\n\n\nCopy number aberration (CNA) candidates include candidates predicted as the\ncausal for the cis-activated genes in the regulatory territory.\n\n\n\n\ncandidate.inTAD\n: cis-activated candidate by the CNA\n\n\nchr\n: chromosome name\n\n\nstart\n: genomic start position\n\n\nend\n: genomic end location\n\n\nlogR\n: log ratio of the CNA\n\n\n\n\nSNV/indel candidates\n\n\n\nSNV/indel candidates include predicted candidates as functional and predicted\ntranscription factors. The mutations are also annotated for known regulatory\nelements reported by the \nNIH Roadmap Epigenomics Project\n by collecting 111\ncell lines.\n\n\n\n\nchrom\n: chromosome name\n\n\npos\n: genomic start position\n\n\nref\n: reference allele genotype\n\n\nmut\n: mutant allele genotype\n\n\ntype\n: mutation type (either \nsnv\n or \nindel\n)\n\n\ntarget\n: cis-activated candidate\n\n\ndist\n: distance between the mutation and transcription start sites of the target gene\n\n\ntf\n: transcription factors predicted to have the binding motif introduced by the mutation\n\n\nEpiRoadmap_enhancer\n: enhancer regions that overlap with the mutation (from the \nNIH Roadmap Epigenomics Project\n)\n\n\nEpiRoadmap_promoter\n: promoter regions that overlap with the mutation (from the \nNIH Roadmap Epigenomics Project\n)\n\n\nEpiRoadmap_dyadic\n: dyadic regions that overlap with the mutation (from the \nNIH Roadmap Epigenomics Project\n)\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nchrom\n\n\npos\n\n\nref\n\n\nmut\n\n\ntype\n\n\ntarget\n\n\ndist\n\n\ntf\n\n\nEpiRoadmap_enhancer\n\n\nEpiRoadmap_promoter\n\n\nEpiRoadmap_dyadic\n\n\n\n\n\n\n\n\n\n\nchr1\n\n\n47696311\n\n\nC\n\n\nT\n\n\nsnv\n\n\nTAL1\n\n\n1696\n\n\nBCL11A,CEBPG,PBX2,YY1,ZBTB4\n\n\n\n\nBrain,Digestive,ES-deriv,ESC,HSC \n B-cell,Heart,Muscle,Other,Sm. Muscle,iPSC\n\n\n\n\n\n\n\n\n\n\nOHE results\n\n\n\nOHE results are the raw results for the outlier expression test.\n\n\n\n\nGene\n: gene symbol\n\n\nfpkm.raw\n: FPKM value\n\n\nsize.bi\n: number of cases in the bi-allelic reference cohort\n\n\np.bi\n: p-value in the outlier test using the bi-allelic reference cohort\n\n\nrank.bi\n: rank of the expression level in the case under analysis compared to the bi-allelic reference cohort\n\n\nsize.cohort\n: number of cases in the entire reference cohort\n\n\np.cohort\n: p-value in the outlier test using the entire reference cohort\n\n\nrank.cohort\n: rank of the expression level in the case under analysis compared to the entire reference cohort\n\n\nsize.white\n: number of cases in the whitelist reference cohort\n\n\np.white\n: p-value in the outlier test using the whitelist reference cohort\n\n\nrank.white\n: rank of the expression level in the case under analysis compared to the whitelist reference cohort\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nGene\n\n\nfpkm.raw\n\n\nsize.bi\n\n\np.bi\n\n\nrank.bi\n\n\nsize.cohort\n\n\np.cohort\n\n\nrank.cohort\n\n\nsize.white\n\n\np.white\n\n\nrank.white\n\n\n\n\n\n\n\n\n\n\n7SK\n\n\n4.5937\n\n\nna\n\n\nna\n\n\nna\n\n\n264\n\n\n0.716284011918374\n\n\n162\n\n\nna\n\n\nna\n\n\nna\n\n\n\n\n\n\nA1BG\n\n\n0.2312\n\n\n24\n\n\n0.900132642257996\n\n\n21\n\n\n264\n\n\n0.84055666600945\n\n\n222\n\n\nna\n\n\nna\n\n\nna\n\n\n\n\n\n\n\n\nGene level ASE results\n\n\n\nGene level ASE results are the raw results from the gene level ASE test.\n\n\n\n\ngene\n: gene accession number (\nRefSeq\n ID)\n\n\ngsym\n: gene symbol\n\n\nchrom\n: chromosome name\n\n\nstrand\n: strand orientation\n\n\nstart\n: genomic start location\n\n\nend\n: genomic end location\n\n\ncdsStartStat\n: coding sequence (CDS) start status\n\n\ncdsEndStat\n: coding sequence (CDS) end status\n\n\nmarkers\n: number of heterozygous markers in this gene\n\n\nase_markers\n: number of heterozygous markers showing allelic specific expressions (ASE)\n\n\naverage_ai_all\n: average B-allele frequency (BAF) difference between RNA and DNA for all heterozygous markers\n\n\naverage_ai_ase\n: average BAF difference between RNA and DNA for ASE markers\n\n\npval_all_markers\n: p-value for each marker in the ASE test\n\n\npval_ase_markers\n: p-value for ASE markers in the ASE test\n\n\nai_all_markers\n: BAF difference between RNA and DNA for all heterozygrous markers\n\n\nai_ase_markers\n: BAF difference between RNA and DNA for ASE markers\n\n\ncomb.pval\n: combined p-value for the ASE test\n\n\nmean.delta\n: average BAF difference between RNA and DNA for all markers\n\n\nrawp\n: raw p-value for the ASE test\n\n\nBonferroni\n: adjusted p-value for the ASE test (single-step Bonferroni)\n\n\nABH\n: adjusted p-value for the ASE test (Benjamini-Hochberg)\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\ngene\n\n\ngsym\n\n\nchrom\n\n\nstrand\n\n\nstart\n\n\nend\n\n\ncdsStartStat\n\n\ncdsEndStat\n\n\nmarkers\n\n\nase_markers\n\n\naverage_ai_all\n\n\naverage_ai_ase\n\n\npval_all_markers\n\n\npval_ase_markers\n\n\nai_all_markers\n\n\nai_ase_markers\n\n\ncomb.pval\n\n\nmean.delta\n\n\nrawp\n\n\nBonferroni\n\n\nABH\n\n\n\n\n\n\n\n\n\n\nNM_024684\n\n\nAAMDC\n\n\nchr11\n\n\n+\n\n\n77532207\n\n\n77583398\n\n\ncmpl\n\n\ncmpl\n\n\n2\n\n\n0\n\n\n0.079\n\n\nna\n\n\n0.924775093657227,0.0331439677875056\n\n\nna\n\n\n0.00892857142857145,0.149122807017544\n\n\nna\n\n\n0.175073458624837\n\n\n0.0790256892230577\n\n\n0.175073458624837\n\n\n1\n\n\n0.480780882445856\n\n\n\n\n\n\nNM_015423\n\n\nAASDHPPT\n\n\nchr11\n\n\n+\n\n\n105948291\n\n\n105969419\n\n\ncmpl\n\n\ncmpl\n\n\n2\n\n\n0\n\n\n0.023\n\n\nna\n\n\n0.749258624760841,1\n\n\nna\n\n\n0.0384615384615384,0.00769230769230766\n\n\nna\n\n\n0.86559726476049\n\n\n0.023076923076923\n\n\n0.86559726476049\n\n\n1\n\n\n0.873257417545981\n\n\n\n\n\n\n\n\nSingle marker ASE results\n\n\n\nSingle marker ASE results are the raw results from the single marker ASE test.\n\n\n\n\nchrom\n: chromosome name\n\n\npos\n: genomic start position\n\n\nref\n: reference allele genotype\n\n\nmut\n: non-reference allele genotype\n\n\ncvg_wgs\n: coverage of the marker from the whole genome sequence (WGS)\n\n\nmut_freq_wgs\n: non-reference allele fraction in the WGS\n\n\ncvg_rna\n: coverage of the marker from the RNA-seq\n\n\nmut_freq_rna\n: non-reference allele fraction in the RNA-seq\n\n\nref.1\n: read count of the reference allele in the RNA-seq\n\n\nvar\n: read count of the non-reference allele in the RNA-seq\n\n\npvalue\n: p-value from the binomial test\n\n\ndelta.abs\n: absolute difference of the non-reference allele fraction between the WGS and RNA-seq\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nchrom\n\n\npos\n\n\nref\n\n\nmut\n\n\ncvg_wgs\n\n\nmut_freq_wgs\n\n\ncvg_rna\n\n\nmut_freq_rna\n\n\nref.1\n\n\nvar\n\n\npvalue\n\n\ndelta.abs\n\n\n\n\n\n\n\n\n\n\nchr11\n\n\n204147\n\n\nG\n\n\nA\n\n\n36\n\n\n0.472\n\n\n85\n\n\n0.553\n\n\n38\n\n\n47\n\n\n0.385669420119278\n\n\n0.0529411764705883\n\n\n\n\n\n\nchr11\n\n\n205198\n\n\nC\n\n\nA\n\n\n23\n\n\n0.522\n\n\n83\n\n\n0.313\n\n\n57\n\n\n26\n\n\n0.000877551780002863\n\n\n0.186746987951807", 
            "title": "cis-X"
        }, 
        {
            "location": "/guides/tools/cis-x/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/cis-x/#getting-started", 
            "text": "Todo", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/cis-x/#input-file-configuration", 
            "text": "cis-X requires six tab-delimited input files to be prepared in advance.   Note  Even though CNV/LOH regions, somatic SNV/indels, somatic SVs, and\nsomatic CNVs can be \"empty\", using such inputs will produce results with a\nmuch higher false positive rate.", 
            "title": "Input file configuration"
        }, 
        {
            "location": "/guides/tools/cis-x/#uploading-data", 
            "text": "cis-X requires a total of eight files to be uploaded, as described in \" Input\nfile configuration \". These files can be uploaded via the  data transfer\napplication  or  command line .", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/cis-x/#running-the-tool", 
            "text": "Todo", 
            "title": "Running the tool"
        }, 
        {
            "location": "/guides/tools/cis-x/#monitoring-run-progress", 
            "text": "Todo", 
            "title": "Monitoring run progress"
        }, 
        {
            "location": "/guides/tools/cis-x/#analysis-of-results", 
            "text": "Upon a successful run of cis-X, seven tab-delimited files are saved to the\nresults directory. These raw results can be processed through external tools\nfor further analysis.", 
            "title": "Analysis of results"
        }, 
        {
            "location": "/guides/tools/cis-x/#interpreting-results", 
            "text": "", 
            "title": "Interpreting results"
        }, 
        {
            "location": "/guides/tools/mutational-spectrum/", 
            "text": "Warning\n\n\nMutational Spectrum is an upcoming St. Jude Cloud tool and is not yet\npublicly available. See \nMutational Spectrum on GitHub\n for more\ninformation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthors\n\n\nScott Newman, Michael Macias\n\n\n\n\n\n\nPublication\n\n\nN/A\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nMutational Spectrum\n finds and quantifies COSMIC mutational signatures\nacross samples. This is done by finding the optimal non-negative linear\ncombination of mutation signatures to reconstruct a mutation matrix. It\nbuilds the initial mutation matrix from multiple single-sample VCFs and, by\ndefault, fits it to \nmutational signatures from COSMIC\n.\n\n\nMutational Spectrum supports both hg19 (GRCh37) and hg38 (GRCh38).\n\n\nOverview\n\n\nInputs\n\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVCF(s)\n\n\nArray of files\n\n\nList of VCF inputs. Can be single-sample or multi-sample and uncompressed or gzipped.\n\n\n[\n*.vcf\n, \n*.vcf.gz\n]\n\n\n\n\n\n\nSample sheet\n\n\nFile\n\n\nTab-delimited file (no headers) with sample ID and tag pairs [optional]\n\n\n*.txt\n\n\n\n\n\n\nGenome build\n\n\nString\n\n\nGenome build used as reference. Can be either \"GRCh37\" or \"GRCh38\". [default: \"GRCh38\"]\n\n\nGRCh38\n\n\n\n\n\n\nMinimum mutation burden\n\n\nInteger\n\n\nMinimum number of somatic SNVs a sample must have to be considered for analysis [default: 9]\n\n\n15\n\n\n\n\n\n\nMinimum signature contribution\n\n\nInteger\n\n\nMinimum number of mutations attributable to a single signature [default: 9]\n\n\n100\n\n\n\n\n\n\nOutput prefix\n\n\nString\n\n\nPrefix to append to output filenames [optional]\n\n\nmutspec\n\n\n\n\n\n\nDisabled VCF column\n\n\nInteger\n\n\nVCF column (starting from sample names, zero-based) to ignore when reading VCFS [optional]\n\n\n1\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nRaw signatures\n\n\nFile\n\n\nTab-delimited file of the raw results with sample contributions for each signature\n\n\n\n\n\n\nSignatures visualization\n\n\nFile\n\n\nHTML file for interactive plotting\n\n\n\n\n\n\nSample sheet\n\n\nFile\n\n\nTab-delimited file (no headers) with sample ID and tag pairs\n\n\n\n\n\n\n\n\nProcess\n\n\n\nMutational Spectrum runs four steps using subcommands of \nmutspec\n.\n\n\n\n\nSplit VCFs (single or multi-sample) to multiple single-sample VCFs.\n\n\nIf not given, generate a sample sheet from the directory of single-sample\n     VCFs.\n\n\nBuild a mutation matrix and reconstruct/fit it using COSMIC mutation\n     signatures.\n\n\nCreate a visualization file using the fitted signatures.\n\n\n\n\nGetting started\n\n\n\n\nTodo\n\n\n\n\nInput configuration\n\n\nMutational Spectrum only requires VCFs as inputs. This can be a single\nmulti-sample VCF, multiple single-sample VCFs, or a combination of both. All\nother inputs are optional.\n\n\nSample sheet\n\n\n\nSample sheet\n is a tab-delimited file (no headers) with two columns: the\nsample ID and a tag. The tag is an arbitrary identifier used to group the\nsamples, typically a disease abbreviation or tissue of origin.\n\n\nIf not given, a sample sheet will be generated automatically.\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSJACT001_D\n\n\nACT\n\n\n\n\n\n\nSJACT002_D\n\n\nACT\n\n\n\n\n\n\nSJBALL063_D\n\n\nBALL\n\n\n\n\n\n\nSJHGG017_D\n\n\nHGG\n\n\n\n\n\n\n\n\nOutput prefix\n\n\n\nOutput prefix\n is the prefix to append to the output filenames. By default,\nif a single input VCF is given, its basename is used as the output prefix. If\nmultiple input VCFs are given, a default \"mutspec\" prefix is used. This\nbehavior can be overridden by a user-defined prefix.\n\n\nExample\n\n\n\n\n\n\n\n\n\nVCF(s)\n\n\nPrefix\n\n\nOutput filename for raw signatures\n\n\n\n\n\n\n\n\n\n\n[\npcgp.b38.refseq.goodbad.vcf\n]\n\n\npcgp.b38.refseq.goodbad\n\n\npcgp.b38.refseq.goodbad.signatures.txt\n\n\n\n\n\n\n[\nSJOS013_D.vcf\n, \nSJRHB007_D.vcf\n]\n\n\nmutspec\n\n\nmutspec.signatures.txt\n\n\n\n\n\n\n\n\nDisabled VCF column\n\n\n\nDisabled VCF column\n is the column index to ignore when reading VCFs. This\nis useful when the inputs are tumor-normal VCFs, and one column should be\nignored. Otherwise, the results would likely be duplicated.\n\n\nThe argument is a zero-based index relative to the sample names in the header\nof the VCF. For example, in a VCF with samples \nSJEPD003_D\n and \nSJEPD003_G\n,\nthe germline sample (\nSJEPD003_G\n) can be discarded by setting the \ndisabled\nVCF column\n to \n1\n.\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n\n\n\n\n#CHROM\n\n\nPOS\n\n\nID\n\n\nREF\n\n\nALT\n\n\nQUAL\n\n\nFILTER\n\n\nINFO\n\n\nFORMAT\n\n\nSJEPD003_D\n\n\nSJEPD003_G\n\n\n\n\n\n\n\n\nUploading data\n\n\nMutational Spectrum requires at least one VCF and an optional sample sheet to\nbe uploaded. These files can be uploaded via the \ndata transfer application\n\nor \ncommand line\n.\n\n\nRunning the tool\n\n\n\n\nTodo\n\n\n\n\nMonitoring run progress\n\n\n\n\nTodo\n\n\n\n\nAnalysis of results\n\n\nUpon a successful run of Mutational Spectrum, three files are saved to the\nresults directory: raw signature contributions, a visualization file, and a\nsample sheet.\n\n\nInterpreting results\n\n\nRaw signatures\n\n\n\nRaw signatures\n is a tab-delimited file of the raw results with sample\ncontributions for each signature. Column 1 is the sample name, columns\n2-(N-1) are the COSMIC signatures contribution counts, and column N is the\ngroup tag, where N is the total number of columns. The number of columns is\nvariable since if the signature has no contributions for all samples, it is\ncompletely omitted.\n\n\nNote that the last column \ntissue\n is a misnomer. It aligns to the arbitrary\ntag given in the \nsample sheet\n.\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nSignature.1\n\n\nSignature.2\n\n\n\u2026\n\n\nSignature.30\n\n\ntissue\n\n\n\n\n\n\n\n\n\n\nSJACT001_D\n\n\n1.71758029\n\n\n131.033723\n\n\n\u2026\n\n\n18.6910151\n\n\nACT\n\n\n\n\n\n\nSJAMLM7005_D\n\n\n51.9627312\n\n\n7.10850351\n\n\n\u2026\n\n\n0\n\n\nAMLM7\n\n\n\n\n\n\n\n\nSignatures visualization\n\n\n\nSignatures visualization\n is an HTML file that can be used for interactive\nplotting.\n\n\nWhen opened in a web browser, a set of controls allows plotting various\nstacked bar charts: total contributions by signature, total contributions by\ntag, and total contributions by sample per tag. The total contributions can be\nstacked as absolute values or as a percentage of the total.\n\n\nSample sheet\n\n\n\nWhen no sample sheet is given as an input, one is generated automatically,\nbut it is not guaranteed the derived tags will be of any use. This generated\nsample sheet is given as an output in the case the tags need to be manually\nedited, and the job is resubmitted with it as an input.\n\n\nWhen a sample sheet is given as an input, the sample sheet output is a copy\nof the input.\n\n\nSee also the description for the input \nsample sheet\n.", 
            "title": "Mutational Spectrum"
        }, 
        {
            "location": "/guides/tools/mutational-spectrum/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/tools/mutational-spectrum/#getting-started", 
            "text": "Todo", 
            "title": "Getting started"
        }, 
        {
            "location": "/guides/tools/mutational-spectrum/#input-configuration", 
            "text": "Mutational Spectrum only requires VCFs as inputs. This can be a single\nmulti-sample VCF, multiple single-sample VCFs, or a combination of both. All\nother inputs are optional.", 
            "title": "Input configuration"
        }, 
        {
            "location": "/guides/tools/mutational-spectrum/#uploading-data", 
            "text": "Mutational Spectrum requires at least one VCF and an optional sample sheet to\nbe uploaded. These files can be uploaded via the  data transfer application \nor  command line .", 
            "title": "Uploading data"
        }, 
        {
            "location": "/guides/tools/mutational-spectrum/#running-the-tool", 
            "text": "Todo", 
            "title": "Running the tool"
        }, 
        {
            "location": "/guides/tools/mutational-spectrum/#monitoring-run-progress", 
            "text": "Todo", 
            "title": "Monitoring run progress"
        }, 
        {
            "location": "/guides/tools/mutational-spectrum/#analysis-of-results", 
            "text": "Upon a successful run of Mutational Spectrum, three files are saved to the\nresults directory: raw signature contributions, a visualization file, and a\nsample sheet.", 
            "title": "Analysis of results"
        }, 
        {
            "location": "/guides/tools/mutational-spectrum/#interpreting-results", 
            "text": "", 
            "title": "Interpreting results"
        }, 
        {
            "location": "/guides/visualizations/bigwig-viewer/", 
            "text": "ProteinPaint BigWig Viewer\n\n\nThe ProteinPaint interactive coverage viewer is used to visualize any\nbigWig files information. You can follow these steps to get an\nunderstand of how it works.\n\n\n\n\nOpen up the custom viewer file output by your pipeline. The name of\n    this file will vary, so consult the specific pipeline guide to know\n    where to find it.\n\n\n\n\nClick \"Launch\" in the bottom right corner to launch the custom\n    viewer.\n\n\n\n\n\n\n\n\nOnce the page has loaded, you will be able to see the bigWig viewer.\n    You can navigate around the genome by gene or genomic location.\n    Alongside the coverage track is the GENCODE gene reference.", 
            "title": "BigWig Viewer"
        }, 
        {
            "location": "/guides/visualizations/bigwig-viewer/#proteinpaint-bigwig-viewer", 
            "text": "The ProteinPaint interactive coverage viewer is used to visualize any\nbigWig files information. You can follow these steps to get an\nunderstand of how it works.   Open up the custom viewer file output by your pipeline. The name of\n    this file will vary, so consult the specific pipeline guide to know\n    where to find it.   Click \"Launch\" in the bottom right corner to launch the custom\n    viewer.     Once the page has loaded, you will be able to see the bigWig viewer.\n    You can navigate around the genome by gene or genomic location.\n    Alongside the coverage track is the GENCODE gene reference.", 
            "title": "ProteinPaint BigWig Viewer"
        }, 
        {
            "location": "/guides/visualizations/fusion-viewer/", 
            "text": "ProteinPaint Fusion Viewer\n\n\nThe ProteinPaint interactive fusion viewer is used to visualize putative\nfusions called by CICERO. You can follow these steps to get an\nunderstand of how it works.\n\n\n\n\nOpen up the custom viewer file output by your pipeline. The name of\n    this file will vary, so consult the specific pipeline guide to know\n    where to find it.\n\n\n\n\nClick \"Launch\" in the bottom right corner to launch the custom\n    viewer.\n\n\n\n\n\n\n\n\nOnce the page has finished loading, you will be presented with a\n    summary of all of the fusions produced by the pipeline. Each bullet\n    point is a separate category for the structural variants, with the\n    more interesting fusions at the top. Click one of the categories to\n    view the fusions in that category.\n\n\n\n\n\n\n\n\nYou can see all of the fusions in that category are now listed on\n    the screen. Hover over one of the fusions to see the detailed view.\n\n\n\n\n\n\n\n\nThe popup contains a large amount of information that might\n    interesting to you based on your use case, such as the transcript\n    and other metrics like read counts, quality metrics, and recurrence.", 
            "title": "Fusion Viewer"
        }, 
        {
            "location": "/guides/visualizations/fusion-viewer/#proteinpaint-fusion-viewer", 
            "text": "The ProteinPaint interactive fusion viewer is used to visualize putative\nfusions called by CICERO. You can follow these steps to get an\nunderstand of how it works.   Open up the custom viewer file output by your pipeline. The name of\n    this file will vary, so consult the specific pipeline guide to know\n    where to find it.   Click \"Launch\" in the bottom right corner to launch the custom\n    viewer.     Once the page has finished loading, you will be presented with a\n    summary of all of the fusions produced by the pipeline. Each bullet\n    point is a separate category for the structural variants, with the\n    more interesting fusions at the top. Click one of the categories to\n    view the fusions in that category.     You can see all of the fusions in that category are now listed on\n    the screen. Hover over one of the fusions to see the detailed view.     The popup contains a large amount of information that might\n    interesting to you based on your use case, such as the transcript\n    and other metrics like read counts, quality metrics, and recurrence.", 
            "title": "ProteinPaint Fusion Viewer"
        }, 
        {
            "location": "/guides/portals/sickle-cell/", 
            "text": "The Sickle Cell Genomics Portal contains two viewers for the exploration of data from the  \nSickle Cell Genome Project (SGP)\n dataset.\n\n\nGenome Browser\n\n\nOverview\n\n\nUpon launching the browser, you will see an image similar to the one shown here.\n\n\n\nA description of the elements of the browser are as follows:\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nNavigation tools and track selector. (\nSee Navigation Buttons section\n)\n\n\n\n\n\n\n2\n\n\nDNase hypersensitivity tracks.  By default, four epigenetic tracks are shown.  These are DNAse hypersensitivity tracks for Hematopoeitic stem cells (HSC), T Cells, Monocytes, and B Cells.  Additional tracks can be viewed by selecting the \u2018Tracks\u2019 button (See Adding/Removing Tracks section below)\n\n\n\n\n\n\n3\n\n\nRefSeq genes.  Gene models from the RefSeq database are displayed in this tracks.\n\n\n\n\n\n\n4\n\n\n-log10 of the p-value of the association of each variants with pain rate in individuals with Sickle Cell Disease.  The analysis has only been performed around the  KIAA1109/Tenr/IL2/IL21 region.   Each dot on the track represents a genomic variant (Single Nucleotide Variant (SNV) or small insertion/deletion (INDEL)).  The Y-axis for the track represents the -log10 of the p-value.  The higher the value, the more stastically significant the association between the variant and pain rate is.  Clicking on a variant will open op a window that gives further details about the variant.  (See Figure 3).\n\n\n\n\n\n\n5\n\n\n-log10 of the p-value of the association of each variants with age of first vaso-occlusive crisis in individuals with Sickle Cell Disease.  See (4) above for more information on this type of track.\n\n\n\n\n\n\n6\n\n\nFilters: Filters allow variants within tracks to be filtered by numerous citeria.  \nSee Filter description\n\n\n\n\n\n\n\n\nNavigation buttons\n\n\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\na\n\n\nLocation/Locus entry field.  One can enter genomic coordinates in the form of chromosome:start-end (for example chr1:12345-9876), or a gene name or a SNP rs ID.\n\n\n\n\n\n\nb\n\n\nBrowser zoom in and out\n\n\n\n\n\n\nc\n\n\nTracks: Add or hide tracks (See section below on adding/hiding tracks)\n\n\n\n\n\n\nd\n\n\nMore:  Save svg image of browser, get DNA sequence or highlight regions of the browser.\n\n\n\n\n\n\n\n\nFilters\n\n\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\na\n\n\nFilters for pain rate p-value track\n\n\n\n\n\n\nb\n\n\nFilters for age of first vaso-occlusive crisis (VOC) p-value\n\n\n\n\n\n\nc\n\n\nThe highlighted filter shows which value is  used for the Y-axis on the browser track.  The value can be changed.\n\n\n\n\n\n\nd\n\n\nA highligthed value within a filter shows which filter value is set.  The number next to the filter represents the number of individuals that meet the filter criteria.\n\n\n\n\n\n\n\n\nGetting Started\n\n\nFinding a variant of interest\n\n\nA user can navigate to a gene or to a variant ID.\nEnter in the variant ID rs13140464 into the search text field at the top of the browser. (See below)\n\n\n\nPressing enter will center the browser of the selected variant.  (see below)\n\n\n\nZooming in and out\n\n\nOne can use the buttons next to the search field to zoom in and out along the genome.  Press the x50 button to zoom out 50 fold\n\n\n\nThis will show a larger region of the chromosome.\n\n\n\nOne can now see three DNase peaks (1) around the rs13140464 variant(2).  In addition there is another variant (3) seen near one of the DNase peaks.\n\n\n\nObtaining additional variant information\n\n\nLeft clicking a variant (see red circle below), will cause a new window to pop up in the browser that will contain additional information about the variant.\n\n\n\nAdding and removing tracks\n\n\nSelect the tracks button from the top of the genome browser.\n\n\nA window displaying selected tracks and tracks available for selection will pop up.\n\n\nOne can scroll down to see additional tracks.  Try selecting and unselecting various tracks and observe the updated tracks on the browser.\n\n\nGetting DNA sequence\n\n\nSelect the 'More' button at the top of the browser.\n\n\nSeveral options will be avaialble.  Select the DNA sequence button.\n\n\nYou will be shown the DNA sequence for the region.\n\n\n\nVariants and Phenotype Viewer\n\n\nOverview\n\n\nWhen the Variants and Phenotype Viewer is launched, the user will be presented with the following visualization.\n\n\nThe different elements of the view are as follows.\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nSettings and sort buttons.  In addition a link to this help document.\n\n\n\n\n\n\n2\n\n\nLegend for different tracks in the viewer\n\n\n\n\n\n\n3\n\n\nPhenotypic data displayed with an individual represented in each column\n\n\n\n\n\n\n4\n\n\nGentoypic data displayed with an individual represented in each column\n\n\n\n\n\n\n\n\nLabels\n\n\nSee glossary for further details\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHb\n\n\nHemoglobin\n\n\n\n\n\n\nHbF\n\n\nFetal Hemoglobin\n\n\n\n\n\n\nHbA2\n\n\nVariant of hemoglobin that contains two alpha subunits and two delta subunits\n\n\n\n\n\n\nMCV\n\n\nMean corpuscular  volume. This is the average size of red blood cells\n\n\n\n\n\n\nPainRate\n\n\nNumber of hospitalizations per year over a two year period.\n\n\n\n\n\n\nSickle cell genotype\n\n\n(SS_Genotype in legend.  Whether patient is SS or SB0\n\n\n\n\n\n\nAlpha deletion\n\n\nWhether the individual has an alpha globin deletion (het=1 deleted allele, homo=2 deleted alleles)\n\n\n\n\n\n\nrs######\n\n\nSeveral variants that we have found to be associated with pain in Sickle Cell Disease\n\n\n\n\n\n\n\n\nGetting Started\n\n\nSorting\n\n\nHover your mouse over the MCV label in the graph.  A box will pop up with several icons.  Select the triangle that is pointed to the left to sort individuals by MCV.\n\n\n\nThe following graph shows individuals sorted by MCV.  Blank columns represent no data available.  Note that PainRate, Sickle cell genotype and alpha deletion status appear to correlate with MCV values.\n\n\n\nView values for all patients\n\n\nHovering over one column will enable the viewing of all  phenotypic values for that patient.\n\n\nUndo\n\n\nWhile exploring the data, one may inadvertently sort or remove data.  One can undo the changes by selecting the undo button at the top of the viewer.  The redo button will revert the undo.\n\n\n\nGlossary\n\n\n\n\n\n\n\n\nFetal hemoglobin (HbF)\n\nFetal hemoglobin contains two subunits of gamma-globin and two units of alpha-globin, while adult hemoglobin contains two subuints of beta-globin and two units of alpha-globin. \n\n\n\n\n\n\nHeriditary persistance of fetal hemoglobin (HPFH)\n\nIndividuals with HPFH have elevated levels of fetal hemoglobin. These elevated levels reduce or eliminate many of the symptoms of Sickle Cell Disease.\n\n\n\n\n\n\nPrincipal Component Analysis (PCA)\n\nA \nmethod\n for reducing high dimensional data into low-dimensional representations.\n\n\n\n\n\n\nSC\n\nAn individual with one copy of the sickle cell allele \nrs334\n and one copy of \nhemoglobin C\n.\n\n\n\n\n\n\nS\n+\n\nAn individual with \nbeta-thalassemia\n who has one copy of the sickle cell allele \nrs334\n and one copy of a beta-globin gene that has reduced expression.\n\n\n\n\n\n\nS\n0\n\nAn individual with \nbeta-thalassemia\n who has one copy of the sickle cell allele \nrs334\n and one copy of a beta-globin gene that is not expressed or is deleted.\n\n\n\n\n\n\n\nSS\n \nAn individual with \nsickle cell disease\n who is homozygous for the sickle cell allele \nrs334\n.\n\n\n\n\n\n\nSCCRIP\n\nThe \nSickle Cell Research and Intervention Program\n.", 
            "title": "Sickle Cell Genomics Portal"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#genome-browser", 
            "text": "", 
            "title": "Genome Browser"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#overview", 
            "text": "Upon launching the browser, you will see an image similar to the one shown here.  A description of the elements of the browser are as follows:     #  Description      1  Navigation tools and track selector. ( See Navigation Buttons section )    2  DNase hypersensitivity tracks.  By default, four epigenetic tracks are shown.  These are DNAse hypersensitivity tracks for Hematopoeitic stem cells (HSC), T Cells, Monocytes, and B Cells.  Additional tracks can be viewed by selecting the \u2018Tracks\u2019 button (See Adding/Removing Tracks section below)    3  RefSeq genes.  Gene models from the RefSeq database are displayed in this tracks.    4  -log10 of the p-value of the association of each variants with pain rate in individuals with Sickle Cell Disease.  The analysis has only been performed around the  KIAA1109/Tenr/IL2/IL21 region.   Each dot on the track represents a genomic variant (Single Nucleotide Variant (SNV) or small insertion/deletion (INDEL)).  The Y-axis for the track represents the -log10 of the p-value.  The higher the value, the more stastically significant the association between the variant and pain rate is.  Clicking on a variant will open op a window that gives further details about the variant.  (See Figure 3).    5  -log10 of the p-value of the association of each variants with age of first vaso-occlusive crisis in individuals with Sickle Cell Disease.  See (4) above for more information on this type of track.    6  Filters: Filters allow variants within tracks to be filtered by numerous citeria.   See Filter description", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#navigation-buttons", 
            "text": "#  Description      a  Location/Locus entry field.  One can enter genomic coordinates in the form of chromosome:start-end (for example chr1:12345-9876), or a gene name or a SNP rs ID.    b  Browser zoom in and out    c  Tracks: Add or hide tracks (See section below on adding/hiding tracks)    d  More:  Save svg image of browser, get DNA sequence or highlight regions of the browser.", 
            "title": "Navigation buttons"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#filters", 
            "text": "#  Description      a  Filters for pain rate p-value track    b  Filters for age of first vaso-occlusive crisis (VOC) p-value    c  The highlighted filter shows which value is  used for the Y-axis on the browser track.  The value can be changed.    d  A highligthed value within a filter shows which filter value is set.  The number next to the filter represents the number of individuals that meet the filter criteria.", 
            "title": "Filters"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#finding-a-variant-of-interest", 
            "text": "A user can navigate to a gene or to a variant ID.\nEnter in the variant ID rs13140464 into the search text field at the top of the browser. (See below)  Pressing enter will center the browser of the selected variant.  (see below)", 
            "title": "Finding a variant of interest"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#zooming-in-and-out", 
            "text": "One can use the buttons next to the search field to zoom in and out along the genome.  Press the x50 button to zoom out 50 fold  This will show a larger region of the chromosome.  One can now see three DNase peaks (1) around the rs13140464 variant(2).  In addition there is another variant (3) seen near one of the DNase peaks.", 
            "title": "Zooming in and out"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#obtaining-additional-variant-information", 
            "text": "Left clicking a variant (see red circle below), will cause a new window to pop up in the browser that will contain additional information about the variant.", 
            "title": "Obtaining additional variant information"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#adding-and-removing-tracks", 
            "text": "Select the tracks button from the top of the genome browser. \nA window displaying selected tracks and tracks available for selection will pop up. \nOne can scroll down to see additional tracks.  Try selecting and unselecting various tracks and observe the updated tracks on the browser.", 
            "title": "Adding and removing tracks"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#getting-dna-sequence", 
            "text": "Select the 'More' button at the top of the browser. \nSeveral options will be avaialble.  Select the DNA sequence button. \nYou will be shown the DNA sequence for the region.", 
            "title": "Getting DNA sequence"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#variants-and-phenotype-viewer", 
            "text": "", 
            "title": "Variants and Phenotype Viewer"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#overview_1", 
            "text": "When the Variants and Phenotype Viewer is launched, the user will be presented with the following visualization. \nThe different elements of the view are as follows.     #  Description      1  Settings and sort buttons.  In addition a link to this help document.    2  Legend for different tracks in the viewer    3  Phenotypic data displayed with an individual represented in each column    4  Gentoypic data displayed with an individual represented in each column", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#labels", 
            "text": "See glossary for further details     #  Description      Hb  Hemoglobin    HbF  Fetal Hemoglobin    HbA2  Variant of hemoglobin that contains two alpha subunits and two delta subunits    MCV  Mean corpuscular  volume. This is the average size of red blood cells    PainRate  Number of hospitalizations per year over a two year period.    Sickle cell genotype  (SS_Genotype in legend.  Whether patient is SS or SB0    Alpha deletion  Whether the individual has an alpha globin deletion (het=1 deleted allele, homo=2 deleted alleles)    rs######  Several variants that we have found to be associated with pain in Sickle Cell Disease", 
            "title": "Labels"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#getting-started_1", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#sorting", 
            "text": "Hover your mouse over the MCV label in the graph.  A box will pop up with several icons.  Select the triangle that is pointed to the left to sort individuals by MCV.  The following graph shows individuals sorted by MCV.  Blank columns represent no data available.  Note that PainRate, Sickle cell genotype and alpha deletion status appear to correlate with MCV values.", 
            "title": "Sorting"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#view-values-for-all-patients", 
            "text": "Hovering over one column will enable the viewing of all  phenotypic values for that patient.", 
            "title": "View values for all patients"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#undo", 
            "text": "While exploring the data, one may inadvertently sort or remove data.  One can undo the changes by selecting the undo button at the top of the viewer.  The redo button will revert the undo.", 
            "title": "Undo"
        }, 
        {
            "location": "/guides/portals/sickle-cell/#glossary", 
            "text": "Fetal hemoglobin (HbF) \nFetal hemoglobin contains two subunits of gamma-globin and two units of alpha-globin, while adult hemoglobin contains two subuints of beta-globin and two units of alpha-globin.     Heriditary persistance of fetal hemoglobin (HPFH) \nIndividuals with HPFH have elevated levels of fetal hemoglobin. These elevated levels reduce or eliminate many of the symptoms of Sickle Cell Disease.    Principal Component Analysis (PCA) \nA  method  for reducing high dimensional data into low-dimensional representations.    SC \nAn individual with one copy of the sickle cell allele  rs334  and one copy of  hemoglobin C .    S + \nAn individual with  beta-thalassemia  who has one copy of the sickle cell allele  rs334  and one copy of a beta-globin gene that has reduced expression.    S 0 \nAn individual with  beta-thalassemia  who has one copy of the sickle cell allele  rs334  and one copy of a beta-globin gene that is not expressed or is deleted.    SS  \nAn individual with  sickle cell disease  who is homozygous for the sickle cell allele  rs334 .    SCCRIP \nThe  Sickle Cell Research and Intervention Program .", 
            "title": "Glossary"
        }, 
        {
            "location": "/faq/", 
            "text": "Q. Will I be charged for using St. Jude Cloud?\n\n\nA. Although you may be prompted to enter billing information, you will not\nbe charged for anything with the exception of the following items:\n\n\n\n\nAny copy of the St. Jude data you receive is considered \"sponsored\",\n  so you do not have to pay a fee to store this data in St. Jude\n  Cloud. You will be charged for any \nderivative\n files obtained\n  through running analyses on St. Jude data and stored on the St. Jude\n  Cloud.\n\n\nIf you elect to \ndownload\n any data from SJCloud, you will be\n  charged an egress fee by DNAnexus. This fee is usually negligible\n  unless you are downloading entire cohorts. We are actively\n  investigating ways to minimize or eliminate these costs.\n\n\nIf you run any of the following tools, you will be charged for the\n  compute resources used in producing the results as well as storage\n  fees associated with storing the results files.\n\n\nChIP-Seq Peak Caller\n\n\nHLA Typing and Neoepitope Prediction\n\n\nRapid RNA-Seq Fusion Detection\n\n\nWARDEN Differential Expression Analysis\n\n\n\n\n\n\n\n\nQ. How can I delete my account?\n\n\nA. If you'd like to delete your account, please email DNAnexus support at\n\n with the following email.\n\n\nHi DNAnexus,\n\n  Would you please assist me in deleting my St. Jude Cloud account? My username is _____.\n\nThank you!\n\n\n\n\n\nQ. Where can I find the Terms of Service or the Privacy Policy?\n\n\nA. You can find the Terms of Service\n\nhere\n and the Privacy Policy\n\nhere\n.", 
            "title": "Frequently Asked Questions"
        }
    ]
}